{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b69915e",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c07a01",
   "metadata": {},
   "source": [
    "Author & Instructor: Diana NURBAKOVA, PhD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161338bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/styles.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422babca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe21429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the \"resources\" directory to the path\n",
    "project_root = Path().resolve().parent\n",
    "resources_path = project_root / 'resources'\n",
    "sys.path.insert(0, str(resources_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50279902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fda368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InteractiveRegression import (\n",
    "    create_interactive_regression,\n",
    "    create_static_visualization,\n",
    "    compare_lines,\n",
    "    plot_error_example,\n",
    "    plot_3d_regression, \n",
    "    calculate_confidence_intervals, \n",
    "    plot_regression_with_intervals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polynomial import (generate_polynomials_plots, get_scenarios, get_poly_fit, get_overfit_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e2008",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673bbae",
   "metadata": {},
   "source": [
    "By the end of this session, you will be able to:\n",
    "- Construct a linear regression model using OLS\n",
    "- Use polynomial features \n",
    "- Apply regularisation mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ec22d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4>üéØ Will you love this film?</h3>\n",
    "\n",
    "Netflix has millions of user ratings, but how do they predict if you'll rate a movie 4.2 stars vs 4.3 stars?\n",
    "\n",
    "By the end, you'll understand how to build the core engine that powers recommendation systems.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e3b3c",
   "metadata": {},
   "source": [
    "Before addressing this problem, let's start with the following toy example. \n",
    "\n",
    "We have 9 observations of infants' height and age:\n",
    "\n",
    "| Age | Height |\n",
    "| ---- | ----- |\n",
    "| 1.0 | 70.56 |\n",
    "| 1.5 | 67.68 |\n",
    "| 2.0 | 80.88 | \n",
    "| 2.5 | 82.32 |\n",
    "| 3.0 | 84.00 |\n",
    "| 3.5 | 90.00 | \n",
    "| 4.0 | 93.60 |\n",
    "| 4.5 | 105.36 |\n",
    "| 5.0 | 109.92 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763066a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example data points\n",
    "data_x = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]) # age\n",
    "data_y = np.array([70.56, 67.68, 80.88, 82.32, 84.00, 90.00, 93.60, 105.36, 109.92]) # height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bfcb5",
   "metadata": {},
   "source": [
    "Let's plot height vs age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fad622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of data points\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.scatter(data_x, data_y, c='blue', s=80, alpha=0.8, edgecolors='white', linewidth=2)\n",
    "ax.set_xlim(0.5, 5.5)\n",
    "ax.set_ylim(60, 115)\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Height (cm)\")\n",
    "ax.set_title(\"Age vs Height\")\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3982f",
   "metadata": {},
   "source": [
    "It seems that there is a linear relation between these two measurements. \n",
    "\n",
    "We would like to create a linear model that represents our data or in other words, that allows us to see what a trend is. To do so, we can draw a line that **fits** the data.\n",
    "\n",
    "Let's draw a couple of lines trying to represent (fit) the data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive visualisation of fitted line\n",
    "create_interactive_regression(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7eac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare several lines\n",
    "compare_lines(data_x, data_y, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7daab6",
   "metadata": {},
   "source": [
    "What makes one line mathematically 'better' than another?\n",
    "\n",
    "## How to Find Your Line: Ordinary Least Squares (OLS)\n",
    "<a id=\"ols\"></a>\n",
    "\n",
    "### Slope-Intercept Form of a Line\n",
    "<a id=\"slope-intercept\"></a>\n",
    "\n",
    "Equation of a line: $$y = ax + b$$ \n",
    "where $y$ is a dependent variable, $x$ is an independent variable, $a$ is a slope and $b$ is an intercept.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/line-equation.png\" alt=\"Line equation\", width=\"400\">\n",
    "</center>\n",
    "\n",
    "**Geometric interpretation of slope and intercept**\n",
    "\n",
    "<center>\n",
    "<img src=\"img/slope-intercept.png\" alt=\"Geometric interpretation of slope and intercept\", width=\"400\">\n",
    "</center>\n",
    "\n",
    "$$b = \\text{where the line crosses the } y\\text{-axis}$$\n",
    "$$a = \\frac{\\text{Rise}}{\\text{Run}}= \\frac{\\text{go up}}{\\text{move to the side}} = \\frac{\\text{\\# units over }y}{\\text{\\# units over }x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a733b",
   "metadata": {},
   "source": [
    "### Finding Parameters\n",
    "<a id=\"params-2d-case\"></a>\n",
    "\n",
    "> How to find parameters of a line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a48175",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_example(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e8016",
   "metadata": {},
   "source": [
    "On the one hand, our estimated line is given by: \n",
    "$$\\hat{y} = ax + b$$\n",
    "\n",
    "On the other hand, there are observed data points $(x_i, y_i), i=\\bar{1,n}$.\n",
    "\n",
    "So, we want to minimize the difference between estimated and observed values, i.e. *the error*: $$\\sum_{i=1}^{n}{err}_i^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n}(y_i - (ax_i + b))^2$$\n",
    "\n",
    "Which means that we want to find the values of $a$ and $b$ that minimize this objective function. A candidate is given by:\n",
    "\n",
    "$$\\left\\{\\begin{array}{l} a = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{Cov(X, Y)}{s_X^2} \\\\ b = \\frac{\\sum_{i=1}^n y_i}{n} - a\\frac{\\sum_{i=1}^n x_i}{n} = \\bar{y} - a \\bar{x}\\end{array}\\right.$$\n",
    "\n",
    "Let's prove that these values of $a$ and $b$ minimize our goal function $f = \\sum_{i=1}^n {err}_i^2 = \\sum_{i=1}^{n}(y_i - (ax_i + b))^2$.\n",
    "\n",
    "$$(\\hat{a}, \\hat{b}) = \\argmin{\\sum_{i=1}^n {err}_i^2} = \\argmin{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} = \\argmin{\\sum_{i=1}^{n}(y_i - (ax_i + b))^2}$$\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary> Click to reveal the solution </summary>\n",
    "\n",
    "1. Calculate the partial derivatives and set them equal to 0\n",
    "$$\\left\\{\\begin{array}{l}\\frac{\\partial f}{\\partial a} = -2 \\sum_{i=1}^{n}(y_i - ax_i - b)x_i = 0 \\\\ \\frac{\\partial f}{\\partial b} =  -2 \\sum_{i=1}^{n}(y_i - ax_i - b) = 0\\end{array}\\right.$$\n",
    "\n",
    "First, let's focus on the second equation:\n",
    "$$\\sum_{i=1}^{n}(y_i - ax_i - b) = 0$$\n",
    "which is equivalent to:\n",
    "$$\\sum_{i=1}^{n}y_i -\\sum_{i=1}^n ax_i - nb = 0$$\n",
    "\n",
    "Let's solve it for $b$:\n",
    "$$b = \\frac{\\sum_{i=1}^{n}y_i -\\sum_{i=1}^n ax_i}{n} = \\frac{\\sum_{i=1}^{n}y_i - a\\sum_{i=1}^n x_i}{n} = \\frac{\\sum_{i=1}^{n}y_i}{n}- a\\frac{\\sum_{i=1}^n x_i}{n} = \\bar{y} - a \\bar{x}$$\n",
    "\n",
    "Let's replace $b$ with its value in the first equation:\n",
    "$$-2 \\sum_{i=1}^{n}(y_i - ax_i - b)x_i = -2 \\sum_{i=1}^{n}(y_i - ax_i - (\\bar{y} - a \\bar{x}))x_i = 0$$\n",
    "$$\\sum_{i=1}^{n}(y_i - ax_i - (\\bar{y} - a \\bar{x}))x_i = 0$$\n",
    "$$\\sum_{i=1}^{n}(y_ix_i - ax_ix_i - \\bar{y}x_i + a \\bar{x}x_i) = 0$$\n",
    "$$\\sum_{i=1}^{n}((y_ix_i - \\bar{y}x_i) - (ax_ix_i - a \\bar{x}x_i)) = 0$$\n",
    "$$\\sum_{i=1}^{n}((y_i - \\bar{y})x_i - (ax_i - a \\bar{x})x_i) = 0$$\n",
    "$$\\sum_{i=1}^{n}(y_i - \\bar{y})x_i - \\sum_{i=1}^{n}(ax_i - a \\bar{x})x_i = 0$$\n",
    "$$\\sum_{i=1}^{n}(y_i - \\bar{y})x_i - a\\sum_{i=1}^{n}(x_i - \\bar{x})x_i = 0$$\n",
    "\n",
    "Let's solve it for $a$:\n",
    "$$a = \\frac{\\sum_{i=1}^{n}(y_i - \\bar{y})x_i}{\\sum_{i=1}^{n}(x_i - \\bar{x})x_i}$$\n",
    "\n",
    "$$a = \\frac{\\sum_{i=1}^{n}(y_ix_i - \\bar{y}x_i)}{\\sum_{i=1}^{n}(x_i^2 - \\bar{x}x_i)} = \\frac{\\sum_{i=1}^{n}y_ix_i - \\bar{y}\\sum_{i=1}^{n}x_i}{\\sum_{i=1}^{n}x_i^2 - \\bar{x}\\sum_{i=1}^{n}x_i} = \\frac{\\sum_{i=1}^{n}y_ix_i - \\bar{y}(n\\bar{x})}{\\sum_{i=1}^{n}x_i^2 - \\bar{x}(n\\bar{x})} = \\frac{\\sum_{i=1}^{n}y_ix_i - n\\bar{y}\\bar{x}}{\\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2}$$\n",
    "\n",
    "Given that: $$Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n} = \\frac{\\sum_{i=1}^n(x_iy_i - x_i\\bar{y} - \\bar{x}y_i + \\bar{x}\\bar{y})}{n} = \\frac{\\sum_{i=1}^nx_iy_i - \\sum_{i=1}^nx_i\\bar{y} - \\sum_{i=1}^n\\bar{x}y_i + n\\bar{x}\\bar{y}}{n}=$$\n",
    "$$=\\frac{\\sum_{i=1}^nx_iy_i - n(\\frac{1}{n}\\sum_{i=1}^nx_i)\\bar{y} - n(\\frac{1}{n}\\sum_{i=1}^ny_i)\\bar{x} + n\\bar{x}\\bar{y}}{n}=\\frac{\\sum_{i=1}^nx_iy_i - n\\bar{x}\\bar{y} - n\\bar{y}\\bar{x} + n\\bar{x}\\bar{y}}{n}=\\frac{\\sum_{i=1}^nx_iy_i - n\\bar{x}\\bar{y}}{n}$$\n",
    "\n",
    "$$a = \\frac{nCov(X, Y)}{ns^2_X} = \\frac{Cov(X, Y)}{s^2_X}$$\n",
    "\n",
    "Thus, we obtain a critical point $(\\hat{a}, \\hat{b}) = \\big(\\frac{Cov(X, Y)}{s^2_X}, \\bar{y} - \\bar{x}\\frac{Cov(X, Y)}{s^2_X}\\big)$\n",
    "\n",
    "\n",
    "2. Apply the second derivative test:\n",
    "- For each critical point, calculate the second partial derivatives:\n",
    "\n",
    "$$f_{aa} = \\frac{\\partial^2f}{\\partial a^2}$$\n",
    "$$f_{bb} = \\frac{\\partial^2f}{\\partial b^2}$$\n",
    "$$f_{ab} = \\frac{\\partial^2f}{\\partial a\\partial b}$$\n",
    "\n",
    "- Then construct the Hessian matrix $\\mathcal{H}$ and compute its determinant (discriminant): $D = f_{aa}\\dot f_{bb} - f_{ab}^2$\n",
    "- At each critical point:\n",
    "\n",
    "    * If $D > 0$ and $f_{aa} > 0$ or equivalently, $tr(\\mathcal{H}) = f_{aa} + f_{bb} > 0$: local minimum\n",
    "    * If $D > 0$ and $f_{aa} < 0$ or equivalently, $tr(\\mathcal{H}) = f_{aa} + f_{bb} < 0$: local maximum\n",
    "    * If $D < 0$: saddle point\n",
    "    * If $D = 0$: test is inconclusive\n",
    "\n",
    "Recall that $f = \\sum_{i=1}^{n}(y_i - (ax_i + b))^2$. Then:\n",
    "\n",
    "$$f_{aa} = \\frac{\\partial^2f}{\\partial a^2} = (-2 \\sum_{i=1}^{n}(y_i - ax_i - b)x_i)'_a = 2\\sum_{i=1}^{n}x_i^2 \\mathbb{> 0}$$\n",
    "\n",
    "$$f_{bb} = \\frac{\\partial^2f}{\\partial b^2} = (-2 \\sum_{i=1}^{n}(y_i - ax_i - b))'_b = 2n$$\n",
    "\n",
    "$$f_{ab} = \\frac{\\partial^2f}{\\partial a\\partial b} = (-2 \\sum_{i=1}^{n}(y_i - ax_i - b)x_i)'_b = 2\\sum_{i=1}^2 x_i = 2n\\bar{x}$$\n",
    "\n",
    "So, we obtain the following Hessian matrix: $$\\mathcal{H}(a, b) = \\left(\\begin{matrix}\\frac{\\partial^2f}{\\partial a^2} & \\frac{\\partial^2f}{\\partial a\\partial b} \\\\ \\frac{\\partial^2f}{\\partial b\\partial a} & \\frac{\\partial^2f}{\\partial b^2}\\end{matrix}\\right) = \\left(\\begin{matrix}2\\sum_{i=1}^{n}x_i^2 & 2n\\bar{x} \\\\ 2n\\bar{x} & 2n\\end{matrix}\\right)$$\n",
    "\n",
    "Now, let's compute its determinant:\n",
    "\n",
    "$$D = det(\\mathcal{H}) = f_{aa}\\dot f_{bb} - f_{ab}^2 = 4n\\sum_{i=1}^{n}x_i^2 - 4 n^2 \\bar{x}^2 = 4n^2 (\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 - \\bar{x}^2) = 4n^2Var(X) \\mathbb{> 0}$$\n",
    "\n",
    "Thus, we obtain $D > 0, f_{aa} > 0$ and $tr(\\mathcal{H}) = 2\\sum_{i=1}^{n}x_i^2 + 2n > 0$. Therefore, we can conclude that our critical point $(\\hat{a}, \\hat{b})$ is a local minimum of $f$. \n",
    "\n",
    "3. Check boundary conditions, if the domain has boundaries.\n",
    "\n",
    "Since this function is defined on all of $\\mathbb{R}^2$ with no constraints, we only need the critical point analysis.\n",
    "\n",
    "4. Compare and identify global minimum / maximum by evaluating $f(x,y)$ at all local minima / maxima found in the previous steps.\n",
    "\n",
    "In our case, there is only one critical point. So, our point is also a global minimum. \n",
    "\n",
    "</details>\n",
    "\n",
    "Now, let's check the optimal solution of a line for our toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create static visualization first\n",
    "print(\"Optimal solution:\")\n",
    "create_static_visualization(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22831ca5",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 1:</h5> Calculate manually the coefficients for our toy example and compare the results of each step with python calculations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049471ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e8b19",
   "metadata": {},
   "source": [
    "## From 1 Independent Variable to $n$ (General Case)\n",
    "<a id=\"ols-general\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d33c99",
   "metadata": {},
   "source": [
    "Now, let's add weight to our toy example and see what our linear model is going to look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "data_z = np.array([12.0, 13.5, 15.0, 16.5, 18.0, 19.8, 21.0, 22.5, 24.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24105007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot regression plane\n",
    "plot_3d_regression(data_x, data_y, data_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18783122",
   "metadata": {},
   "source": [
    "Previously, we have seen that in case of 2 independent variables (attributes or features ), our model consists in finding a plane in 3D space that best fits the data. What happens when the number of attributes is equal to $n$?\n",
    "\n",
    "In general case, our model can be represented as follows: $$y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_p + \\epsilon$$ \n",
    "where $x_i\\in \\mathrm{R}\\ (i=\\bar{1, p})$ are $p$ features, $\\beta_j\\in \\mathrm{R}\\ (j = \\bar{0, p})$ are coefficients of the model, and $\\epsilon\\in\\mathrm{R}^n$ is an error term.\n",
    "\n",
    "Or in matrix form: $$\\mathbf{y} = X\\mathbf{\\beta} + \\mathbf{\\epsilon}$$\n",
    "where $n$ is the number of observations (samples), $p$ the number of features, $y\\in \\mathrm{R}^n, \\mathbf{\\beta} \\in \\mathrm{R}^{p+1}, X\\in \\mathrm{R}^{n\\times(p+1)} \\mathbf{\\epsilon}\\in \\mathrm{R}^{n}$. The corresponding *normal equation* is given by: $X'\\mathbf{y} = X'X\\mathbf{\\beta}$\n",
    "\n",
    "**Assumptions on the error term $\\mathbf{\\epsilon} = [\\epsilon_1, \\epsilon_2, ..., \\epsilon_n]^T \\sim \\mathcal{N}(0, \\mathbf{\\sigma}^2\\mathbf{I})$ (independent, identically distributed normal errors)**:\n",
    "1. Normality: errors follow a normal distributions $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "2. Independence: $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ independently, i.e. $Cov(\\epsilon_i, \\epsilon_j) = 0$ for $i\\neq j$. Which also means that there is no correlation between errors. This validates standard error calculations.\n",
    "3. Zero mean: $\\mathrm{E}[\\epsilon_i] = 0, \\forall i$ ($\\mathrm{E}[\\mathbf{\\mathbf{\\epsilon}}] = 0_n$, zero vector) which implies that there is no systematic bias and ensures unbiased estimates $\\mathrm{E}[\\hat{\\beta}] = \\beta$\n",
    "4. Constant variance (homoscedasticity): $Var(\\epsilon_i) = \\sigma^2 \\mathbf{I}, \\forall i$ where $\\mathbf{I}$ is the identity matrix.\n",
    "\n",
    "\n",
    "\n",
    "This error term represents errors in $y$ and natural randomness in the process. It embodies unobserved variables affecting $y$ and model specification errors.\n",
    "\n",
    "*Note*: In machine learning, we often relax these assumptions. Deep learning, for instance, makes no distributional assumptions about errors. But understanding these classical assumptions helps us know when linear regression is appropriate and how to diagnose problems.\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "\n",
    "In $(p+1)$-dimensional space, we fit a $p$-dimensional hyperplane through the data points. The regression finds the hyperplane that minimizes the sum of squared perpendicular distances. This plane $\\hat{y} = X\\hat{\\beta}$ is the orthogonal projection of $y$ onto the space of $X$.\n",
    "\n",
    "**OLS in general case:**\n",
    "\n",
    "Our *objective* is to minimize $||\\mathbf{y} - X\\mathbf{\\beta}||^2$\n",
    "\n",
    "**Solution** (when $X'X$ is invertible) is given by: $$\\mathbf{\\hat{\\beta}} = (X'X)^{-1}X'y$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491ba34",
   "metadata": {},
   "source": [
    "Let's apply this generalised method to our toy example.\n",
    "\n",
    "1. Step 1: Let's add an intercept column to our features as the first column. At the end, we obtain a 9√ó2 matrix $X$ (9 observations, 2 parameters):\n",
    "\n",
    "$$X = \\left(\\begin{matrix} 1 & 1.0 \\\\ 1 & 1.5 \\\\ 1 & 2.0 \\\\ 1 & 2.5 \\\\ 1 &  3.0 \\\\ 1 & 3.5 \\\\ 1 &  4.0\\\\ 1 &  4.5\\\\ 1 &  5.0 \\end{matrix}\\right)$$\n",
    "\n",
    "We also have out output vector $y$:\n",
    "\n",
    "$$y = \\left(\\begin{matrix} 70.56 \\\\ 67.68 \\\\ 80.88 \\\\ 82.32 \\\\ 84.00 \\\\ 90.00 \\\\ 93.60 \\\\ 105.36 \\\\ 109.92 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aee4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([np.ones(len(data_x)), data_x])  # Add intercept\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1da19",
   "metadata": {},
   "source": [
    "2. Step 2: Calculate transpose $X^T$:\n",
    "\n",
    "$$X^T = \\left(\\begin{matrix} 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\ 1.0 & 1.5 & 2.0 & 2.5 & 3.0 & 3.5 & 4.0 & 4.5 &  5.0 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ba7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eeb36b",
   "metadata": {},
   "source": [
    "3. Step 3: Calculate $X^TX$:\n",
    "\n",
    "$$X^TX = \\left(\\begin{matrix} 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\ 1.0 & 1.5 & 2.0 & 2.5 & 3.0 & 3.5 & 4.0 & 4.5 &  5.0 \\end{matrix}\\right)\\left(\\begin{matrix} 1 & 1.0 \\\\ 1 & 1.5 \\\\ 1 & 2.0 \\\\ 1 & 2.5 \\\\ 1 &  3.0 \\\\ 1 & 3.5 \\\\ 1 &  4.0\\\\ 1 &  4.5\\\\ 1 &  5.0 \\end{matrix}\\right)$$\n",
    "\n",
    "Let's perform the calculation for each element:\n",
    "\n",
    "$X^TX[1, 1] = 1\\times 1 + 1\\times 1 + 1\\times 1 + 1\\times 1 + 1\\times 1 + 1\\times 1 + 1\\times 1 + 1\\times 1 + 1\\times 1 = 9$\n",
    "\n",
    "$X^TX[1, 2] = 1\\times 1.0 + 1\\times 1.5 + 1\\times 2.0 + 1\\times 2.5 + 1\\times 3.0 + 1\\times 3.5 + 1\\times 4.0 + 1\\times 4.5 + 1\\times 5.0 = 27.0$\n",
    "\n",
    "$X^TX[2, 1] = 1.0 \\times 1 + 1.5 \\times 1 + 2.0 \\times 1 + 2.5 \\times 1 + 3.0 \\times 1 + 3.5 \\times 1 + 4.0 \\times 1 + 4.5 \\times 1 + 5.0 \\times 1 = 27.0$\n",
    "\n",
    "$X^TX[2, 2] = 1.0 \\times 1.0 + 1.5 \\times 1.5 + 2.0 \\times 2.0 + 2.5 \\times 2.5 + 3.0 \\times 3.0 + 3.5 \\times 3.5 + 4.0 \\times 4.0 + 4.5 \\times 4.5 + 5.0 \\times 5.0 = 96.0$\n",
    "\n",
    "$$X^TX = \\left(\\begin{matrix} 9.0 & 27 \\\\ 27.0 & 96.0 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtx = X.T @ X\n",
    "print(xtx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d41779",
   "metadata": {},
   "source": [
    "4. Step 4: Calculate the inverse $(X^TX)^{-1}$\n",
    "\n",
    "For a 2x2 matrix $\\left(\\begin{matrix} a & b \\\\ c & d \\end{matrix}\\right)$, the inverse is given by $(1/det) \\times \\left(\\begin{matrix} d & -b \\\\ -c & a \\end{matrix}\\right)$ where $det$ is the determinant of the matrix $X^TX$.\n",
    "\n",
    "$$det(X^TX) = 9.0\\times 96.0 - 27.0\\times 27 = 864.0 - 729.0 = 135.0$$\n",
    "\n",
    "$$(X^TX)^{-1} = \\frac{1}{135.0} \\times \\left(\\begin{matrix} 96.0 & -27 \\\\ -27.0 & 9.0 \\end{matrix}\\right) = \\left(\\begin{matrix} \\frac{96.0}{135} & \\frac{-27}{135} \\\\ \\frac{-27.0}{135} & \\frac{9.0}{135} \\end{matrix}\\right) = \\left(\\begin{matrix} 0.7111 & -0.2 \\\\ -0.2 & 0.0667 \\end{matrix}\\right)$$\n",
    "\n",
    "Let's verify this is correct by checking $(X^TX)(X^TX)^{-1} = I$:\n",
    "\n",
    "$$(X^TX)(X^TX)‚Åª¬π = \\left(\\begin{matrix} 9.0 & 27 \\\\ 27.0 & 96.0 \\end{matrix}\\right) \\left(\\begin{matrix} 0.7111 & -0.2 \\\\ -0.2 & 0.0667 \\end{matrix}\\right) =\\left(\\begin{matrix} 9\\times0.7111 + 27\\times(-0.2) & 9\\times(-0.2) + 27\\times 0.0667 \\\\ 27\\times0.7111 + 96\\times(-0.2) & 27\\times(-0.2) + 96\\times 0.0667 \\end{matrix}\\right) =$$\n",
    "\n",
    "$$= \\left(\\begin{matrix} 6.4 - 5.4 & -1.8 + 1.8 \\\\ 19.2 - 19.2 & -5.4 + 6.4 \\end{matrix}\\right) =\\left(\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c73457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the inverse\n",
    "xtx_inv = np.linalg.inv(xtx)\n",
    "print(xtx_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc915b9d",
   "metadata": {},
   "source": [
    "5. Step 5: Calculate $X^Ty$\n",
    "\n",
    "$$X^Ty = \\left(\\begin{matrix} 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\ 1.0 & 1.5 & 2.0 & 2.5 & 3.0 & 3.5 & 4.0 & 4.5 &  5.0 \\end{matrix}\\right)\\left(\\begin{matrix} 70.56 \\\\ 67.68 \\\\ 80.88 \\\\ 82.32 \\\\ 84.00 \\\\ 90.00 \\\\ 93.60 \\\\ 105.36 \\\\ 109.92 \\end{matrix}\\right) =$$\n",
    "$$\\left(\\begin{matrix} 1\\times 70.56 + 1\\times 67.68 + 1\\times 80.88 + 1\\times 82.32 + 1\\times 84.00 + 1\\times 90.00 + 1\\times 93.60 + 1\\times 105.36 + 1\\times 109.92 \\\\ 1\\times 70.56 + 1.5\\times 67.68 + 2\\times 80.88 + 2.5\\times 82.32 + 3\\times 84.00 + 3.5\\times 90.00 + 4\\times 93.60 + 4.5\\times 105.36 + 5\\times 109.92\\end{matrix}\\right) =$$\n",
    "$$= \\left(\\begin{matrix} 784.32 \\\\ 70.56 + 101.52 + 161.76 + 205.80 + 252.00 + 315.00 + 374.40 + 474.12 + 549.60\\end{matrix}\\right) = \\left(\\begin{matrix} 784.32 \\\\ 2504.76\\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xty = X.T @ data_y\n",
    "print(xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff94691",
   "metadata": {},
   "source": [
    "6. Step 6: Calculate $\\mathbf{\\hat{\\beta}} = (X'X)^{-1}X'y$\n",
    "\n",
    "$$\\mathbf{\\hat{\\beta}} = (X'X)^{-1}X'y = \\left(\\begin{matrix} 0.7111 & -0.2 \\\\ -0.2 & 0.0667 \\end{matrix}\\right)\\left(\\begin{matrix} 784.32 \\\\ 2504.76\\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5c3ec",
   "metadata": {},
   "source": [
    "$$=\\left(\\begin{matrix} 0.7111\\times 784.32 + (-0.2)\\times 2504.76 \\\\ -0.2 \\times 784.32 +  0.0667 \\times 2504.76 \\end{matrix}\\right) = \\left(\\begin{matrix} 557.73 - 500.952  \\\\ -156.864 +  167.0675 \\end{matrix}\\right) = \\left(\\begin{matrix} \\mathbf{56.778}  \\\\ \\mathbf{10.203} \\end{matrix}\\right) = \\left(\\begin{matrix} \\mathbf{\\beta_0}  \\\\ \\mathbf{\\beta_1} \\end{matrix}\\right) = \\left(\\begin{matrix} intercept  \\\\ slope \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = xtx_inv @ xty\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3c992",
   "metadata": {},
   "source": [
    "The resulting regression equation:\n",
    "$Height = 56.778 + 10.203 \\times Age$\n",
    "\n",
    "*Interpretation*:\n",
    "- Intercept (56.778 cm): Expected height when $age = 0$ (theoretical baseline)\n",
    "- Slope (10.203 cm/year): For each additional year of age, height increases by about 10.2 cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f6d13",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE) vs OLS\n",
    "<a id=\"mle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac2a9b",
   "metadata": {},
   "source": [
    "Under the assumption that errors are normally distributed, MLE and OLS yield identical results. The key insight is that minimizing squared errors is equivalent to maximizing the likelihood of observing the data.\n",
    "\n",
    "Our linear regression model has the following form: $y = X\\beta + \\epsilon$. This is a deterministic relationship plus an error term. To make distributional assumptions about the error term Œµ to enable statistical inference.\n",
    "\n",
    "Assume errors are independent and normally distributed with zero mean and constant variance $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I)$, and are independent from $X$. \n",
    "\n",
    "According to the linear transformation property, if $Z \\sim \\mathcal{N}(\\mu, \\Sigma)$ then $aX + b \\sim \\mathcal{N}(a\\mu + b, a^2\\Sigma)$. In our case, since $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I)$, then $y = X\\beta + \\epsilon \\sim \\mathcal{N}(0 + X\\beta, \\sigma^2I) = \\mathcal{N}(X\\beta, \\sigma^2I)$. This is true if we consider that $X$ is fixed and non-random, and is therefore treated like experimental conditions (*classical regression theory*). In this case, using our toy example, we would say \"*Suppose we fix the ages at [1.0, 1.5, 2.0, ..., 5.0]. For these fixed ages, the heights vary randomly according to: $y \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$*\".\n",
    "\n",
    "If we want to emphasise that observational data $X$ and $y$ are both random (*modern regression theory*), we rather use conditional notation: $y|X = X\\beta + \\epsilon|X = X\\beta + \\epsilon \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$ which means \"*Given that we observe specific values of $X$, the distribution of $y$ follows this normal distribution*\". In terms of our example, we would say \"*In practice, both age and height are random variables. But given that we observe specific ages, the conditional distribution of heights is:* $y|X \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$\".\n",
    "\n",
    "Since $y|X \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$, each individual observation follows: $y_i|x_i \\sim \\mathcal{N}(x_i^T\\beta, \\sigma^2)$, where:\n",
    "* $x_i$ is the i-th row of $X$ (i-th observation's features)\n",
    "* $x_i^T\\beta = \\beta_0 + \\beta_1x_1 + ... + \\beta_px_p = \\mu_i$ is the expected value. Let's denote it with $\\mu_i$.\n",
    "* $\\sigma^2$ is the variance (same for all observations). \n",
    "\n",
    "Thus, for a single observation $y_i$, the probability density function (PDF) is given by:\n",
    "$$f(y_i|x_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}}$$\n",
    "\n",
    "Assume observations are independent given $X$. Under this key assumption, the joint density is the product of individual densities: \n",
    "$$f(y_1y_2...y_m|X, \\beta, \\sigma^2) = f(y_1|x_1, \\beta, \\sigma^2) \\times f(y_2|x_2, \\beta, \\sigma^2) \\times ... \\times f(y_n|x_n, \\beta, \\sigma^2) = \\prod_{i=1}^n(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}})$$\n",
    "\n",
    "Now, considering our age-height data, the likelihood can be expressed as: \"*Given these specific ages, what's the probability of observing these specific heights for different values of $\\beta$ and $\\sigma^2$*\". So:\n",
    "\n",
    "$$L(\\beta, \\sigma^2) = f(y|X, \\beta, \\sigma^2) = \\prod_{i=1}^n(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}})$$\n",
    "\n",
    "We can factor out the constants:\n",
    "$$L(\\beta, \\sigma^2) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\\prod_{i=1}^n(e^{-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}})$$\n",
    "\n",
    "$$L(\\beta, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}e^{\\sum_{i=1}^n\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)} = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}e^{\\left(\\frac{-\\sum_{i=1}^n(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)} = \\left(2\\pi\\sigma^2\\right)^{-n/2}e^{\\left(\\frac{-\\sum_{i=1}^n(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)}$$\n",
    "\n",
    "Recalling that the sum of squared error is given by $SSE = \\sum_{i=1}^n (y_i = x_i^T\\beta)^2$, we obtain: \n",
    "\n",
    "$$L(\\beta, \\sigma^2) = \\left(2\\pi\\sigma^2\\right)^{-n/2}e^{\\left(\\frac{-SSE}{2\\sigma^2}\\right)}$$\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\\mathcal{l}(\\beta, \\sigma^2) = \\ln L(\\beta, \\sigma^2) = \\ln\\left(\\left(2\\pi\\sigma^2\\right)^{-n/2}\\right) + \\ln(e^{\\left(\\frac{-SSE}{2\\sigma^2}\\right)}) =$$\n",
    "$$= -n/2 \\ln(2\\pi\\sigma^2) + \\left(\\frac{-SSE}{2\\sigma^2}\\right) = -n/2\\ln(2\\pi) - n/2\\ln(\\sigma^2) - \\frac{SSE}{2\\sigma^2}$$\n",
    "$$= -n/2\\ln(2\\pi) - n\\ln(\\sigma) - \\frac{SSE}{2\\sigma^2}$$\n",
    "\n",
    "To find the MLE, we maximize $\\mathcal{l}(\\beta, \\sigma^2)$ with respect to $\\beta$ and $\\sigma$:\n",
    "\n",
    "$$\\left\\{\\begin{aligned}\\frac{\\partial\\mathcal{l}}{\\partial\\beta} = 0 \\\\\\frac{\\partial\\mathcal{l}}{\\partial\\sigma} = 0\\end{aligned}\\right.$$\n",
    "\n",
    "Since the only term containing $\\beta$ is $- \\frac{SSE}{2\\sigma^2}$:\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{l}}{\\partial\\beta} = \\frac{\\partial}{\\partial \\beta}\\left(- \\frac{SSE}{2\\sigma^2}\\right) = -\\frac{1}{2\\sigma^2}\\frac{\\partial SSE}{\\partial\\beta}$$\n",
    "\n",
    "Since $SSE = (y-X\\beta)^T(y-X\\beta)$:\n",
    "$$\\frac{\\partial SSE}{\\partial\\beta} = -2X^T(y - X\\beta)$$\n",
    "\n",
    "Then:\n",
    "$$\\frac{\\partial\\mathcal{l}}{\\partial\\beta} = -\\frac{1}{2\\sigma^2}(-2X^T(y - X\\beta)) = 0$$\n",
    "\n",
    "$$\\frac{X^T(y - X\\beta)}{\\sigma^2} = 0$$\n",
    "\n",
    "$$X^T(y - X\\beta) = 0$$\n",
    "\n",
    "$$X^Ty = X^TX\\beta$$\n",
    "\n",
    "$$\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "which is exactly the OLS estimator ($\\hat{\\beta}_{MLE} = \\hat{\\beta}_{OLS}$).\n",
    "\n",
    "**To sum up**: Under normal error assumptions, maximizing likelihood is equivalent to minimizing sum of squared errors.\n",
    "From the log-likelihood: $\\mathcal{l}(\\beta, \\sigma^2) = -n/2 log(2\\pi\\sigma^2) - SSE/(2\\sigma^2)$\n",
    "\n",
    "For fixed $\\sigma^2$, maximizing $\\mathcal{l}$ is equivalent to minimizing SSE, since:\n",
    "- The first term doesn't depend on $\\beta$\n",
    "- The second term decreases as $SSE$ decreases\n",
    "\n",
    "For $\\sigma$, maximising the log-likelihood gives:\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{l}}{\\partial\\sigma} = -\\frac{n}{\\sigma} + \\frac{SSE}{\\sigma^3} = 0$$\n",
    "$$\\frac{1}{\\sigma}\\left(\\frac{SSE}{\\sigma^2} - n\\right) = 0$$\n",
    "$$\\frac{SSE}{\\sigma^2} - n = 0$$\n",
    "$$\\frac{SSE}{\\sigma^2} = n$$\n",
    "$$\\sigma^2 = \\frac{SSE}{n}$$\n",
    "$$\\hat{\\sigma}_{MLE} = \\sqrt{\\frac{SSE}{n}}$$\n",
    "\n",
    "This differs slightly from the unbiased OLS estimator: $\\hat{\\sigma}_{OLS} = \\sqrt{SSE/(n-p)}$ where $p$ is the number of parameters, i.e. the estimators differ by degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15133e6a",
   "metadata": {},
   "source": [
    "Let's verify that with our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bde1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS solution\n",
    "X = np.column_stack([np.ones(len(data_x)), data_x])\n",
    "beta_ols = np.linalg.inv(X.T @ X) @ (X.T @ data_y)\n",
    "# Calculate residuals and unbiased sigma\n",
    "y_pred = X @ beta_ols\n",
    "residuals = data_y - y_pred\n",
    "\n",
    "n = len(data_y)\n",
    "p = X.shape[1]\n",
    "sse = np.sum(residuals**2)\n",
    "# unbiased estimator\n",
    "sigma_ols = np.sqrt(sse / (n - p))\n",
    "print(\"OLS results\")\n",
    "print(f\"intercept: {beta_ols[0]:.4f}\")\n",
    "print(f\"scope: {beta_ols[1]:.4f}\")\n",
    "print(f\"sigma: {sigma_ols:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b647307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a95e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE solution\n",
    "def negative_log_likelihood(params, X: np.array, y: np.array):\n",
    "    \"\"\"Negative log-likelihood for linear regression with normal errors\n",
    "\n",
    "    Model: y_i = Œ≤‚ÇÄ + Œ≤‚ÇÅ * x_i + Œµ_i, where Œµ_i ~ N(0, œÉ¬≤)\n",
    "    \n",
    "    Log-likelihood: ‚Ñì(Œ≤‚ÇÄ, Œ≤‚ÇÅ, œÉ) = Œ£·µ¢ log(œÜ((y_i - Œ≤‚ÇÄ - Œ≤‚ÇÅ*x_i)/œÉ)) - n*log(œÉ)\n",
    "    where œÜ is the standard normal PDF\n",
    "    \n",
    "    Args:\n",
    "        params: values of parameters beta and sigma\n",
    "        X (np.array): features X\n",
    "        y (np.array): vector y\n",
    "\n",
    "    Returns:\n",
    "        negative log-likelihood\n",
    "    \"\"\"\n",
    "    beta0, beta1, log_sigma = params\n",
    "    \n",
    "    sigma = np.exp(log_sigma)\n",
    "    # predicted values\n",
    "    predictions = beta0 + beta1 * X\n",
    "    # residuals\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Log-likelihood calculation\n",
    "    # For normal distribution: log(L) = -n/2 * log(2œÄ) - n*log(œÉ) - 1/(2œÉ¬≤) * Œ£(residuals¬≤)\n",
    "    n = len(y)\n",
    "    log_likelihood = (-n/2 * np.log(2*np.pi) \n",
    "                     - n * np.log(sigma) \n",
    "                     - np.sum(residuals**2) / (2 * sigma**2))\n",
    "    \n",
    "    \n",
    "    # Negative log-likelihood to minimize\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OLS as initial guess\n",
    "# initial_guess = [beta_ols[0], beta_ols[1], np.log(sigma_ols)]\n",
    "initial_guess = [60, 10, 5]\n",
    "# Minimize negative log-likelihood\n",
    "result = minimize(negative_log_likelihood, initial_guess, args=(data_x, data_y), \n",
    "                     method='BFGS', options={'disp': False})\n",
    "\n",
    "beta_0_mle, beta_1_mle, _ = result.x\n",
    "\n",
    "# prediction MLE \n",
    "y_pred_mle = beta_0_mle + beta_1_mle * data_x \n",
    "residuals_mle = data_y - y_pred_mle \n",
    "sigma_mle = np.sqrt(np.mean(residuals_mle**2))\n",
    "\n",
    "print(\"MLE results\")\n",
    "print(f\"intercept: {beta_0_mle:.4f}\")\n",
    "print(f\"slope: {beta_1_mle:.4f}\")\n",
    "print(f\"sigma: {sigma_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf39987",
   "metadata": {},
   "source": [
    "Note that the intercept and the slope are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd0755",
   "metadata": {},
   "source": [
    "## Confidence Intervals for Coefficients\n",
    "<a id=\"confidence-intervals\"></a>\n",
    "\n",
    "**Confidence intervals** provide a range of plausible values for regression coefficients, expressing uncertainty in our parameter estimates. The coefficients are then given by: $$\\hat{\\beta}_i \\pm t(\\alpha/2, n-p-1) * SE(\\hat{\\beta}_i)$$\n",
    "where:\n",
    "- $\\hat{\\beta}_i$ denotes our estimated coefficients;\n",
    "- $t(\\alpha/2, n-p-1)$ denotes the critical $t$-value with $(n-p-1)$ degrees of freedom;\n",
    "- $SE(\\hat{\\beta}_i) = \\sqrt{\\sigma^2 diag((X'X)^{-1})}$ denotes standard error of coefficient and where $\\sigma^2 = SSE / (n-p-1)$ is the estimated error variance.\n",
    "\n",
    "*Interpretation*: If we repeated this study many times, 95% of such intervals would contain the true coefficient value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients with confidence intervals\n",
    "coeffs, se, ci_low, ci_high = calculate_confidence_intervals(data_x, data_y, confidence_level=0.95)\n",
    "print(f\"Intercept: {coeffs[0]:.3f} ¬± {se[0]:.3f}\")\n",
    "print(f\"Slope: {coeffs[1]:.3f} ¬± {se[1]:.3f}\")\n",
    "print(f\"95% CI for slope: [{ci_low[1]:.3f}, {ci_high[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f796974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "coeffs, sigma, r2 = plot_regression_with_intervals(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecc578",
   "metadata": {},
   "source": [
    "On the above plot, we can interpret the red shaded area as follows: \"We are 95% confident that the average height for children of age $x$ falls within this band\". This area represents uncertainty in the fitted line itself.\n",
    "\n",
    "Note that the area is narrower around the center of data (where $\\bar{x}$ is).\n",
    "\n",
    "As for the *prediction interval* (gray shaded area), we can interpret it as follows: \"We are 95% confident that a single new observation at age $x$ will fall within this band\". \n",
    "\n",
    "Note that it is always wider than confidence interval and accounts for both model uncertainty AND individual variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4127a1",
   "metadata": {},
   "source": [
    "### $R^2$ ($R$-squared)\n",
    "<a id=\"r-squared\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c480efb",
   "metadata": {},
   "source": [
    "> How much better my regression line is than another one? How much better is my regression line compared to just guessing the average every time?\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "In terms of our toy example, let's say that we are trying to predict children's heights, but we know nothing about their ages. Our best strategy would be to always guess the average height (say, 85 cm). We'd be wrong most of the time, with some kids much taller and others much shorter than your guess.\n",
    "\n",
    "Now, if we ass age into our model to predict height, we would like to know what fraction of our \"wrongness\" we've eliminated by using the regression instead of just guessing the average.\n",
    "\n",
    "A measure that can help us with this issue is called **coefficient of determination**, $R^2$:\n",
    "- $R^2 = 0$: Your regression line is no better than always guessing the average\n",
    "- $R^2 = 0.7$: Your regression eliminates 70% of the error you'd make by always guessing the average\n",
    "- $R^2 = 1$: Perfect prediction - your line passes through every data point exactly\n",
    "\n",
    "$R^2$ can be understood as the percentage of variation explained: If height varies wildly when you ignore age, but becomes much more predictable when you account for age, then $R^2$ will be high. \n",
    "\n",
    "<strong>Coefficient of determination</strong> $R^2$ is defined as: $$R^2 = 1 - \\frac{SSE}{SST}$$ \n",
    "where: \n",
    "- $SSE$ denotes the sum of squared errors and is given by: $SSE = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$. It measures how much variation remains unexplained after fitting your regression line. This is the residual error your model still makes.\n",
    "- $SST$ denotes the total sum of squares and is given by: $SST = \\sum_{i=1}^n(y_i - \\bar{y})^2$. It measures the total variation in the data around the mean. This represents how much the data would \"spread out\" if you knew nothing except the average.\n",
    "- $\\frac{SSE}{SST}$ this ratio tells you what proportion of the original variation you failed to explain with your model.\n",
    "\n",
    "An alternative formulation is: $R^2 = \\frac{SSR}{SST}$ where $SSR = SST - SSE$ denotes the sum of squares due to regression.\n",
    "\n",
    "$R^2$ ranges between 0 and 1 (or 0-100%).\n",
    "\n",
    "*Warning!*: High $R^2$ doesn't imply causation - it only measures linear association.\n",
    "\n",
    "Note that $R^2$ is sample dependent. It tends to increase as you add more variables, even if they're irrelevant (leading to adjusted $R^2$ for multiple regression).\n",
    "\n",
    "$R^2$ equals the square of the correlation coefficient ($r$). This connects the geometric notion of how tightly data clusters around a line (correlation) with the variance-explained interpretation ($R^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4da27",
   "metadata": {},
   "source": [
    "### Residual Plot\n",
    "<a id=\"residual-plot\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360fcaa",
   "metadata": {},
   "source": [
    "If your model is correctly specified, residuals should look like pure random noise. Any patterns in residuals indicate model inadequacy. This is because: if $y = f(x) + \\epsilon$ where $\\epsilon \\sim \\mathcal(0, \\sigma^2)$ then $residuals = y - \\hat{y} \\approx \\epsilon$ (random noise). Patterns in residuals imply that the function $f(x)$ is not linear.\n",
    "\n",
    "Residual plots are the primary diagnostic tool for regression. It plots residuals ($\\hat{e} = y - \\hat{y}$) against fitted values or predictor variables. It allows to check regression assumptions:\n",
    "\n",
    "- Linearity: Residuals should be randomly scattered\n",
    "- Homoscedasticity: Constant variance across fitted values\n",
    "- Independence: No patterns in residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88991f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 0: Data that respects the assumptions\n",
    "np.random.seed(42) # fix random seed to reproducibility\n",
    "\n",
    "ex0_x = np.random.rand(400) * 10\n",
    "ex0_error = np.random.rand(400) * 2\n",
    "ex0_y_true = 2 * ex0_x + 1 + ex0_error\n",
    "\n",
    "# visualisation\n",
    "create_static_visualization(ex0_x, ex0_y_true, x_label_text=\"x\", y_label_text=\"y = 2x + 1 + eps\")\n",
    "print(\"Random scatter around zero. No visible patterns of residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4eee2",
   "metadata": {},
   "source": [
    "### MSE and RMSE\n",
    "<a id=\"mse-rmse\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd93a9",
   "metadata": {},
   "source": [
    "We would like to further assess the quality of our predictions. \n",
    "\n",
    "> How wrong are our predictions, on average?\n",
    "\n",
    "*Intuition:* \n",
    "\n",
    "You are making a prediction of the height given the age = 3:\n",
    "- True height: 84.00\n",
    "- Predicted height: 87.147\n",
    "- Error: 3.147\n",
    "\n",
    "Now, imagine that you make many such predictions. We are interested in summarising the typical magnitude of the errors across all predictions. \n",
    "\n",
    "Two measures can be used: **MSE** (Mean Squared Error) and **RMSE** (Root Mean Squared Error)\n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^n(y_i - \\hat{y_i})^2}{n} = \\frac{\\sum_{i=1}^n e_i^2}{n}$$\n",
    "where:\n",
    "- $n$ is the number of observations\n",
    "- $y_i$ is the true value for observation $i$\n",
    "- $\\hat{y_i}$ is the predicted value for observation $i$\n",
    "- $e_i = (y_i - \\hat{y_i})$ is residual (prediction error) for observation $i$\n",
    "\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y_i})^2}{n}}$$\n",
    "\n",
    "Note that one of the advantages to use of squared errors is that this penalises large errors more heavily or in other words, quadratic penalty makes the model focus on reducing large mistakes. Example: error of 2 contributes 4 to MSE, while error of 4 contributes 16 to MSE (i.e. 4x(the penalty for 2)).\n",
    "\n",
    "| | MSE | RMSE |\n",
    "|--|:----:|:------:|\n",
    "| Formula | $$MSE = \\frac{\\sum_{i=1}^n(y_i - \\hat{y_i})^2}{n} = \\frac{\\sum_{i=1}^n e_i^2}{n}$$ |  $$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y_i})^2}{n}}$$| \n",
    "| Units | squared units of the target variable | same units as the target variable|\n",
    "| Interpretation | $\\Rightarrow$ direct interpretation is challenging | standard deviation of prediction errors </br>(e.g. RMSE = $15,000 means \"*typical prediction error is about $15,000*\")</br> Roughly 68% of predictions are within ¬±1 RMSE of true values (assuming normal error distribution)| \n",
    "| Range | $MSE \\geq 0$ | $RMSE \\geq 0$ |\n",
    "|Perfect prediction | $MSE =0$ | $RMSE = 0$|\n",
    "| Scale sensitivity | increases quadratically with the scale of data | increases linearly with the scale of data|\n",
    "| Sensitivity to outliers | yes | yes |\n",
    "|When to use | - Mathematical derivations: MSE has nicer mathematical properties </br> - Optimization: Algorithms often minimize MSE directly </br> - Comparing model complexity: In regularization, we add penalties to MSE </br> - Computational efficiency: Avoiding square root calculation | - Reporting results: RMSE is more interpretable </br> - Setting expectations: \"Typical error is about X units\" </br> - Comparing models on the same scale: RMSE maintains original units </br> - Communicating with non-technical stakeholders|\n",
    "\n",
    "Note: MSE and RMSE are **necessary but not sufficient** for model evaluation. Always complement them with residual analysis, validation curves, and domain-specific metrics to get the complete picture of model performance.\n",
    "\n",
    "When it comes to **expected error over all possible training sets**, in other words expected MSE over all possible training sets $\\mathrm{E}[(y_0 - \\hat{f}(x_0))^2]$, it can be represented as $\\mathrm{E}[(y_0 - \\hat{f}(x_0))^2] = Bias^2 + Variance + Noise$, i.e. using *bias-variance decomposition*. Let's demonstrate that.\n",
    "\n",
    "Let $y = f(x) + \\epsilon$ be a true function with $\\epsilon \\sim \\mathcal{N(0, \\sigma^2)}$. Let $\\hat{f}(x)$ be an estimator trained on dataset $D$. And let $(x_0, y_0)$ where $y_0 = f(x_0) + \\epsilon$ be a new point (out of sample $D$). What's the expected squared error $\\mathrm{E}[(y_0 - \\hat{f}(x_0))^2]$?\n",
    "\n",
    "1. Substitute $y_0 = f(x_0) + \\epsilon_0$:\n",
    "$$\\mathrm{E}[(y_0 - \\hat{f}(x_0))^2] = \\mathrm{E}[(f(x_0) + \\epsilon_0 - \\hat{f}(x_0))^2]$$\n",
    "\n",
    "2.  Add and substract $\\mathrm{E}[\\hat{f}(x_0)]$ (the expected prediction):\n",
    "$$= \\mathrm{E}[(f(x_0) + \\epsilon_0 - \\mathrm{E}[\\hat{f}(x_0)] + \\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0))^2]$$\n",
    "\n",
    "3. Rearrange terms\n",
    "$$= \\mathrm{E}[(\\underbrace{(f(x_0) - \\mathrm{E}[\\hat{f}(x_0)])}_{a} + \\underbrace{(\\epsilon_0)}_{b}  + \\underbrace{(\\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0))}_{c})^2]$$\n",
    "\n",
    "4. Expand the square $(a + b + c)^2 = a^2 + b^2 + c^2 + 2ab + 2ac + 2bc$\n",
    "$$= \\mathrm{E}[((f(x_0) - \\mathrm{E}[\\hat{f}(x_0)]))^2] + \\mathrm{E}[(\\epsilon_0)^2] + \\mathrm{E}[(\\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0))^2] + \\text{cross\\_terms}$$\n",
    "\n",
    "5. Evaluate cross terms\n",
    "- $\\mathrm{E}[\\underbrace{\\epsilon_0}_{noise}\\cdot\\underbrace{(f(x_0) - \\mathrm{E}[\\hat{f}(x_0)])}_{bias}] = 0$ as noise is independent of deterministic bias\n",
    "- $\\mathrm{E}[\\underbrace{\\epsilon_0}_{noise}\\cdot\\underbrace{(\\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0))}_{\\text{deviation from expected prediction}}] = \\mathrm{E}[\\epsilon_0]\\cdot\\mathrm{E}[\\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0)] = 0$ as test point noise $\\epsilon_0$ is independent of training data randomness and $\\mathrm{\\epsilon_0} = 0 \\Rightarrow \\mathrm{E}[\\epsilon_0 \\cdot \\text{anything\\_independent\\_of\\_}\\epsilon_0] = 0$ \n",
    "- $\\mathrm{E}[\\underbrace{(f(x_0) - \\mathrm{E}[\\hat{f}(x_0)])}_{bias}\\cdot\\underbrace{(\\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0))}_{\\text{deviation from expected prediction}}] = 0$ as bias is independent of the \"variability part\" that becomes the variance when squared\n",
    "\n",
    "6. Final decomposition\n",
    "$$= \\mathrm{E}[((f(x_0) - \\mathrm{E}[\\hat{f}(x_0)]))^2] + \\sigma^2 + \\mathrm{E}[(\\mathrm{E}[\\hat{f}(x_0)] - \\hat{f}(x_0))^2] + 0 =$$\n",
    "$$= Bias^2(\\hat{f}(x_0)) + \\text{Irreducible Error} + Variance(\\hat{f}(x_0))$$\n",
    "\n",
    "Practical Meaning:\n",
    "\n",
    "- Bias¬≤: How far off our average prediction is from truth\n",
    "- Variance: How much our predictions vary across different training sets\n",
    "- Irreducible Error: Noise that can't be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ad617",
   "metadata": {},
   "source": [
    "## Linear Regression from Scratch\n",
    "<a id=\"linreg-from-scratch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930d80d",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 2:</h5> Write linear regression model in general form from scratch using Python. The function should return estimated coefficients: Œ≤ = (X'X)^-1 X'y. Test on our toy example.\n",
    "\n",
    "```\n",
    "def linear_regression_scratch(X: np.array, y: np.array) -> np.array:\n",
    "    \"\"\"Implements linear regression model using OLS and returns the coefficients.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): array of observed feature values\n",
    "        y (np.array): array of observed outcomes\n",
    "\n",
    "    Returns:\n",
    "        np.array: estimated coefficients\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Some hints:\n",
    "- Mind to add the intercept column to X. You can initialise it with ones. To do so, you can use [numpy.column_stack()](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html)\n",
    "- For matrix multiplication, you can use a dedicated operator [`@`](https://peps.python.org/pep-0465/).\n",
    "- To transpose a matrix, you can use the property [matrix.T](https://numpy.org/doc/2.3/reference/generated/numpy.matrix.T.html). Alternatively, you can use [numpy.transpose()](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)\n",
    "- To invert a matrix, you can use [numpy.linalg.inv()](https://numpy.org/doc/2.0/reference/generated/numpy.linalg.inv.html) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def linear_regression_scratch(X: np.array, y: np.array) -> np.array:\n",
    "    \"\"\"Implements linear regression model using OLS and returns the coefficients.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): array of observed feature values\n",
    "        y (np.array): array of observed outcomes\n",
    "\n",
    "    Returns:\n",
    "        np.array: estimated coefficients\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# test on the toy data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4761180",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 3:</h5> Calculate predicted values for our toy example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16751197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ff67c",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 4:</h5> Write a function that calculates the residuals and plots the Residual Plot. Apply the function to our toy example.\n",
    "\n",
    "```\n",
    "def calculate_residuals(y_pred: np.array, x: np.array, y_true: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculates residuals of the model and plot Residual Plot.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (np.array): predicted values\n",
    "        x (np.array): array of observed feature values\n",
    "        y_true (np.array): array of observed outcomes\n",
    "\n",
    "    Returns:\n",
    "        np.array: residuals (error = y_true - y_pred)\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER \n",
    "def calculate_residuals(y_pred: np.array, x: np.array, y_true: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculates residuals of the model and plot Residual Plot.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (np.array): predicted values\n",
    "        x (np.array): array of observed feature values\n",
    "        y_true (np.array): array of observed outcomes\n",
    "\n",
    "    Returns:\n",
    "        np.array: residuals (error = y_true - y_pred)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7c491",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 5:</h5> Write a function that calculates SSE, MSE, RNSE and R^2. Test on our toy example.\n",
    "\n",
    "```\n",
    "def calculate_perf_stats(y_observed: np.array, y_pred: np.array) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Calculates statistics (R-squared, MSE, RMSE, SSE) given observed and predicted values.\n",
    "\n",
    "    Args:\n",
    "        y_observed (np.array): observed values\n",
    "        y_pred (np.array): predicted values\n",
    "    \n",
    "    Return:\n",
    "        R-squared, MSE, RMSE, SSE\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def calculate_perf_stats(y_observed: np.array, y_pred: np.array) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Calculates statistics (R-squared, MSE, RMSE, SSE) given observed and predicted values.\n",
    "\n",
    "    Args:\n",
    "        y_observed (np.array): observed values\n",
    "        y_pred (np.array): predicted values\n",
    "    \n",
    "    Return:\n",
    "        R-squared, MSE, RMSE, SSE\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3b21f",
   "metadata": {},
   "source": [
    "## Linear Regression in Python\n",
    "<a id=\"linreg-python\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137d620",
   "metadata": {},
   "source": [
    "### Using `scipy.stats.lingress`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196278a",
   "metadata": {},
   "source": [
    "The first option consists in using [`scipy.stats.lingress`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) function that calculates a linear least-squares regression for two sets of measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d285749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress # linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression \n",
    "slope, intercept, r_value, p_value, std_err = linregress(data_x, data_y)\n",
    "\n",
    "# create a regression line\n",
    "reg_line = slope * data_x + intercept\n",
    "\n",
    "# print the results\n",
    "print(f\"Regression line: regression_line = {slope} * x + {intercept}\")\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"R-value (Pearson correlation coefficient): {r_value}\")\n",
    "print(f\"R-squared (determination coefficient): {r_value**2}\")\n",
    "print(f\"Standard error of the estimated slope (gradient): {std_err}\")\n",
    "print(f\"p-value (H0: the slope is zero): {p_value}\")\n",
    "\n",
    "# show the data with the regression line\n",
    "plt.scatter(data_x, data_y, label='Observed data')\n",
    "plt.plot(data_x, reg_line, color='red', label='Regression line')\n",
    "plt.legend()\n",
    "plt.title('Linear regression with scipy.stats.lingress')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f31f4",
   "metadata": {},
   "source": [
    "### Using `sklearn.linear_model.LinearRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ce430",
   "metadata": {},
   "source": [
    "Alternatively, we can use [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) that fits a linear model with coefficients w = (w1, ‚Ä¶, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a715a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe82de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model object\n",
    "lin_model = LinearRegression()\n",
    "# fit the data\n",
    "lin_model.fit(data_x.reshape(-1, 1), data_y)\n",
    "# display the parameters of the model\n",
    "slope_sk = lin_model.coef_\n",
    "intercept_sk = lin_model.intercept_\n",
    "r_squared_sk = lin_model.score(data_x.reshape(-1, 1), data_y)\n",
    "print(f\"Linear regression equation: f_sk = {slope_sk[0]}age + {intercept_sk}\")\n",
    "print(f\"Determination coefficient: {r_squared_sk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b15b7",
   "metadata": {},
   "source": [
    "To predict the value using this model, we can use `predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08165d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "lin_model.predict(np.array([[2.2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365906e",
   "metadata": {},
   "source": [
    "## When Regression Breaks Down\n",
    "<a id=\"breaks-down\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdb554",
   "metadata": {},
   "source": [
    "Regression model breaks down when the statistical assumptions are violated. \n",
    "1. Linearity violation\n",
    "2. Independence violation\n",
    "3. Homoscedasticity violation\n",
    "4. Normality violation\n",
    "\n",
    "Let's explore that with examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02588d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 0: Data that respects the assumptions\n",
    "np.random.seed(42) # fix random seed to reproducibility\n",
    "\n",
    "ex0_x = np.random.rand(400) * 10\n",
    "ex0_error = np.random.rand(400) * 2\n",
    "ex0_y_true = 2 * ex0_x + 1 + ex0_error\n",
    "\n",
    "# visualisation\n",
    "create_static_visualization(ex0_x, ex0_y_true, x_label_text=\"x\", y_label_text=\"y = 2x + 1 + eps\")\n",
    "print(\"Random scatter around zero. No visible patterns of residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f193ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 (Linearity Violation): Quadratic relationship fitted with linear model\n",
    "ex1_x = np.linspace(0, 10, 100)\n",
    "ex1_y = ex1_x**2 + np.random.normal(0, 5, 100)\n",
    "\n",
    "# visualisation\n",
    "create_static_visualization(ex1_x, ex1_y, x_label_text=\"x\", y_label_text=\"y = x^2 + eps\")\n",
    "print(\"We can see a curved residual pattern shown by this linear fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41be9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 (Independence Violation): Time series with autocorrelation\n",
    "ex2_t = np.arange(400)\n",
    "ex2_error = np.cumsum(np.random.normal(0, 1, 400))  # Autocorrelated errors\n",
    "ex2_y = 2 + 3*ex2_t + ex2_error\n",
    "\n",
    "# visualisation\n",
    "create_static_visualization(ex2_t, ex2_y, x_label_text=\"t\", y_label_text=\"y = 2 + 3*t + autocor.error\")\n",
    "print(\"Residuals will show patterns over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3 (Homoscedasticity Violation): Variance increases with x\n",
    "ex3_x = np.linspace(1, 10, 400)\n",
    "ex3_y = 2 + 3*ex3_x + np.random.normal(0, ex3_x, 400)  # Variance proportional to X\n",
    "\n",
    "# visualisation\n",
    "create_static_visualization(ex3_x, ex3_y, x_label_text=\"x\", y_label_text=\"y = 2 + 3*x + error.increass.var\")\n",
    "print(\"Residual plot  shows fan shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4 (Normality Violation): Heavy-tailed errors\n",
    "ex4_x = np.linspace(0, 10, 400)\n",
    "ex4_y = 2 + 3*ex4_x + np.random.laplace(0, 1, 400)  # Laplace distribution (heavy tails)\n",
    "ex5_y = 2 + 3*ex4_x + (np.random.exponential(2, 400) - 2)  # exponential distribution (right-skewed)\n",
    "\n",
    "create_static_visualization(ex4_x, ex4_y, x_label_text=\"x\", y_label_text=\"y = 2 + 3*x + laplace.error\")\n",
    "print(\"In comparison to normal errors, errors following Laplace distribution create a wider spread pattern in residual plot\")\n",
    "\n",
    "create_static_visualization(ex4_x, ex5_y, x_label_text=\"x\", y_label_text=\"y = 2 + 3*x + exp.error\")\n",
    "print(\"In comparison to normal errors, errors following exponential distribution (right-skewed) create more residuals on one side of zero than the other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479044b",
   "metadata": {},
   "source": [
    "A further analysis with *Q-Q plot* (Quantile-Quantile plot) can be performed to check normality violations. \n",
    "\n",
    "Q-Q plots (Quantile-Quantile plots) compare the quantiles of your data against the quantiles of a theoretical distribution (usually normal).\n",
    "\n",
    "**Interpretation:** \n",
    "\n",
    "On X-axis, there are theoretical quantiles (what you'd expect from a normal distribution) and on Y-axis, there are sample quantiles (actual values from your data, ordered). A reference line (perfect normal distribution) is also usually plotted. \n",
    "\n",
    "In case of \"perfect normality\", the points lie exactly on the diagonal reference line, which means that data quantiles match theoretical normal quantiles perfectly. Note that random scatter around the line is acceptable.\n",
    "\n",
    "> How to use it for decision making for regression?\n",
    "\n",
    "- Acceptable: Points roughly follow line with random scatter\n",
    "- Borderline: Slight systematic deviations but overall linear trend\n",
    "- Problematic: Strong S-curves (points curve away or toward line at ends), severe skewness (points below line on left, above on right), or multiple modes (steps or multiple curves)\n",
    "\n",
    "One way to plot it is using [`scipy.stats.probplot`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot # quantiles for a probability plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4_residuals = (2 + 3*ex4_x) - ex4_y\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "probplot(ex4_residuals, dist=\"norm\", plot=ax) # plot parameter should be set for visualisation\n",
    "print(\"Q-Q plot shows deviations from normal line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d2749",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "<a id=\"polynomial-features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b893ce7",
   "metadata": {},
   "source": [
    "By now, we have explored linear terms. But sometimes, it's not enough. \n",
    "\n",
    "> Why do we need non-linear terms?\n",
    "\n",
    "- Real relationships are rarely linear\n",
    "- Linear models underfit curved data\n",
    "- Poor predictions and $R^2$\n",
    "\n",
    "Let's consider the following examples revealing the limitations of linear terms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems revealed\n",
    "fig_scenarios, mod_res = get_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba61fab",
   "metadata": {},
   "source": [
    "To overcome these limitations, we can use **polynomial terms**: $x, x^2, x^3$. Our model then becomes: $y = \\beta_0 + \\beta_1 x^1 + \\beta_2 x^2 + ... \\beta_p x^p$. Note that the model is still \"linear\" in parameters. As a general rule, the use of higher degree polynomials imply a better fit on the seen (train) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial solution\n",
    "fig_poly = get_poly_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6de0f",
   "metadata": {},
   "source": [
    "But there is a high risk of **overfitting**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit example\n",
    "get_overfit_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e43c4",
   "metadata": {},
   "source": [
    "We may note that training scores, both $R^2$ and $MSE$ improve with the increase of the polynomial degree. However, this performance is not maintained on the test data, demonstrating bad generalisation. \n",
    "\n",
    "On the above plot, we don't really evaluate and show variance (how much predictions change across different training sets) evolution as it requires multiple training sets.\n",
    "\n",
    "Or consider the following example: \n",
    "Our true underlying function that we want to learn is $y = 1.5 * x^2 + 0.5 * x + 0.3$. See a green line on the top left plot. We generate a small dataset of 30 samples (observations) by adding some noise which follows Normal distribution.\n",
    "\n",
    "![](img/overfitting.png)\n",
    "\n",
    "As a general rule, we talk about underfitting/overfitting in the following contexts:\n",
    "- High bias, low variance $\\Rightarrow$ underfitting (e.g. degree 1)\n",
    "- Low bias, low variance $\\Rightarrow$ optimal balance (e.g. degree 3-4)\n",
    "- Low bias, high variance $\\Rightarrow$ overfitting (e.g. degree 5+)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9cdd7",
   "metadata": {},
   "source": [
    "To create polynomial features in Python, you can use [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. Thus, for the input of the form `[a, b]`, the degree-2 polynomial features are `[1, a, b, a^2, ab, b^2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69049be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polynomial features of a given degree\n",
    "degree = 2\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "# transform our toy example features\n",
    "poly_features.fit_transform(data_x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd208342",
   "metadata": {},
   "source": [
    "In the example above, we note that the first column is a column of ones. It corresponds to an intercept term in a linear model. As there is only one feature in `data_x`, then the second column corresponds to the values of features themselves. And the third column corresponds to the degree-2 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e330f0",
   "metadata": {},
   "source": [
    "print(f\"\\nüéØ KEY LEARNING OBJECTIVES:\")\n",
    "print(f\"1. Understand when linear models are insufficient\")\n",
    "print(f\"2. Learn how polynomial features extend linear regression\")\n",
    "print(f\"3. Recognize and prevent overfitting with high-degree polynomials\")\n",
    "print(f\"4. See how regularization solves polynomial overfitting\")\n",
    "print(f\"5. Understand interaction terms and feature explosion\")\n",
    "\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è BIAS-VARIANCE INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Degree 1: High bias, low variance (underfitting)\")\n",
    "print(f\"‚Ä¢ Degree 2-3: Often optimal balance\")\n",
    "print(f\"‚Ä¢ Degree 5+: Low bias, high variance (overfitting)\")\n",
    "print(f\"‚Ä¢ Regularization: Manages variance while keeping flexibility\")\n",
    "\n",
    "print(f\"\\nüöÄ BRIDGE TO ADVANCED TOPICS:\")\n",
    "print(f\"‚Ä¢ Neural Networks: Learned polynomial-like features\")\n",
    "print(f\"‚Ä¢ Kernel Methods: Implicit infinite-degree polynomials\")  \n",
    "print(f\"‚Ä¢ Splines: Piecewise polynomials\")\n",
    "print(f\"‚Ä¢ Feature Engineering: Domain-specific transformations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f2c1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aabf702",
   "metadata": {},
   "source": [
    "## Regularisation in Linear Regression\n",
    "<a id=\"regularisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ecf33",
   "metadata": {},
   "source": [
    "In standard linear regression, we minimize the least squares loss:\n",
    "$L(\\beta) = ||y - X\\beta||^2$\n",
    "This works well when we have more observations than features (n > p) and our model isn't too complex. However, problems arise when:\n",
    "\n",
    "- High dimensionality: $p \\approx n$ or $p > n$ (common in AI applications)\n",
    "- Multicollinearity: Features are highly correlated\n",
    "- Limited data: Small sample sizes relative to model complexity\n",
    "- Noise: We want to prevent fitting to random fluctuations\n",
    "\n",
    "The result is **overfitting**: perfect fit to training data but poor generalization to new data.\n",
    "\n",
    "Regularization adds a penalty term to our objective function that constrains model complexity:\n",
    "$$L_{regularized}(\\beta) = ||y - X\\beta||^2 + \\lambda¬∑Penalty(\\beta)$$\n",
    "where:\n",
    "- $\\lambda$ is a regularization parameter controlling penalty strength\n",
    "- $Penalty(\\beta)$ is a function that penalizes complex models\n",
    "\n",
    "> What penalty can be used?\n",
    "\n",
    "### Ridge Regression (L2 Regularisation)\n",
    "\n",
    "Objective function: \n",
    "\n",
    "$$L_{Ridge}(\\beta) = ||y - X\\beta||^2 + \\lambda¬∑||\\beta||_2^2$$\n",
    "\n",
    "where the penalty term is given by: $||\\beta||_2^2 = \\beta_1^2 + \\beta_2^2 + ... + \\beta_p^2$\n",
    "\n",
    "Key Properties:\n",
    "\n",
    "- Shrinkage: Pulls coefficients toward zero proportionally\n",
    "- Stability: Always has a unique solution, even when X'X is singular\n",
    "- Grouping effect: Correlated features get similar coefficients\n",
    "- No feature selection: Coefficients approach zero but never become exactly zero\n",
    "\n",
    "Solution is then given by:\n",
    "\n",
    "$$\\hat{\\beta}_{Ridge} = (X'X + \\lambda I)^{-1}X'y$$\n",
    "\n",
    "*Geometric interpretation*: The penalty creates a circular constraint region around the origin. The solution occurs where the loss function contours first touch this circle.\n",
    "\n",
    "When to use:\n",
    "\n",
    "- Many relevant features\n",
    "- Features are correlated\n",
    "- Prediction accuracy is primary goal\n",
    "- Interpretability is less important\n",
    "\n",
    "### Lasso Regression (L1 Regularisation)\n",
    "\n",
    "Objective Function:\n",
    "$$L_{Lasso}(\\beta) = ||y - X\\beta||^2 + \\lambda||\\beta||_1$$\n",
    "where the penalty term is given by: $||\\beta||_1 = |\\beta_1| + |\\beta_2| + ... + |\\beta_p|$\n",
    "\n",
    "Key Properties:\n",
    "\n",
    "- Sparsity: Produces exactly zero coefficients (automatic feature selection)\n",
    "- Non-differentiable: Requires specialized optimization algorithms\n",
    "- Instability: Can arbitrarily select one feature from a group of correlated features\n",
    "- Interpretability: Simpler models with fewer features\n",
    "\n",
    "Solution is then given by:\n",
    "\n",
    "$$\\hat{\\beta}_{Lasso} = (X'X + \\lambda I)^{-1}X'y$$\n",
    "\n",
    "*Geometric interpretation*: The L1 penalty creates a diamond-shaped constraint region. The sharp corners make it likely that the optimal solution will have some coefficients exactly at zero.\n",
    "\n",
    "When to use:\n",
    "\n",
    "- Feature selection is desired\n",
    "- Believe many features are irrelevant\n",
    "- Want interpretable models\n",
    "- Features are not highly correlated \n",
    "\n",
    "### Elastic Net\n",
    "Objective Function:\n",
    "$$L_{ElasticNet}(\\beta) = ||y - X\\beta||^2 + Œª_1||\\beta||_1 + Œª_2||\\beta||_2^2$$\n",
    "\n",
    "Often parameterized as:\n",
    "$$L_{ElasticNet}(\\beta) = ||y - X\\beta||^2 + Œª[\\alpha||\\beta||_1 + (1-Œ±)||\\beta||_2^2]$$\n",
    "where $\\alpha \\in [0,1]$ controls the mix between L1 and L2 penalties.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Feature selection (from L1) + Stability (from L2)\n",
    "- Handles correlated features better than pure Lasso\n",
    "- Flexible: Can tune the balance between sparsity and grouping\n",
    "\n",
    "When to use:\n",
    "\n",
    "- High-dimensional data with correlated features\n",
    "- Want both feature selection and stability\n",
    "- Examples: Genomic data, text analysis, image processing\n",
    "\n",
    "> How bias-variance tradeoff is affected by regularisation?\n",
    "\n",
    "Regularization fundamentally manages the bias-variance tradeoff:\n",
    "\n",
    "|Underularised Regression | Regularised Regression|\n",
    "|---|---|\n",
    "|Low bias (if model is correctly specified)|Introduces bias (coefficients shrunk toward zero)|\n",
    "|High variance (especially with limited data)|Reduces variance (more stable predictions)|\n",
    "||Often lower total error: $\\mathrm{E}[(\\hat{y} - y)^2] = Bias^2 + Variance + Noise$|\n",
    "\n",
    "</br>\n",
    "\n",
    "> How to select the regularisation parameter $\\lambda$?\n",
    "\n",
    "The choice of $\\lambda$ is critical and usually done through **cross-validation**:\n",
    "1. Split data into $k$ folds\n",
    "2. For each $\\lambda$ value, train on $k-1$ folds and validate on the remaining fold\n",
    "3. Average validation error across all folds\n",
    "4. Select $\\lambda$ that minimizes average validation error\n",
    "\n",
    "To visually determine $\\lambda$, you can plot validation curves, i.e. training and validation error vs. $\\lambda$.\n",
    "\n",
    "**Practical implementation tip:** Always standardize features before applying regularization, since the penalty treats all coefficients equally: $X_{scaled} = \\frac{X - \\mu}{\\sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d2d7e",
   "metadata": {},
   "source": [
    "## Model Selection Criteria: AIC and BIC\n",
    "<a id=\"aic-bic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e721d",
   "metadata": {},
   "source": [
    "Model selection criteria help us balance two competing goals:\n",
    "- *Goodness of fit*: How well does the model explain the observed data?\n",
    "- *Model complexity*: How many parameters does the model use?\n",
    "\n",
    "It is like finding the \"sweet spot\" between underfitting (too simple) and overfitting (too complex). These criteria penalize models for being complex while rewarding them for fitting the data well.\n",
    "\n",
    "|| Akaike Information Criterion (AIC) | Bayesian Information Criterion (BIC)|\n",
    "| ---|------ | ------ |\n",
    "|Definition| $$AIC = 2k - 2ln(L)$$- $k$ = number of parameters in the model</br>- $L$ = max likelihood of the model</br>- $ln(L)$ = log-likelihood of the model | $$BIC = k\\cdot ln(n) - 2ln(L)$$- $k$ = number of parameters</br>- $n$ = sample size</br>- $L$ = maximum likelihood</br>- $ln(L)$ = log-likelihood |\n",
    "|Key properties |1. *Asymptotic efficiency*: Selects the model that minimizes prediction error as sample size approaches infinity </br>2. *Consistent selection*: Among nested models, tends to select the true model with high probability </br>3. *Relative measure*: Only meaningful when comparing models; absolute values don't matter</br>4. *Sample size independent penalty*: The penalty term ($2k$) doesn't depend on sample size |1. *Bayesian foundation*: Derived from Bayesian model comparison principles </br>2. *Consistent*: Selects the true model with probability approaching 1 as $n \\rightarrow \\infty$</br>3. *Sample size dependent penalty*: Penalty increases with sample size $(k\\cdot ln(n))$</br>4. *More conservative*: Generally penalizes complexity more heavily than AIC, especially for larger samples|\n",
    "|Advantages|- Well-suited for prediction-focused model selection</br>- Performs well in small samples</br>- Less likely to underfit</br>- Solid theoretical foundation in information theory|- Strong theoretical foundation in Bayesian statistics</br>- More conservative, reducing overfitting risk</br>- Consistent model selection (selects true model asymptotically)</br>- Adapts penalty based on sample size|\n",
    "|Limitations|- May overfit in large samples<br>- Can select overly complex models when the true model is among candidates</br>- Doesn't account for sample size in penalty term|- May underfit, especially in small samples</br>- Can be too conservative for prediction tasks</br>- Assumes one of the candidate models is the \"true\" model</br>- Less suitable when the focus is purely predictive|\n",
    "|When to use |- Primary goal is prediction accuracy</br>- Working with small to moderate sample sizes</br>- You want to err on the side of including potentially relevant variables</br>- The focus is on finding the best approximating model rather than the \"true\" model</br>- Cross-validation is not feasible|- Primary goal is identifying the true underlying model</br>- Working with large sample sizes</br>- You want to avoid overfitting</br>- Parsimony is particularly important (note: the penalty terms embody parsimony by making complex models \"pay\" for additional parameters)</br>- You're working within a Bayesian framework</br>- The candidate models include the true model|\n",
    "\n",
    "**Practical Recommendations**\n",
    "\n",
    "1. Use both criteria: Compare results from both AIC and BIC to understand the trade-off between fit and complexity\n",
    "2. Consider sample size: In small samples ($n < 40$), AIC may be preferable. In large samples ($n > 100$), BIC's stronger penalty may be more appropriate\n",
    "3. Domain knowledge: Don't rely solely on statistical criteria; incorporate subject matter expertise\n",
    "4. Cross-validation: When possible, complement AIC/BIC with cross-validation for more robust model selection\n",
    "5. Model diagnostics: Always check residuals and other diagnostics after selection\n",
    "6. Uncertainty: If models have similar AIC/BIC values, consider model averaging or report uncertainty in model selection\n",
    "\n",
    "As a standard practice, normal likelihood is used (most software does this automatically). But if you encounter other type of error distribution, use it. You can start with normal, then investigate if results seem unreasonable. \n",
    "\n",
    "|When normal likelihood is reasonable|When to consider alternatives|\n",
    "|------|-----|\n",
    "|- Residuals approximately normal (check with QQ plots, normality tests)</br>- No severe outliers</br>- Primary goal is prediction (normal likelihood often works well even with non-normal errors)</br>- Large samples (Central Limit Theorem makes normal approximation better)|- Heavy tails/outliers ‚Üí t-distribution likelihood</br>- Binary outcomes ‚Üí Bernoulli likelihood (logistic regression)</br>- Count data ‚Üí Poisson likelihood</br>- Positive continuous data ‚Üí Gamma likelihood</br>- Skewed data ‚Üí Consider transformations or appropriate GLM (Generalized Linear Models)|\n",
    "\n",
    "As discussed in the section [MLE vs OLS](#maximum-likelihood-estimation-mle-vs-ols), the log-likelihood for a normal distribution is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63459b1c",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 6:</h5> Write a function that calculates AIC, BIC, log-likelihood, RSS, $\\sigma^2$.\n",
    "\n",
    "```\n",
    "def calculate_aic_bic(y_true, y_pred, k, n):\n",
    "    \"\"\"\n",
    "    Calculate AIC and BIC given predictions and model complexity\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual values\n",
    "        y_pred: predicted values\n",
    "        k: number of parameters (including intercept)\n",
    "        n: sample size\n",
    "    \n",
    "    Returns:\n",
    "        AIC, BIC, log-likelihood, residual sum of squares (RSS), sigma2\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0768cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def calculate_aic_bic(y_true, y_pred, k, n):\n",
    "    \"\"\"\n",
    "    Calculate AIC and BIC given predictions and model complexity\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual values\n",
    "        y_pred: predicted values\n",
    "        k: number of parameters (including intercept)\n",
    "        n: sample size\n",
    "    \n",
    "    Returns:\n",
    "        AIC, BIC, log-likelihood, residual sum of squares (RSS), sigma2\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d19e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial_model(x, y, degree):\n",
    "    \"\"\"Fit polynomial model and return predictions and parameters\"\"\"\n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    x_poly = poly_features.fit_transform(x.reshape(-1, 1))\n",
    "    \n",
    "    # Fit model using normal equations (more numerically stable for small datasets)\n",
    "    coefficients = np.linalg.lstsq(x_poly, y, rcond=None)[0]\n",
    "    y_pred = x_poly @ coefficients\n",
    "    \n",
    "    k = len(coefficients)  # number of parameters\n",
    "    \n",
    "    return y_pred, coefficients, k, x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Model Comparison\n",
    "models_poly = {}\n",
    "results_poly = []\n",
    "\n",
    "# Fit polynomial models of different degrees\n",
    "for degree in [1, 2, 3]:\n",
    "    model_name = f\"Polynomial Degree {degree}\"\n",
    "    \n",
    "    # Fit model\n",
    "    y_pred, coeffs, k, X_poly = fit_polynomial_model(data_x, data_y, degree)\n",
    "    \n",
    "    # Calculate AIC and BIC\n",
    "    aic, bic, log_lik, rss, sigma2 = calculate_aic_bic(data_y, y_pred, k, n)\n",
    "    \n",
    "    # Store results\n",
    "    models_poly[model_name] = {\n",
    "        'degree': degree,\n",
    "        'coefficients': coeffs,\n",
    "        'y_pred': y_pred,\n",
    "        'k': k,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'log_likelihood': log_lik,\n",
    "        'rss': rss,\n",
    "        'r_squared': 1 - rss / np.sum((data_y - np.mean(data_y))**2)\n",
    "    }\n",
    "    \n",
    "    results_poly.append({\n",
    "        'Model': model_name,\n",
    "        'Degree': degree,\n",
    "        'Parameters (k)': k,\n",
    "        'Log-Likelihood': log_lik,\n",
    "        'RSS': rss,\n",
    "        'R¬≤': 1 - rss / np.sum((data_y - np.mean(data_y))**2),\n",
    "        'AIC': aic,\n",
    "        'BIC': bic\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "df_poly = pd.DataFrame(results_poly)\n",
    "print(\"\\nPolynomial Model Results:\")\n",
    "print(df_poly.round(3))\n",
    "\n",
    "# Calculate AIC and BIC differences\n",
    "df_poly['ŒîAIC'] = df_poly['AIC'] - df_poly['AIC'].min()\n",
    "df_poly['ŒîBIC'] = df_poly['BIC'] - df_poly['BIC'].min()\n",
    "\n",
    "print(\"\\nModel Comparison (Œî = difference from best model):\")\n",
    "print(df_poly[['Model', 'Parameters (k)', 'AIC', 'ŒîAIC', 'BIC', 'ŒîBIC']].round(3))\n",
    "\n",
    "# Calculate Akaike weights\n",
    "delta_aic = df_poly['ŒîAIC'].values\n",
    "akaike_weights = np.exp(-delta_aic/2) / np.sum(np.exp(-delta_aic/2))\n",
    "df_poly['Akaike Weight'] = akaike_weights\n",
    "\n",
    "print(\"\\nAkaike Weights (Model Selection Probabilities):\")\n",
    "for i, row in df_poly.iterrows():\n",
    "    print(f\"{row['Model']}: {row['Akaike Weight']:.3f} ({row['Akaike Weight']*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models\n",
    "best_aic_poly = df_poly.loc[df_poly['AIC'].idxmin(), 'Model']\n",
    "best_bic_poly = df_poly.loc[df_poly['BIC'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"‚Ä¢ Best model by AIC: {best_aic_poly}\")\n",
    "print(f\"‚Ä¢ Best model by BIC: {best_bic_poly}\")\n",
    "\n",
    "if best_aic_poly == best_bic_poly:\n",
    "    print(f\"‚Ä¢ Both criteria agree on {best_aic_poly}\")\n",
    "else:\n",
    "    print(\"‚Ä¢ AIC and BIC disagree - suggests trade-off between fit and complexity\")\n",
    "\n",
    "# Check model uncertainty\n",
    "max_weight_poly = df_poly['Akaike Weight'].max()\n",
    "if max_weight_poly > 0.9:\n",
    "    print(f\"‚Ä¢ High confidence in model selection (weight = {max_weight_poly:.3f})\")\n",
    "elif max_weight_poly > 0.7:\n",
    "    print(f\"‚Ä¢ Moderate confidence in model selection (weight = {max_weight_poly:.3f})\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ Substantial uncertainty in model selection (weight = {max_weight_poly:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18102859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multiple_regression(x1, x2, y):\n",
    "    \"\"\"Fit multiple regression model: y ~ x1 + x2\"\"\"\n",
    "    X = np.column_stack([np.ones(len(x1)), x1, x2])  # add intercept\n",
    "    coefficients = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    y_pred = X @ coefficients\n",
    "    k = len(coefficients)\n",
    "    \n",
    "    return y_pred, coefficients, k, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Selection\n",
    "models_var = {}\n",
    "results_var = []\n",
    "\n",
    "# Model 1: Height ~ Age (Linear)\n",
    "y_pred1, coeffs1, k1, X1 = fit_polynomial_model(data_x, data_y, 1)\n",
    "aic1, bic1, log_lik1, rss1, sigma2_1 = calculate_aic_bic(data_y, y_pred1, k1, n)\n",
    "\n",
    "models_var['Height ~ Age'] = {\n",
    "    'y_pred': y_pred1,\n",
    "    'k': k1,\n",
    "    'aic': aic1,\n",
    "    'bic': bic1,\n",
    "    'coefficients': coeffs1\n",
    "}\n",
    "\n",
    "results_var.append({\n",
    "    'Model': 'Height ~ Age',\n",
    "    'Variables': 'Age',\n",
    "    'Parameters (k)': k1,\n",
    "    'Log-Likelihood': log_lik1,\n",
    "    'R¬≤': 1 - rss1 / np.sum((data_y - np.mean(data_y))**2),\n",
    "    'AIC': aic1,\n",
    "    'BIC': bic1\n",
    "})\n",
    "\n",
    "# Model 2: Height ~ Weight (Linear)\n",
    "y_pred2, coeffs2, k2, X2 = fit_polynomial_model(data_z, data_y, 1)\n",
    "aic2, bic2, log_lik2, rss2, sigma2_2 = calculate_aic_bic(data_y, y_pred2, k2, n)\n",
    "\n",
    "models_var['Height ~ Weight'] = {\n",
    "    'y_pred': y_pred2,\n",
    "    'k': k2,\n",
    "    'aic': aic2,\n",
    "    'bic': bic2,\n",
    "    'coefficients': coeffs2\n",
    "}\n",
    "\n",
    "results_var.append({\n",
    "    'Model': 'Height ~ Weight',\n",
    "    'Variables': 'Weight',\n",
    "    'Parameters (k)': k2,\n",
    "    'Log-Likelihood': log_lik2,\n",
    "    'R¬≤': 1 - rss2 / np.sum((data_y - np.mean(data_y))**2),\n",
    "    'AIC': aic2,\n",
    "    'BIC': bic2\n",
    "})\n",
    "\n",
    "# Model 3: Height ~ Age + Weight (Multiple regression)\n",
    "y_pred3, coeffs3, k3, X3 = fit_multiple_regression(data_x, data_z, data_y)\n",
    "aic3, bic3, log_lik3, rss3, sigma2_3 = calculate_aic_bic(data_y, y_pred3, k3, n)\n",
    "\n",
    "models_var['Height ~ Age + Weight'] = {\n",
    "    'y_pred': y_pred3,\n",
    "    'k': k3,\n",
    "    'aic': aic3,\n",
    "    'bic': bic3,\n",
    "    'coefficients': coeffs3\n",
    "}\n",
    "\n",
    "results_var.append({\n",
    "    'Model': 'Height ~ Age + Weight',\n",
    "    'Variables': 'Age + Weight',\n",
    "    'Parameters (k)': k3,\n",
    "    'Log-Likelihood': log_lik3,\n",
    "    'R¬≤': 1 - rss3 / np.sum((data_y - np.mean(data_y))**2),\n",
    "    'AIC': aic3,\n",
    "    'BIC': bic3\n",
    "})\n",
    "\n",
    "# Create results DataFrame for variable selection\n",
    "df_var = pd.DataFrame(results_var)\n",
    "print(\"\\nVariable Selection Results:\")\n",
    "print(df_var.round(3))\n",
    "\n",
    "# Calculate differences\n",
    "df_var['ŒîAIC'] = df_var['AIC'] - df_var['AIC'].min()\n",
    "df_var['ŒîBIC'] = df_var['BIC'] - df_var['BIC'].min()\n",
    "\n",
    "print(\"\\nModel Comparison (Variable Selection):\")\n",
    "print(df_var[['Model', 'Variables', 'Parameters (k)', 'AIC', 'ŒîAIC', 'BIC', 'ŒîBIC']].round(3))\n",
    "\n",
    "# Calculate Akaike weights for variable selection\n",
    "delta_aic_var = df_var['ŒîAIC'].values\n",
    "akaike_weights_var = np.exp(-delta_aic_var/2) / np.sum(np.exp(-delta_aic_var/2))\n",
    "df_var['Akaike Weight'] = akaike_weights_var\n",
    "\n",
    "print(\"\\nAkaike Weights (Variable Selection Probabilities):\")\n",
    "for i, row in df_var.iterrows():\n",
    "    print(f\"{row['Model']}: {row['Akaike Weight']:.3f} ({row['Akaike Weight']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models\n",
    "best_aic_var = df_var.loc[df_var['AIC'].idxmin(), 'Model']\n",
    "best_bic_var = df_var.loc[df_var['BIC'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"‚Ä¢ Best model by AIC: {best_aic_var}\")\n",
    "print(f\"‚Ä¢ Best model by BIC: {best_bic_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ab9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze variable importance\n",
    "if 'Age + Weight' in best_aic_var:\n",
    "    print(\"‚Ä¢ Both age and weight appear useful for predicting height\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ Single variable model ({best_aic_var.split('~')[1].strip()}) is preferred\")\n",
    "\n",
    "# Check if adding variables is worthwhile\n",
    "simple_model_idx = df_var[df_var['Variables'] == 'Age'].index[0]\n",
    "complex_model_idx = df_var[df_var['Variables'] == 'Age + Weight'].index[0]\n",
    "\n",
    "delta_aic_addition = df_var.loc[complex_model_idx, 'AIC'] - df_var.loc[simple_model_idx, 'AIC']\n",
    "delta_bic_addition = df_var.loc[complex_model_idx, 'BIC'] - df_var.loc[simple_model_idx, 'BIC']\n",
    "\n",
    "print(f\"‚Ä¢ Adding weight to age model changes AIC by {delta_aic_addition:.2f}\")\n",
    "print(f\"‚Ä¢ Adding weight to age model changes BIC by {delta_bic_addition:.2f}\")\n",
    "\n",
    "if delta_aic_addition < -2:\n",
    "    print(\"‚Ä¢ Strong evidence that weight improves the model (ŒîAIC < -2)\")\n",
    "elif delta_aic_addition > 2:\n",
    "    print(\"‚Ä¢ Strong evidence against including weight (ŒîAIC > 2)\")\n",
    "else:\n",
    "    print(\"‚Ä¢ Weak evidence about including weight (-2 ‚â§ ŒîAIC ‚â§ 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813709ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Data and polynomial fits\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(data_x, data_y, color='black', s=50, alpha=0.7, label='Data')\n",
    "\n",
    "# Plot polynomial fits\n",
    "x_smooth = np.linspace(data_x.min(), data_x.max(), 100)\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (model_name, model_info) in enumerate(models_poly.items()):\n",
    "    degree = model_info['degree']\n",
    "    coeffs = model_info['coefficients']\n",
    "    \n",
    "    # Create polynomial features for smooth line\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    x_smooth_poly = poly_features.fit_transform(x_smooth.reshape(-1, 1))\n",
    "    y_smooth = x_smooth_poly @ coeffs\n",
    "    \n",
    "    ax1.plot(x_smooth, y_smooth, color=colors[i], \n",
    "             label=f'Degree {degree} (AIC={model_info[\"aic\"]:.1f})')\n",
    "\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_ylabel('Height')\n",
    "ax1.set_title('Polynomial Model Fits')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: AIC/BIC comparison for polynomial models\n",
    "ax2 = axes[0, 1]\n",
    "x_pos = np.arange(len(df_poly))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x_pos - width/2, df_poly['AIC'], width, label='AIC', alpha=0.8)\n",
    "bars2 = ax2.bar(x_pos + width/2, df_poly['BIC'], width, label='BIC', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Polynomial Degree')\n",
    "ax2.set_ylabel('Information Criterion Value')\n",
    "ax2.set_title('AIC vs BIC: Polynomial Models')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'Degree {d}' for d in df_poly['Degree']])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height:.1f}', ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Variable selection results\n",
    "ax3 = axes[1, 0]\n",
    "x_pos_var = np.arange(len(df_var))\n",
    "width = 0.35\n",
    "\n",
    "bars3 = ax3.bar(x_pos_var - width/2, df_var['AIC'], width, label='AIC', alpha=0.8)\n",
    "bars4 = ax3.bar(x_pos_var + width/2, df_var['BIC'], width, label='BIC', alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Model')\n",
    "ax3.set_ylabel('Information Criterion Value')\n",
    "ax3.set_title('AIC vs BIC: Variable Selection')\n",
    "ax3.set_xticks(x_pos_var)\n",
    "ax3.set_xticklabels(['Age', 'Weight', 'Age+Weight'], rotation=45)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height:.1f}', ha='center', va='bottom')\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Akaike weights\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Combine polynomial and variable selection weights\n",
    "all_models = list(df_poly['Model']) + list(df_var['Model'])\n",
    "all_weights = list(df_poly['Akaike Weight']) + list(df_var['Akaike Weight'])\n",
    "\n",
    "# Separate into two groups for better visualization\n",
    "poly_models = df_poly['Model'].tolist()\n",
    "poly_weights = df_poly['Akaike Weight'].tolist()\n",
    "\n",
    "var_models = [m.split('~')[1].strip() for m in df_var['Model']]\n",
    "var_weights = df_var['Akaike Weight'].tolist()\n",
    "\n",
    "x_pos1 = np.arange(len(poly_models))\n",
    "x_pos2 = np.arange(len(var_models)) + len(poly_models) + 0.5\n",
    "\n",
    "bars5 = ax4.bar(x_pos1, poly_weights, label='Polynomial Models', alpha=0.8)\n",
    "bars6 = ax4.bar(x_pos2, var_weights, label='Variable Selection', alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel('Akaike Weight')\n",
    "ax4.set_title('Model Selection Probabilities')\n",
    "ax4.set_xticks(list(x_pos1) + list(x_pos2))\n",
    "ax4.set_xticklabels([f'Deg {i+1}' for i in range(len(poly_models))] + var_models, rotation=45)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar in bars5:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height*100:.1f}%', ha='center', va='bottom')\n",
    "for bar in bars6:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height*100:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "print(f\"\\nDATASET SUMMARY:\")\n",
    "print(f\"‚Ä¢ Sample size: n = {n} (small sample)\")\n",
    "print(f\"‚Ä¢ Variables: Age (predictor), Height (response), Weight (additional predictor)\")\n",
    "print(f\"‚Ä¢ Age-Height correlation: r = {np.corrcoef(data_x, data_y)[0,1]:.3f}\")\n",
    "print(f\"‚Ä¢ Weight-Height correlation: r = {np.corrcoef(data_z, data_y)[0,1]:.3f}\")\n",
    "print(f\"‚Ä¢ Age-Weight correlation: r = {np.corrcoef(data_x, data_z)[0,1]:.3f}\")\n",
    "\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "print(f\"1. Polynomial Model Selection:\")\n",
    "print(f\"   ‚Ä¢ AIC prefers: {best_aic_poly}\")\n",
    "print(f\"   ‚Ä¢ BIC prefers: {best_bic_poly}\")\n",
    "print(f\"   ‚Ä¢ Model uncertainty: {'Low' if max_weight_poly > 0.9 else 'Moderate' if max_weight_poly > 0.7 else 'High'}\")\n",
    "\n",
    "print(f\"\\n2. Variable Selection:\")\n",
    "print(f\"   ‚Ä¢ AIC prefers: {best_aic_var}\")\n",
    "print(f\"   ‚Ä¢ BIC prefers: {best_bic_var}\")\n",
    "print(f\"   ‚Ä¢ Both age and weight are {('strong' if min(delta_aic_addition, delta_bic_addition) < -2 else 'weak')} predictors\")\n",
    "\n",
    "print(f\"\\n3. Statistical Insights:\")\n",
    "print(f\"   ‚Ä¢ Small sample size (n={n}) means BIC penalty is modest: ln({n}) = {np.log(n):.2f}\")\n",
    "print(f\"   ‚Ä¢ Limited power to distinguish between models\")\n",
    "print(f\"   ‚Ä¢ Results should be interpreted cautiously due to small n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99e21a",
   "metadata": {},
   "source": [
    "**Recommendations:**\n",
    "- For prediction: Use model selected by AIC\n",
    "- For interpretation: Consider BIC's more parsimonious choice\n",
    "- Collect more data to reduce model selection uncertainty\n",
    "- Validate results with cross-validation or independent data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55d83b",
   "metadata": {},
   "source": [
    "## Back to the Opening Problem\n",
    "\n",
    "Will be seen as a lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42defc10",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "- [Linear Regression by StatQuest](https://www.youtube.com/watch?v=7ArmBVF2dCs)\n",
    "- [R-squared, Clearly Explained!!! by StatQuest](https://www.youtube.com/watch?v=2AQKmw14mHM)\n",
    "- [The Main Ideas of Fitting a Line to Data by StatQuest](https://www.youtube.com/watch?v=PaFPbb66DxQ)\n",
    "- [Regularization Part 1: Ridge (L2) Regression by StatQuest](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "- [Regularization Part 2: Lasso (L1) Regression by StatQuest](https://www.youtube.com/watch?v=NGf0voTMlcs&t=290s)\n",
    "- [Regularization Part 3: Elastic Net Regression by StatQuest](https://www.youtube.com/watch?v=1dKRdX9bfIo&t=132s)\n",
    "- [Ridge vs Lasso Regression, Visualized!!!](https://www.youtube.com/watch?v=Xm2C_gTAl8c)\n",
    "\n",
    "- [Bias and Variance by StatQuest](https://www.youtube.com/watch?v=EuBBz3bI-aA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dadec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probability-statistics-ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
