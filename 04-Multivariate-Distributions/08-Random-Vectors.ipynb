{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a543407",
   "metadata": {},
   "source": [
    "# Random Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c407a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/styles.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fd79f",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bdbbc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b58574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "#sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the \"resources\" directory to the path\n",
    "project_root = Path().resolve().parent\n",
    "resources_path = project_root / 'resources'\n",
    "sys.path.insert(0, str(resources_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568408cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivariate import(interactive_carousel_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0c864",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3>üéØ Viral Ad Carousel Mystery</h3>\n",
    "\n",
    "**Context**: You're an ML engineer at a social media company. The product team has designed a new ad carousel feature where N = 10 advertisements are displayed in a circular sequence. User testing shows that each ad gets a \"like\" (*L*) or \"skip\" (*S*) with roughly equal probability ($p = 0.5$).\n",
    "\n",
    "**The Bonus System**: Marketing wants to reward advertisers with a bonus whenever their ad stands out from the crowd. Specifically, an ad earns a bonus if the user's reaction to it differs from BOTH adjacent ads.\n",
    "\n",
    "Example:\n",
    "\n",
    "|Ad sequence: |   [1] | [2] | [3] | [4] | [5] | [6] | [7] | [8] | [9] | [10]|\n",
    "|----|:---:|:---:|:--:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| **User reactions:** | L   |S|   L |  L |  S |  L |  S |  S |  L |  S|\n",
    "| **Bonuses?:**    |   No  |YES| No|  No | YES |No|  YES |No  |YES | YES|\n",
    "\n",
    "Total:  5 bonuses\n",
    "\n",
    "**Your Task**: The CFO asks you: \"On average, how many bonuses will we pay per user? We need this for our quarterly budget!\"\n",
    "\n",
    "Quick Poll (make a guess!):\n",
    "\n",
    "- About 2 bonuses?\n",
    "- About 5 bonuses?\n",
    "- About 8 bonuses?\n",
    "\n",
    "**Challenge**: Can you solve this without simulating millions of users?\n",
    "\n",
    "üí° Why This Is Hard (Without Today's Tools)</h4>\n",
    "\n",
    "- There are 2^10 = 1,024 possible user reaction sequences\n",
    "- Each sequence has a different number of bonuses\n",
    "- The dependencies between adjacent ads make direct calculation complex\n",
    "- We could simulate, but that doesn't give us the exact mathematical answer\n",
    "\n",
    "**Promise:** By the end of this lesson, you'll solve this elegantly using random vectors\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e765989",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"img/circular_carousel_with_relationships.svg\" alt=\"Carousel ad\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ad2ca",
   "metadata": {},
   "source": [
    "## Random Vectors - The Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342924c1",
   "metadata": {},
   "source": [
    "When training a neural network, we don't process one feature at a time. We process feature vectors:\n",
    "\n",
    "- Image: [pixel‚ÇÅ, pixel‚ÇÇ, ..., pixel‚Çá‚Çà‚ÇÑ] for 28√ó28 MNIST\n",
    "- Text embedding: [dim‚ÇÅ, dim‚ÇÇ, ..., dim‚ÇÖ‚ÇÅ‚ÇÇ] for BERT\n",
    "- User profile: [age, income, clicks, time_on_site, ...]\n",
    "\n",
    "*Question*: How do we mathematically model the joint behavior of multiple random quantities?\n",
    "\n",
    "Let's start simple. Imagine you're tracking 3 metrics for a web user:\n",
    "\n",
    "- $X_1 = \\text{time spent on page (seconds)}$\n",
    "- $X_2 = \\text{number of clicks}$\n",
    "- $X_3 = \\text{scroll depth (percentage)}$\n",
    "\n",
    "Instead of treating these separately, we group them: $\\mathbf{X} = [X_1, X_2, X_3]$. This is a random vector - a vector whose components are random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96dfdf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Random Vector</h4>\n",
    "\n",
    "Let $(\\Omega, A, P)$ be a probability space. A random vector is a mapping:\n",
    "$$X: \\Omega \\rightarrow \\mathbb{R}^n$$\n",
    "represented as:\n",
    "$$\\mathbf{X} = [X_1, X_2, ..., X_n]^T$$\n",
    "where each $X_i$ is a random variable.\n",
    "\n",
    "Notation: We use bold uppercase letters ($\\mathbf{X}$, $\\mathbf{Y}$, $\\mathbf{Z}$) for random vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c2caa",
   "metadata": {},
   "source": [
    "Two Ways to Visualize Random Vectors:\n",
    "\n",
    "1. View 1: Direct Mapping from Sample Space\n",
    "- Sample space $\\Omega \\rightarrow \\mathbb{R}^n$\n",
    "- Each outcome œâ maps to a point $(x_1, x_2, ..., x_n)$ in $n$-dimensional space\n",
    "\n",
    "<center>\n",
    "<img src=\"img/vectors-1.png\" width=\"400px\">\n",
    "</center>\n",
    "\n",
    "2. View 2: Vector of Random Variables\n",
    "- Each component $X_i: \\Omega \\rightarrow \\mathbb{R}$\n",
    "- The vector combines $n$ separate random variables\n",
    "\n",
    "<center>\n",
    "<img src=\"img/vectors-2.png\" width=\"400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4ac77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application Spotlight: Feature Vectors</h4>\n",
    "In Machine Learning:\n",
    "\n",
    "- Input features: $X = [x_1, x_2, ..., x_n]$ (e.g., user age, income, browsing history)\n",
    "- Model weights: $W = [w_1, w_2, ..., w_n]$ (learned parameters)\n",
    "- Hidden layer activations: $H = [h_1, h_2, ..., h_k]$\n",
    "- Prediction: $\\hat{y} = f(X¬∑W)$\n",
    "\n",
    "Random vectors allow us to model uncertainty in all these quantities simultaneously!\n",
    "\n",
    "*Example*: In A/B testing, each user's behavior is a random vector [clicks, time_on_site, conversion]. We need to understand the joint distribution to make business decisions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad517d6b",
   "metadata": {},
   "source": [
    "## Joint Distributions for $n$ Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a00924",
   "metadata": {},
   "source": [
    "**Scenario**: You have sensor data from an autonomous vehicle:\n",
    "\n",
    "- $X_1 = \\text{speed (km/h)}$\n",
    "- $X_2 = \\text{steering angle (degrees)}$\n",
    "- $X_3 = \\text{brake pressure (PSI)}$\n",
    "\n",
    "Knowing each distribution separately isn't enough! You need to know: \"*What's the probability that speed > 100 AND steering angle > 30 AND brake pressure < 50?*\"\n",
    "\n",
    "This requires the joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6da7c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Joint Distribution (n variables)</h4>\n",
    "\n",
    "Let $X_1, X_2, ..., X_n$ be $n$ random variables defined on $(\\Omega, \\mathcal{A}, \\mathbb{P})$.\n",
    "\n",
    "<h5>Discrete case</h5>:\n",
    "\n",
    "The **joint probability mass function** (or *joint PMF*) is given by:\n",
    "\n",
    "$$\\mathbb{P}_{X_1,X_2,...,X_n}(x_1, x_2, ..., x_n) = \\mathbb{P}(X_1=x_1, X_2=x_2, ..., X_n=x_n)$$\n",
    "\n",
    "The **joint CDF** of $n$ r.v. $X_1,...,X_n$ is defined as:\n",
    "$$F_{X_1X_2...X_n}(x_1,x_2,...,x_n) = \\mathbb{P}([X_1\\leq x_1] \\ \\cap\\ [X_2\\leq x_2]\\ \\cap\\ ...\\ \\cap\\ [X_n \\leq x_n])$$\n",
    "\n",
    "<h5>Continuous case</h5>:\n",
    "\n",
    "The **joint probability density function (PDF)** $f_{X_1X_2...X_n}(x_1,x_2,...,x_n)$ satisfies:\n",
    "For any set $A\\subset \\mathbb{R}^n$:\n",
    "$$\\forall A\\in \\mathbb{R}^n, \\ \\mathbb{P}\\left((X_1, X_2, ..., X_n)\\in A\\right) = \\int_{}...\\int\\limits_A...\\int f_{X_1X_2...X_n}(t_1, t_2, ..., t_n)dt_1 dt_2 ... dt_n$$\n",
    "\n",
    "The **joint CDF** of $X_1,...,X_n$ is given by:\n",
    "\n",
    "$$F_{X_1X_2...X_n}(x_1,x_2,...,x_n) = \\mathbb{P}(X_1\\leq x_1, X_2\\leq x_2,..., X_n \\leq x_n)  =$$\n",
    "$$= \\int_{-\\infty}^{x_1}\\int_{-\\infty}^{x_2}...\\int_{-\\infty}^{x_n}f_{X_1X_2...X_n}(t_1, t_2, ..., t_n)dt_1 dt_2 ... dt_n$$\n",
    "\n",
    "\n",
    "**Key Properties:**\n",
    "\n",
    "* $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty}f_{X_1X_2...X_n}(x_1, x_2, ..., x_n)dx_1 dx_2 ... dx_n = 1$\n",
    "* $\\lim \\limits_{x\\to-\\infty} F_{\\mathbf{X}}(x,...,x) = 0$\n",
    "* $\\lim \\limits_{x\\to+\\infty} F_{\\mathbf{X}}(x,...,x) = 1$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20326b",
   "metadata": {},
   "source": [
    "<div class=\"alert example\">\n",
    "<h4>Calculated Example: Finding the Normalizing Constant</h4>\n",
    "\n",
    "Three features from a recommendation system have joint PDF:\n",
    "\n",
    "$$f_{XYZ}(x,y,z) = \\left\\{ \\begin{array}{ll} c(3x + 2y + z) & 0 \\leq x \\leq 1, \\ 0\\leq y \\leq 1, \\ 0\\leq z \\leq 1 \\\\ 0 & \\text{otherwise} \\end{array}\\right.$$\n",
    "\n",
    "where $c$ is a constant.\n",
    "\n",
    "Find the constant $c$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbff60",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal solution</summary>\n",
    "\n",
    "When dealing with a joint PDF. Our solution strategy:\n",
    "\n",
    "$$\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty}f_{XYZ}(x,y,z)dxdydz = 1$$\n",
    "\n",
    "Using this expression, we can find $c$. Note that outside $0 \\leq x \\leq 1, \\ 0\\leq y \\leq 1, \\ 0\\leq z \\leq 1$ the PDF equals 0. Then:\n",
    "\n",
    "$$\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty}f_{XYZ}(x,y,z)dxdydz = \\int\\limits_{0}^{1}\\int\\limits_{0}^{1}\\int\\limits_{0}^{1}c(3x + 2y + z)dxdydz = 1$$\n",
    "\n",
    "$$\\int\\limits_{0}^{1}\\int\\limits_{0}^{1}\\int\\limits_{0}^{1}c(3x + 2y + z)dxdydz = \\int\\limits_{0}^{1}\\int\\limits_{0}^{1}c\\left(\\frac{3x^2}{2} + 2yx + zx\\right)\\Bigg\\rvert_{0}^{1}dydz = \\int\\limits_{0}^{1}\\int\\limits_{0}^{1}c\\left(\\frac{3}{2} + 2y + z\\right)dydz =$$\n",
    "\n",
    "$$=\\int\\limits_{0}^{1}c\\left(\\frac{3}{2}y + \\frac{2y^2}{2} + zy\\right)\\Bigg\\rvert_{0}^{1}dz = \\int\\limits_{0}^{1}c\\left(\\frac{3}{2}y + y^2 + zy\\right)\\Bigg\\rvert_{0}^{1}dz =\\int\\limits_{0}^{1}c\\left(\\frac{3}{2} + 1 + z\\right)dz=$$\n",
    "\n",
    "$$=\\int\\limits_{0}^{1}c\\left(\\frac{5}{2} + z\\right)dz = c\\left(\\frac{5}{2}z + \\frac{z^2}{2}\\right)\\Bigg\\rvert_{0}^{1} = c\\left(\\frac{5}{2} + \\frac{1}{2}\\right) = 3c$$\n",
    "\n",
    "Then:\n",
    "$$3c = 1$$\n",
    "$$c = \\frac{1}{3}$$\n",
    "\n",
    "Therefore, we can substitute $c$ with its value in $f_{XYZ}$:\n",
    "\n",
    "$$f_{XYZ}(x,y,z) = \\left\\{ \\begin{array}{ll} \\frac{1}{3}(3x + 2y + z) & 0 \\leq x \\leq 1, \\ 0\\leq y \\leq 1, \\ 0\\leq z \\leq 1 \\\\ 0 & \\text{otherwise} \\end{array}\\right.$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2860f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Key Insight: The Integration Order</h4>\n",
    "\n",
    "Notice we integrated $x$ first, then $y$, then $z$. For continuous functions, we can change the order (Fubini's theorem), but:\n",
    "\n",
    "- Always respect the limits of integration\n",
    "- In ML applications, choosing the right order can simplify computation\n",
    "- Think about which variables are \"easier\" to integrate first\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094bc00",
   "metadata": {},
   "source": [
    "## Marginal Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d006db4",
   "metadata": {},
   "source": [
    "**Scenario:** You have a dataset with 100 features, but you want to analyze just feature #37. Do you need the full 100-dimensional distribution?\n",
    "\n",
    "**Answer:** No! You can get the distribution of X‚ÇÉ‚Çá by \"marginalizing out\" the other 99 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769eb564",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Marginal Distribution</h4>\n",
    "\n",
    "Let $\\mathbf{X} = (X_1,...,X_n)$ be a random vector in $\\mathbb{R}^n$.\n",
    "\n",
    "We call the $k^{th}$ **marginal distribution** $k\\in\\{1,...,n\\}$ of $\\mathbf{X}$ the distribution of the r.v. $X_k$.\n",
    "\n",
    "How to compute it:\n",
    "\n",
    "<h5>Discrete case:</h5>\n",
    "\n",
    "$$P_{X_i}(x_i) = \\sum_{x_1}...\\sum_{x_{i-1}}\\sum_{x_{i+1}}...\\sum_{x_n} P(x_1,...,x_n)$$\n",
    "\n",
    "<h5>Continuous case:</h5>\n",
    "\n",
    "$$f_{X_i}(x_i) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty}f_{X_1X_2...X_n}(x_1, x_2, ..., x_n)dx_1...dx_{i-1}dx_{i+1}...dx_n$$\n",
    "\n",
    "*Interpretation*: \"Integrate out\" or \"sum out\" all variables except $X_i$.\n",
    "\n",
    "$$F_{X_i}(x) = \\mathbb{P}(X_i\\leq x) = \\mathbb{P}(X_1\\in\\mathbb{R}, ..., X_{i-1}\\in\\mathbb{R}, X_i\\leq x, X_{i+1}\\in\\mathbb{R}  ,..., X_n\\in\\mathbb{R}) =$$\n",
    "\n",
    "$$= \\lim\\limits_{y\\to +\\infty}F_X(\\underbrace{y,...,y}_{i-1\\ \\text{elements}},\\overbrace{x}^{i^{th}},\\underbrace{y,...,y}_{\\text{from }(i+1)^{th}})$$\n",
    "\n",
    "In the continuous case:\n",
    "$$F_{X_i}(x) = \\int_{-\\infty}^x f_{X_i}(t_1,...,t_n)dt_i$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0d9e0",
   "metadata": {},
   "source": [
    "<div class=\"alert example\">\n",
    "<h4>Calculated Example: Finding Marginal Density</h4>\n",
    "\n",
    "Using our previous example with joint PDF:\n",
    "\n",
    "$$f_{XYZ}(x,y,z) = \\left\\{ \\begin{array}{ll} (1/3)(3x + 2y + z) & 0 \\leq x \\leq 1, \\ 0\\leq y \\leq 1, \\ 0\\leq z \\leq 1 \\\\ 0 & \\text{otherwise} \\end{array}\\right.$$\n",
    "\n",
    "find the marginal density of $X$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71993c33",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal solution</summary>\n",
    "\n",
    "As we noted previously, the joint PDF equals 0 outside $0 \\leq x \\leq 1, \\ 0\\leq y \\leq 1, \\ 0\\leq z \\leq 1$. Then for $0\\leq x\\leq 1$, the marginal density can be calculated as follows:\n",
    "\n",
    "$$f_X(x) = \\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty}f_{XYZ}(x,y,z)dydz =\\int\\limits_{0}^{1}\\int\\limits_{0}^{1}\\frac{1}{3}(3x + 2y + z)dydz = \\int\\limits_{0}^{1}\\frac{1}{3}(3xy + 2\\frac{y^2}{2} + zy)\\Bigg\\rvert_{0}^{1}dz =$$\n",
    "\n",
    "$$= \\int\\limits_{0}^{1}\\frac{1}{3}(3x + 1 + z)dz = \\frac{1}{3}\\left(3xz + z +\\frac{z^2}{2}\\right)\\Bigg\\rvert_{0}^{1} = \\frac{1}{3}\\left(3x + 1 + \\frac{1}{2}\\right) = \\frac{1}{3}\\left(3x + \\frac{3}{2}\\right) = x + \\frac{1}{2}$$\n",
    "\n",
    "for $x \\in [0, 1]$.\n",
    "\n",
    "Check: \n",
    "$$\\int_0^1 x + \\frac{1}{2} dx = \\bigg[\\frac{x^2}{2} + \\frac{x}{2}\\bigg]_0^1 = \\frac{1^2}{2} + \\frac{1}{2} - (\\frac{0^2}{2} + \\frac{0}{2}) = 1$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d4f7c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application: Feature Selection</h4>\n",
    "\n",
    "**Problem**: You have 1,000 features but want to select the top 10 for your model.\n",
    "\n",
    "Approach using marginals:\n",
    "\n",
    "- Compute marginal distributions of each feature\n",
    "- Calculate marginal statistics (mean, variance, entropy)\n",
    "- Rank features by information content\n",
    "- Select top-k features\n",
    "\n",
    "**Why this works**: Marginal distributions tell us about individual feature importance, even when features are part of a high-dimensional joint distribution.\n",
    "\n",
    "**Real example**: In text classification, word frequencies (marginals) often suffice even though words appear in complex joint patterns (n-grams).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7b7e0",
   "metadata": {},
   "source": [
    "## Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb46f9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Expectation of a Random Vector</h4>\n",
    "\n",
    "Let $\\mathbf{X} = (X_1, ..., X_n)$ be a random vector defined on a probability space $(\\Omega, \\mathcal{A}, \\mathbb{P})$. Let $\\mathbb{E}[X_i]\\in \\mathbb{R}, \\forall i=1,...,n$ be the expectation of $X_i$. The expectation of $\\mathbf{X}$ is defined as:\n",
    "\n",
    "$$\\mathbb{E}\\mathbf{X} = \\left(\\mathbb{E}[X_1],...,\\mathbb{E}[X_n]\\right) \\in \\mathbb{R}^n$$\n",
    "\n",
    "Component-wise computation: Just take the expectation of each component.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d9807",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"  style='background-color:white'>\n",
    "<h4>Properties of Expectation</h4>\n",
    "\n",
    "Let $\\mathbf{X}$ and $\\mathbf{Y}$ be two random vectors in $\\mathbb{R}^n$. Let $A \\in \\mathcal{M}_n(\\mathbb{R})$ be a square matrix of order $n$ with real coefficients. We have:\n",
    "\n",
    "1. $\\mathbb{E}[A\\mathbf{X}] = A\\ \\mathbb{E}\\mathbf{X}$\n",
    "2. $\\mathbb{E}(\\mathbf{X} + \\mathbf{Y}) = \\mathbb{E}\\mathbf{X} + \\mathbb{E}\\mathbf{Y}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e89aec",
   "metadata": {},
   "source": [
    "## Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed1b93",
   "metadata": {},
   "source": [
    "**Scenario:** You're training a model on image data. You notice:\n",
    "\n",
    "- Pixel intensity at position (i,j) strongly correlates with position (i+1, j)\n",
    "- But weakly correlates with position (i+50, j+50)\n",
    "\n",
    "How do we capture all pairwise relationships between n variables efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b2418",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Covariance Matrix</h4>\n",
    "\n",
    "Let $\\mathbf{X} = (X_1, ..., X_n)^T$ be a random vector in $\\mathbb{R}^n$.\n",
    "\n",
    "The **covariance matrix**, denoted $\\Sigma_{\\mathbf{X}}$ or $K_{\\mathbf{X}\\mathbf{X}}$, is a symmetric square matrix such that:\n",
    "\n",
    "$$K_{\\mathbf{X}\\mathbf{X}} = \\Sigma_{\\mathbf{X}} = \\left(Cov(X_i,X_j)\\right)_{i,j=1,...,n}$$\n",
    "\n",
    "Properties:\n",
    "\n",
    "1. *symmetric*, i.e.: $Cov(X_i,X_j) = Cov(X_j,X_i),\\ \\forall i,j = 1,...,n$\n",
    "2. *positive semi-definite*, i.e.: for $\\forall \\mathbf{x} = \\begin{pmatrix}x_1 & x_2 & ... &x_n\\end{pmatrix}^T \\in \\mathbb{R}^n$ we have $\\mathbf{x}^TM\\mathbf{x} \\geq 0$\n",
    "3. Diagonal elements: $Cov(X_i,X_i) = Var(X_i)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eddd7d",
   "metadata": {},
   "source": [
    "For $\\mathbf{X} = (X_1, X_2, X_3)$:\n",
    "\n",
    "$$\\Sigma_\\mathbf{X} = \\begin{pmatrix}Var(X_1) & Cov(X_1,X_2) & Cov(X_1,X_3)\\\\ Cov(X_1, C_2) & Var(X_2) & Cov(X_2,X_3) \\\\ Cov(X_3, X_1) & Cov(X_3,X_2) & Var(X_3)\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93432469",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application: Principal Component Analysis (PCA)</h4>\n",
    "\n",
    "PCA Workflow:\n",
    "\n",
    "1. Center your data: X_centered = X - mean(X)\n",
    "2. Compute covariance matrix: Œ£ = (1/n)X_centered' ¬∑ X_centered\n",
    "3. Eigen decomposition: Œ£ = QŒõQ·µÄ\n",
    "4. Principal components = eigenvectors with largest eigenvalues\n",
    "\n",
    "Why covariance matrix matters:\n",
    "\n",
    "- Captures all pairwise feature relationships\n",
    "- Eigenvectors show directions of maximum variance\n",
    "- Used for dimensionality reduction (keep top-k components)\n",
    "- Essential for whitening, decorrelation, and many preprocessing steps\n",
    "\n",
    "Real example: In face recognition, PCA on pixel covariance matrix reveals \"eigenfaces\" - the most important facial patterns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06510d",
   "metadata": {},
   "source": [
    "## Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e9a57",
   "metadata": {},
   "source": [
    "As a reminder, the intuition behind the notion of independence of events is as follows.\n",
    "\n",
    "The knowledge we have about one event (one random variable) has no influence on the probability of the remaining events.\n",
    "\n",
    "Suppose we have events $A_1, A_2, ..., A_n$. The fact that all events are independent implies that:\n",
    "\n",
    "$$\\mathbb{P}\\left(A_5 \\cap \\overline{A_6}\\right) = \\mathbb{P}\\left(\\underbrace{A_5 \\cap \\overline{A_6}}_{\\text{event of interest}} \\bigg|\\ \\ \\underbrace{\\overline{A_1} \\cup A_2 \\cup \\left(A_3 \\cap \\overline{A_4}\\right) \\cup A_7}_{\\text{what we know}} \\right)$$\n",
    "\n",
    "Note that the indices of the events (in our case $5$ and $6$) forming the event of interest ($A_5 \\cap \\overline{A_6}$) are different from the indices of the events about whose realizations we have information ($A_1$, $A_2$, $A_3$, $A_4$, $A_7$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b11af6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Mutually Independent Events</h4>\n",
    "\n",
    "The events $A_1,A_2,...,A_n$ are said to be **mutually independent** if for all distinct indices $\\forall i,j,...,m : i\\neq j\\neq ... \\neq m$ and for any number of chosen events:\n",
    "\n",
    "$$\\mathbb{P}(A_i\\cap A_j\\ \\cap\\ ... \\ \\cap\\ A_m) = \\mathbb{P}(A_i)\\times\\mathbb{P}(A_j)\\times...\\times\\mathbb{P}(A_m)$$\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88800f92",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Mutually Independent RV</h4>\n",
    "\n",
    "The r.v. $X_1, X_2, ..., X_n$ are said to be **mutually independent** if $\\forall (x_1,...,x_n) \\in \\mathbb{R}^n$, the events $[X_i\\leq x_i], i=1,...,n$ are *mutually independent*.\n",
    "\n",
    "**Property:**\n",
    "\n",
    "If $X_1,...,X_n$ are mutually independent, then:\n",
    "\n",
    "$$Var\\left(\\sum_{i=1}^nX_i\\right) = \\sum_{i=1}^n Var(X_i)$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455693e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: i.i.d.</h4>\n",
    "\n",
    "The r.v. $X_1, X_2, ..., X_n$ are said to be **independent and identically distributed (i.i.d.)** if $\\forall (x_1,...,x_n) \\in \\mathbb{R}^n$, the r.v. $X_i, i=1,...,n$ are mutually independent and have the same CDF:\n",
    "\n",
    "$$F_{X_1}(x) = F_{X_2}(x) = ... = F_{X_n}(x), \\ \\forall x\\in \\mathbb{R}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf2e20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"  style='background-color:white'>\n",
    "<h4>Property: Expected value of the product</h4>\n",
    "\n",
    "Let $X_1,...,X_n$ be $n$ i.i.d. r.v., then:\n",
    "\n",
    "$$\\mathbb{E}[X_1...X_n] = (\\text{independence}) = \\mathbb{E}X_1 \\times \\mathbb{E}X_2 \\times ... \\times \\mathbb{E}X_n =$$\n",
    "$$= (\\text{identically distributed})  = \\mathbb{E}X_1 \\times \\mathbb{E}X_1 \\times ... \\times \\mathbb{E}X_1 = (\\mathbb{E}X_1)^n$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafbaa1",
   "metadata": {},
   "source": [
    "## Important Multivariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dadf105",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"  style='background-color:white'>\n",
    "<h4>Special Case: Multinomial distribution</h4>\n",
    "\n",
    "**Multinomial distribution**, denoted $\\mathcal{M}(n,p_1,...,p_k)$ where $n\\in \\mathbb{N^{*}}$ and $p_i\\in ]0,1[ \\ \\forall i\\in\\{1,...,k\\}$ is the generalization of the binomial distribution, i.e.:\n",
    "\n",
    "$$\\mathcal{B}(n,p) = \\mathcal{M}(n,p,1-p)$$\n",
    "\n",
    "The probability mass function of $\\mathcal{M}(n,p_1,...,p_k)$ is defined by:\n",
    "$$\\mathbb{P}(X_1=\\eta_1,...,X_n=\\eta_n) = \\frac{n!}{\\eta_1!\\times...\\times \\eta_k!}p_1^{\\eta_1}\\times...\\times p_k^{\\eta_k}, \\ \\forall \\eta=(\\eta_1,...,\\eta_k)\\in \\mathbb{N}^k$$\n",
    "where:\n",
    "\n",
    "* $\\sum_{i=1}^k \\eta_i = n$\n",
    "* $\\sum_{i=1}^k p_i = 1$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b32e08",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application: Multiclass Classification</h4>\n",
    "\n",
    "1. Problem: Classifying images into $k=10$ categories (digits 0-9).\n",
    "\n",
    "Model output: Softmax layer produces probabilities (p‚ÇÅ, ..., p‚ÇÅ‚ÇÄ).\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Each image classified n=1 time\n",
    "- Multinomial(1, p‚ÇÅ, ..., p‚ÇÅ‚ÇÄ) models the prediction\n",
    "- Cross-entropy loss: -log(p_true_class)\n",
    "\n",
    "2. Real scenario: Document classification\n",
    "\n",
    "- $n = 100$ words in a document\n",
    "- $k = 20$ topics\n",
    "- Each word assigned to one topic\n",
    "- Multinomial models word-topic assignments\n",
    "\n",
    "3. Naive Bayes Classifier: Assumes features follow multinomial distribution given class label.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ed351",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"  style='background-color:white'>\n",
    "<h4>Special Case: Multivariate Normal Distribution</h4>\n",
    "\n",
    "The **$n$-dimensional normal distribution** or **multivariate normal distribution** (also called *multivariate Gaussian distribution* or *joint normal distribution*), denoted $\\mathcal{N}(\\mu, \\Sigma)$ where $\\mu = (\\mu_1,...,\\mu_n)\\in\\mathbb{R}^n$ and $\\Sigma$ is a covariance matrix (square matrix of order $n$, symmetric positive definite), is the generalization of the normal distribution $\\mathcal{N}(m, \\sigma^2)$ where $\\mu = (m)$ and $\\Sigma = (\\sigma^2)$\n",
    "\n",
    "\n",
    "$$f_{\\mathbf{X}}(x_1,...,x_n) = \\frac{1}{\\sqrt{(2\\pi)^{n}\\det\\Sigma}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T \\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right)$$\n",
    "where \n",
    "\n",
    "* $(\\mathbf{x}-\\mathbf{\\mu})^T$ denotes the column vector composed of $x_i - \\mu_i, \\ \\forall i\\in \\{1,...,n\\}$\n",
    "* $\\Sigma^{-1}$ denotes the inverse of the matrix $\\Sigma$\n",
    "\n",
    "Key term: $\\sqrt{(\\mathbf{x}-\\mathbf{\\mu})^T \\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})}$ is called [**Mahalanobis distance**](https://en.wikipedia.org/wiki/Mahalanobis_distance) between the point $\\mathbf{x}$ and the expectation $\\mathbf{\\mu}$.\n",
    "\n",
    "Special case: When $\\Sigma = \\sigma^2 I$ (identity matrix), reduces to $n$ independent $\\mathcal{N}(\\mu_i, \\sigma^2)$ variables.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719eb961",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application: Gaussian Processes & Anomaly Detection</h4>\n",
    "\n",
    "1. Gaussian Mixture Models (GMM):\n",
    "\n",
    "- Model data as mixture of $k$ multivariate Gaussians\n",
    "- Each cluster has its own $(\\mu_k, \\Sigma_k)$\n",
    "- Used for clustering, density estimation\n",
    "\n",
    "2. Anomaly Detection\n",
    "\n",
    "3. Gaussian Processes:\n",
    "\n",
    "- Infinite-dimensional generalization\n",
    "- Defines distribution over functions\n",
    "- Used for regression with uncertainty quantification\n",
    "\n",
    "4. Kalman Filters:\n",
    "\n",
    "- State estimation assumes multivariate normal distributions\n",
    "- Used in robotics, autonomous vehicles, time series\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117f43c",
   "metadata": {},
   "source": [
    "## Return to Opening Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8236bbc",
   "metadata": {},
   "source": [
    "Recall: $N=10$ ads, each liked/skipped with $p=0.5$.\n",
    "\n",
    "Bonus awarded when ad reaction differs from both neighbors.\n",
    "\n",
    "Question: What's the expected number of bonuses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d93d8",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27eb49",
   "metadata": {},
   "source": [
    "1. Step 1: Model as Random Vector\n",
    "\n",
    "Let's define indicator random variables: $$X_i = \\left\\{\\begin{array}{ll}1 & \\text{ if ad } i \\text{ gets a \"like\"}\\\\ 0 & \\text{ if \"skip\"}\\end{array}\\right. (i = 1, ..., 10)$$\n",
    "$X = (X_1, ..., X_{10})$ is our random vector\n",
    "\n",
    "Each $X_i \\sim Bernoulli(p=0.5)$, independent.\n",
    "\n",
    "2. Step 2: Define Bonus Indicators\n",
    "\n",
    "Let $$B_i = \\left\\{\\begin{array}{ll}1 & \\text{ if ad } i \\text{ earns a bonus}\\\\ 0 & \\text{ otherwise}\\end{array}\\right.$$\n",
    "\n",
    "Bonus condition for ad $i$ (where $1 ‚â§ i ‚â§ 10$):\n",
    "\n",
    "- Ad $i$ differs from $i-1$: $X_i \\neq X_{i-1}$\n",
    "- Ad $i$ differs from $i+1$: $X_i \\neq X_{i+1}$\n",
    "\n",
    "Both must be true! i.e. there are two winning patterns: $LSL$ and $SLS$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$B_i = I(X_i \\neq X_{i-1}\\text{ AND }X_i \\neq X_{i+1})$$\n",
    "\n",
    "Total bonuses: $S = B_1 + B_2 + ... + B_{10}$\n",
    "\n",
    "3. Step 3: Calculate $P(\\text{Bonus for ad }i)$\n",
    "\n",
    "For ad $i$ to get a bonus, we need $X_{i-1}, X_i, X_{i+1}$ in pattern:\n",
    "\n",
    "$(0, 1, 0)$ or $(1, 0, 1)$\n",
    "\n",
    "Since each ad is independent with $p=0.5$:\n",
    "- $P(X_{i-1}=0, X_i=1, X_{i+1}=0) = 0.5 √ó 0.5 √ó 0.5 = 1/8$\n",
    "- $P(X_{i-1}=1, X_i=0, X_{i+1}=1) = 0.5 √ó 0.5 √ó 0.5 = 1/8$\n",
    "- $P(B_i = 1) = 1/8 + 1/8 = 1/4$\n",
    "\n",
    "4. Step 4: Use Linearity of Expectation\n",
    "\n",
    "Key property: $E[X + Y] = E[X] + E[Y]$ (always true, even if $X$ and $Y$ are dependent!)\n",
    "\n",
    "$$E[S] = E[B_1 + B_2 + ... + B_{10}]\n",
    "     = E[B_1] + E[B_2] + ... + E[B_{10}] \\text{   (linearity!)}$$\n",
    "$$= (0\\times P(B_1=0) + 1\\times P(B_1=1)) + (0\\times P(B_2=0) + 1\\times P(B_2=1)) + ... + (0\\times P(B_{10}=0) + 1\\times P(B_{10}=1))$$\n",
    "$$     = P(B_1=1) + P(B_2=1) + ... + P(B_{10}=1)\n",
    "     = 10 √ó (1/4)\n",
    "     = 2.5$$\n",
    "\n",
    "\n",
    "**ANSWER:** expected number of bonuses is 2.5\n",
    "\n",
    "**Generalization**: For $N$ ads and $p$: $E[S] = Np(1-p)$. If $p=0.5$: $E[S] = N\\times 0.5 \\times 0.5 = N/4$.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/ad_carousel_circular_solution.svg\" alt=\"Summary of solution\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315b3f3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Why Intuition Failed</h4>\n",
    "\n",
    "Common wrong reasoning:\n",
    "\n",
    "- \"About half the ads should differ from neighbors\" ‚Üí Guess 5 bonuses\n",
    "- \"Dependencies make it complex\" ‚Üí Give up\n",
    "\n",
    "Why random vector approach works:\n",
    "\n",
    "- Breaks complex problem into simple pieces (indicator variables)\n",
    "- Linearity of expectation works even with dependencies\n",
    "- No need to enumerate all 2¬π‚Å∞ = 1,024 sequences\n",
    "- Scales to any $N$ (try N=100!)\n",
    "\n",
    "The Power of Abstraction: \n",
    "Random vectors + LOTUS + linearity = tractable solutions to seemingly impossible problems!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation\n",
    "\n",
    "def simulate_ad_carousel(n_ads=10, p_like=0.5, n_simulations=100000, circular=True):\n",
    "    \"\"\"\n",
    "    Simulate the ad carousel bonus problem to verify our theoretical result.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_ads : int\n",
    "        Number of ads in carousel\n",
    "    p_like : float\n",
    "        Probability of liking an ad\n",
    "    n_simulations : int\n",
    "        Number of simulations to run\n",
    "    circular : bool\n",
    "        If True, carousel wraps around (ad 1 and ad n are neighbors)\n",
    "        If False, linear arrangement (only ads 2 through n-1 can earn bonuses)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains average bonuses and distribution\n",
    "    \"\"\"\n",
    "    bonus_counts = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate random likes/skips (1 = like, 0 = skip)\n",
    "        reactions = np.random.binomial(1, p_like, n_ads)\n",
    "        \n",
    "        # Count bonuses\n",
    "        bonuses = 0\n",
    "        \n",
    "        if circular:\n",
    "            # ALL ads can earn bonuses in circular arrangement\n",
    "            for i in range(n_ads):\n",
    "                # Use modulo for circular indexing\n",
    "                left_neighbor = reactions[(i - 1) % n_ads]\n",
    "                right_neighbor = reactions[(i + 1) % n_ads]\n",
    "                \n",
    "                # Bonus if ad i differs from both neighbors\n",
    "                if reactions[i] != left_neighbor and reactions[i] != right_neighbor:\n",
    "                    bonuses += 1\n",
    "        else:\n",
    "            # Linear arrangement: only ads 2 through n-1 can earn bonuses\n",
    "            for i in range(1, n_ads - 1):\n",
    "                # Bonus if ad i differs from both neighbors\n",
    "                if reactions[i] != reactions[i-1] and reactions[i] != reactions[i+1]:\n",
    "                    bonuses += 1\n",
    "        \n",
    "        bonus_counts.append(bonuses)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_bonuses = np.mean(bonus_counts)\n",
    "    std_bonuses = np.std(bonus_counts)\n",
    "    \n",
    "    # Calculate theoretical expectation\n",
    "    if circular:\n",
    "        theoretical = n_ads * p_like * (1 - p_like) \n",
    "        arrangement = \"Circular\"\n",
    "    else:\n",
    "        theoretical = (n_ads - 2) * p_like * (1 - p_like) \n",
    "        arrangement = \"Linear\"\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    unique, counts = np.unique(bonus_counts, return_counts=True)\n",
    "    plt.bar(unique, counts / n_simulations, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    plt.axvline(avg_bonuses, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Simulated = {avg_bonuses:.3f}')\n",
    "    plt.axvline(theoretical, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Theoretical = {theoretical:.1f}')\n",
    "    plt.xlabel('Number of Bonuses', fontsize=12)\n",
    "    plt.ylabel('Probability', fontsize=12)\n",
    "    plt.title(f'{arrangement} Carousel: Distribution of Bonuses\\n({n_simulations:,} simulations)', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Example sequences\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show a few example sequences\n",
    "    examples_text = \"Sample Sequences:\\n\\n\"\n",
    "    for i in range(5):\n",
    "        reactions = np.random.binomial(1, p_like, n_ads)\n",
    "        reaction_symbols = ['L' if r == 1 else 'S' for r in reactions]\n",
    "        \n",
    "        bonuses_list = []\n",
    "        for j in range(1, n_ads - 1):\n",
    "            if reactions[j] != reactions[j-1] and reactions[j] != reactions[j+1]:\n",
    "                bonuses_list.append(j)\n",
    "        \n",
    "        examples_text += f\"{'  '.join(reaction_symbols)}\\n\"\n",
    "        examples_text += f\"Bonuses at positions: {bonuses_list if bonuses_list else 'None'}\\n\\n\"\n",
    "    \n",
    "    plt.text(0.1, 0.5, examples_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'average': avg_bonuses,\n",
    "        'std': std_bonuses,\n",
    "        'distribution': (unique, counts / n_simulations)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ae640",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_ad_carousel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f778a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive demo\n",
    "interactive_carousel_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def9498",
   "metadata": {},
   "source": [
    "Now, let's calculate Covariance matrix.\n",
    "\n",
    "From our previous analysis: $E[B_i] = P(B_i=1) = p(1-p)$ and for $p=0.5$: $E[B_i] = P(B_i=1) = p(1-p) = 0.5\\times 0.5 = 0.25$.\n",
    "\n",
    "1. Calculate Variance of $B_i$\n",
    "$Var(B_i) = E[B_i^2] - (E[B_i])^2$\n",
    "\n",
    "Since $B_$ is a Bernoulli random variable:\n",
    "\n",
    "Since $B_i \\in {0,1}$, we have $B_i^2 = B_i$, so: $E[B_i^2] = E[B_i] = p(1-p) = 0.25$\n",
    "\n",
    "Therefore:\n",
    "$Var(B_i) = p(1-p) - [p(1-p)]^2 = p(1-p)[1 - p(1-p)] = p(1-p)[1 - p + p^2]$\n",
    "        \n",
    "For $p = 0.5$:\n",
    "$Var(B_i) = 0.5 \\times 0.5 \\times [1 - 0.5 + 0.25] = 0.25 \\times 0.75 = 0.1875 = 3/16\n",
    "\n",
    "2. Calculate Covariance between bonus indicators\n",
    "\n",
    "The key formula for covariance: $Cov(B_i, B_j) = E[B_iB_j] - E[B_i]E[B_j]$\n",
    "\n",
    "Since all ads are symmetric (circular arrangement): $E[B_i] = E[B_j] = p(1-p) = 0.25$ (for $p = 0.5$)\n",
    "\n",
    "So we need to find: $E[B_iB_j]$\n",
    "\n",
    "This depends on the relationship between $i$ and $j$!\n",
    "\n",
    "- Case 1: $i = j$ (Same Ad)\n",
    "\n",
    "$Cov(B_i, B_i) = Var(B_i) = 3/16 = 0.1875$\n",
    "\n",
    "This is just the variance we calculated above.\n",
    "\n",
    "- Case 2: $|i - j| = 1$ (Adjacent Ads)\n",
    "\n",
    "Example: $Cov(B_2, B_3)$ where ad 2 and ad 3 are neighbors.\n",
    "\n",
    "```\n",
    "Ad 1 - Ad 2 - Ad 3 - Ad 4\n",
    "       ‚Üë      ‚Üë\n",
    "       B·µ¢     B‚±º\n",
    "```\n",
    "\n",
    "Note: </br>\n",
    "a. For $B_2 = 1$: Ad 2 must differ from BOTH ad 1 and ad 3 ($X_2 \\neq X_1$ AND $X_2 \\neq X_3$)</br>\n",
    "b. For $B_3 = 1$: Ad 3 must differ from BOTH ad 2 and ad 4 ($X_3 \\neq X_2$ AND $X_3 \\neq X_4$)\n",
    "\n",
    "So these ads are dependent. \n",
    "\n",
    "For a better demonstration, let's enumerate all possible patterns for $(X_1, X_2, X_3, X_4)$:\n",
    "\n",
    "|X‚ÇÅ|X‚ÇÇ|X‚ÇÉ|X‚ÇÑ|B‚ÇÇ (X‚ÇÇ‚â†X‚ÇÅ & X‚ÇÇ‚â†X‚ÇÉ)|B‚ÇÉ (X‚ÇÉ‚â†X‚ÇÇ & X‚ÇÉ‚â†X‚ÇÑ)|B‚ÇÇB‚ÇÉ|Probability|\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|0|0|0|0|0|0|0|p‚Å¥|\n",
    "|0|0|0|1|0|0|0|p¬≥(1-p)|\n",
    "|0|0|1|0|0|1|0|p¬≥(1-p)|\n",
    "|0|0|1|1|0|0|0|p¬≤(1-p)¬≤|\n",
    "|0|1|0|0|1|1|**1**|p¬≥(1-p)|\n",
    "|0|1|0|1|1|0|0|p¬≤(1-p)¬≤|\n",
    "|0|1|1|0|0|0|0|p¬≤(1-p)¬≤|\n",
    "|0|1|1|1|0|0|0|p(1-p)¬≥\n",
    "|1|0|0|0|0|0|0|p¬≥(1-p)|\n",
    "|1|0|0|1|0|0|0|p¬≤(1-p)¬≤|\n",
    "|1|0|1|0|1|1|**1**|p¬≤(1-p)¬≤|\n",
    "|1|0|1|1|1|0|0|p(1-p)¬≥|\n",
    "|1|1|0|0|0|0|0|p¬≤(1-p)¬≤|\n",
    "|1|1|0|1|0|1|0|p(1-p)¬≥|\n",
    "|1|1|1|0|0|0|0|p(1-p)¬≥|\n",
    "|1|1|1|1|0|0|0|(1-p)‚Å¥|\n",
    "\n",
    "Finding $E[B_2B_3]$:\n",
    "Only two patterns have $B_2B_3 = 1$:\n",
    "\n",
    "a. $(0, 1, 0, 0)$ with probability $p^3(1-p)$</br>\n",
    "b. $(1, 0, 1, 0)$ with probability $p^2(1-p)¬≤$\n",
    "\n",
    "For $p = 0.5$:\n",
    "$$E[B_2B_3] = (0.5)^3(0.5) + (0.5)^2(0.5)^2\n",
    "        = 0.125 √ó 0.5 + 0.25 √ó 0.25\n",
    "        = 0.0625 + 0.0625\n",
    "        = 0.125\n",
    "        = 1/8$$\n",
    "\n",
    "Calculate Covariance:\n",
    "$$Cov(B_2, B_3) = E[B_2B_3] - E[B_2]E[B_3]\n",
    "            = 1/8 - (1/4)(1/4)\n",
    "            = 1/8 - 1/16\n",
    "            = 2/16 - 1/16\n",
    "            = 1/16\n",
    "            = 0.0625$$\n",
    "\n",
    "Result for Adjacent Ads: $Cov(B_i, B_{i+1}) = 1/16$ (for p = 0.5)\n",
    "\n",
    "*Interpretation*: Adjacent bonuses are positively correlated! If ad $i$ gets a bonus, it slightly increases the chance that ad $i+1$ also gets a bonus.\n",
    "\n",
    "- Case 3: $|i - j| = 2$ (One Ad Between)\n",
    "\n",
    "Example: $Cov(B_2, B_4)$ where ads 2 and 4 have ad 3 between them.\n",
    "\n",
    "Configuration:\n",
    "\n",
    "```\n",
    "Ad 1 - Ad 2 - Ad 3 - Ad 4 - Ad 5\n",
    "       ‚Üë             ‚Üë\n",
    "       B·µ¢            B‚±º\n",
    "```\n",
    "\n",
    "For $B_2 = 1$: $X_2 \\neq X_1$ AND $X_2 \\neq X_3$</br>\n",
    "For $B_4 = 1$: $X_4 \\neq X_3‚ÇÉ$ AND $X_4 \\neq X_5$\n",
    "\n",
    "These share $X_3$ as a common neighbor.\n",
    "\n",
    "Key Pattern Analysis:\n",
    "\n",
    "For both $B_2 = 1$ and $B_4 = 1$:\n",
    "\n",
    "    * $B_2 = 1$ requires: $X_1 \\neq X_2 \\neq X_3$\n",
    "    * $B_4 = 1$ requires: $X_3 \\neq X_4 \\neq X_5$\n",
    "\n",
    "The key constraint: $X_2 \\neq X_3$ AND $X_3 \\neq X_4$. \n",
    "This means: $X_2 \\neq X_3 \\neq X_4$\n",
    "\n",
    "So we need an alternating pattern around position 3.\n",
    "\n",
    "If we consider all possible patterns for $(X_1, X_2, X_3, X_4, X_5)$ (32 cases), we will see that for $B_2B_4 = 1$, the pattern must be a perfect alternation, i.e.: $L-S-L-S-L$ or $S-L-S-L-S$. \n",
    "\n",
    "The associated probabilities are:\n",
    "    * (0, 1, 0, 1, 0) - probability = $p^2(1-p)^3$\n",
    "    * (1, 0, 1, 0, 1) - probability = $p^3(1-p)^2$\n",
    "\n",
    "For $p = 0.5$:\n",
    "$$E[B_2B_4] = (0.5)^2(0.5)^3 + (0.5)^3(0.5)^2\n",
    "        = 2 √ó (0.5)^5\n",
    "        = 2 √ó 1/32\n",
    "        = 1/16\n",
    "        = 0.0625$$\n",
    "\n",
    "Calculate Covariance:\n",
    "$$Cov(B‚ÇÇ, B‚ÇÑ) = E[B_2B_4] - E[B_2]E[B_4]\n",
    "            = 1/16 - (1/4)(1/4)\n",
    "            = 1/16 - 1/16\n",
    "            = 0$$\n",
    "\n",
    "Result for Ads with One Between: $Cov(B_i, B_{i+2}) = 0$ (for $p = 0.5$)\n",
    "\n",
    "Interpretation: These bonuses are uncorrelated when $p = 0.5$\n",
    "\n",
    "- Case 4: $|i - j| ‚â• 3$ (Non-overlapping Neighborhoods)\n",
    "\n",
    "Example: $Cov(B_2, B_5)$\n",
    "\n",
    "Configuration:\n",
    "\n",
    "```\n",
    "Ad 1 - Ad 2 - Ad 3 - Ad 4 - Ad 5 - Ad 6\n",
    "       ‚Üë                     ‚Üë\n",
    "       B·µ¢                    B‚±º\n",
    "```\n",
    "\n",
    "$B_2$ depends on: $X_1, X_2, X_3$\n",
    "$B_5$ depends on: $X_4, X_5, X_6$\n",
    "\n",
    "These are completely disjoint sets!\n",
    "\n",
    "Since all $X_i$ are independent:\n",
    "$E[B_2B_5] = E[B_2]E[B_5]$\n",
    "\n",
    "Therefore:\n",
    "$$Cov(B_2, B_5) = E[B_2B_5] - E[B_2]E[B_5]\n",
    "            = E[B_2]E[B_5] - E[B_2]E[B_5]\n",
    "            = 0$$\n",
    "\n",
    "Result for Non-overlapping Neighborhoods: $Cov(B_i, B_j) = 0$ for $|i - j| \\geq 3$\n",
    "\n",
    "3. Complete Covariance Matrix ($N=10$)\n",
    "\n",
    "$$\\begin{pmatrix}Var(X_1) & Cov(X_1,X_2) & Cov(X_1,X_3) & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0} & Cov(X_1,X_9) & Cov(X_1,X_{10})\\\\ Cov(X_2,X_1) & Var(X_2) & Cov(X_2,X_3) & Cov(X_2,X_4) & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0} & Cov(X_2,X_{10}) \\\\ Cov(X_3,X_1) & Cov(X_3,X_2) & Var(X_3) & Cov(X_3,X_4) & Cov(X_3,X_5) & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0}\\\\ \\mathbf{0} & Cov(X_4,X_2) & Cov(X_4,X_3) & Var(X_4) & Cov(X_4,X_5) & Cov(X_4,X_6) & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & Cov(X_5,X_3) & Cov(X_5,X_4) & Var(X_5) & Cov(X_5,X_6) & Cov(X_5,X_7) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_6,X_4) & Cov(X_6,X_5) & Var(X_6) & Cov(X_6,X_7) & Cov(X_6,X_8) & \\mathbf{0} & \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_7,X_5) & Cov(X_7,X_6) & Var(X_7) & Cov(X_7,X_8) & Cov(X_7,X_9) & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_8,X_6) & Cov(X_8,X_7) & Var(X_8) & Cov(X_8,X_9) & Cov(X_8,X_{10}) \\\\ Cov(X_9,X_1) & \\mathbf{0} &\\mathbf{0} &\\mathbf{0} & \\mathbf{0} &\\mathbf{0} & Cov(X_9,X_7) & Cov(X_9,X_8) & Var(X_9) & Cov(X_9,X_{10}) \\\\ Cov(X_{10},X_1) & Cov(X_{10},X_2) & \\mathbf{0} & \\mathbf{0} &\\mathbf{0} &\\mathbf{0} & \\mathbf{0} &  Cov(X_{10},X_8) & Cov(X_{10},X_9) & Var(X_{10})\\end{pmatrix}$$\n",
    "\n",
    "Given that $Cov(X_1, X_3) = Cov(X_2, X_4) = ... = Cov(X_{N-1}, X_1) = Cov(X_N, X_2) = 0$:\n",
    "\n",
    "$$\\begin{pmatrix}Var(X_1) & Cov(X_1,X_2) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0} & \\mathbf{0} & Cov(X_1,X_{10})\\\\ Cov(X_2,X_1) & Var(X_2) & Cov(X_2,X_3) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & Cov(X_3,X_2) & Var(X_3) & Cov(X_3,X_4) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & Cov(X_4,X_3) & Var(X_4) & Cov(X_4,X_5) & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_5,X_4) & Var(X_5) & Cov(X_5,X_6) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_6,X_5) & Var(X_6) & Cov(X_6,X_7) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_7,X_6) & Var(X_7) & Cov(X_7,X_8) & \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & Cov(X_8,X_7) & Var(X_8) & Cov(X_8,X_9) & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} &\\mathbf{0} &\\mathbf{0} & \\mathbf{0} &\\mathbf{0} & \\mathbf{0} & Cov(X_9,X_8) & Var(X_9) & Cov(X_9,X_{10}) \\\\ Cov(X_{10},X_1) & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} &\\mathbf{0} &\\mathbf{0} & \\mathbf{0} &  \\mathbf{0} & Cov(X_{10},X_9) & Var(X_{10})\\end{pmatrix}$$\n",
    "\n",
    "which equals:\n",
    "$$\\begin{pmatrix}\\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0} & \\mathbf{0} & \\frac{1}{16}\\\\ \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0}& \\mathbf{0}& \\mathbf{0}& \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} &\\mathbf{0} &\\mathbf{0} & \\mathbf{0} &\\mathbf{0} & \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16} & \\frac{1}{16} \\\\ \\frac{1}{16} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} &\\mathbf{0} &\\mathbf{0} & \\mathbf{0} &  \\mathbf{0} & \\frac{1}{16} & \\frac{3}{16}\\end{pmatrix}$$\n",
    "\n",
    "This is a tridiagonal matrix with wrap-around (circulant structure).\n",
    "\n",
    "Now, we can calculate Variance of Total Bonuses $S$.\n",
    "\n",
    "$S = B_1 + B_2 + ... + B_{10}$\n",
    "\n",
    "$$Var(S) = Var(\\sum B_i) = \\sum \\sum Cov(B_i, B_j) = \\sum_i Var(B_i) + \\sum_{i\\neq j}Cov(B_i, B_j)$$\n",
    "\n",
    "Note that:\n",
    "$$\\sum_{i=1}^N\\sum_{\\begin{matrix}j=1\\\\j\\neq i\\end{matrix}}^N Cov(X_i,X_j) = 2N\\times Cov(X_1,X_2) + 2N\\times Cov(X_1,X_3) = 2N\\times \\frac{1}{16} + 2N\\times 0 = \\frac{2N}{16}$$\n",
    "\n",
    "Diagonal terms ($i = j$): $10 \\times (3/16) = 30/16$\n",
    "\n",
    "Adjacent terms ($|i-j| = 1$, including wrap): \n",
    "  - Each of 10 ads has 2 neighbors\n",
    "  - But we count each pair once\n",
    "  - Total: $10 pairs \\times (1/16) \\times 2 = 20/16$\n",
    "\n",
    "All other terms: 0\n",
    "\n",
    "Hence: $$Var(X) = \\frac{3N}{16} + \\frac{2N}{16} + 0 = \\frac{5N}{16} = 50/16 = 3.125$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4b5fd",
   "metadata": {},
   "source": [
    "## AI Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe751b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ From Ad Carousels to Modern AI</h4>\n",
    "This same mathematical framework powers:\n",
    "\n",
    "1. Neural Network Training:\n",
    "\n",
    "- Each layer: h^(l) = œÉ(W^(l)h^(l-1) + b^(l))\n",
    "- Gradients are expectations over random mini-batches\n",
    "- LOTUS justifies stochastic gradient descent\n",
    "\n",
    "\n",
    "2. Reinforcement Learning:\n",
    "\n",
    "- State-action pairs form random vectors\n",
    "- Expected return: E[Œ£·µó Œ≥·µó r(s‚Çú, a‚Çú)]\n",
    "- Linearity enables policy gradient methods\n",
    "\n",
    "\n",
    "3. Recommendation Systems:\n",
    "\n",
    "- User features + item features = joint distribution\n",
    "- Expected utility guides recommendations\n",
    "- Covariance matrix reveals latent factors\n",
    "\n",
    "4. Anomaly Detection:\n",
    "\n",
    "- Sensor readings form multivariate normal\n",
    "- Mahalanobis distance identifies outliers\n",
    "- Used in fraud detection, cybersecurity, manufacturing\n",
    "\n",
    "\n",
    "Common Thread: Model uncertainty with random vectors, use mathematical properties (linearity, LOTUS, covariance) to make decisions efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bae17b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "<h4>üè≠ Industry Case Study</h4>\n",
    "\n",
    "Context: Google's ad auction determines which ads to show based on multiple factors:\n",
    "\n",
    "- Bid amount ($B$)\n",
    "- Quality score ($Q$)\n",
    "- Click-through rate ($C$)\n",
    "- User relevance ($R$)\n",
    "\n",
    "Problem: Each factor is uncertain (random variable). Google needs to compute:\n",
    "\n",
    "- Expected revenue: $E[B √ó C]$\n",
    "- Expected user satisfaction: $E[Q √ó R]$\n",
    "- Variance in outcomes: $Var(B √ó C √ó Q)$\n",
    "\n",
    "Solution using Random Vectors:\n",
    "\n",
    "- Model $(B, Q, C, R)$ as a 4-dimensional random vector\n",
    "- Historical data provides joint distribution\n",
    "- Use LOTUS to compute $E[B √ó C]$ without deriving distribution of the product\n",
    "- Covariance matrix reveals correlations:\n",
    "    * High $Q$ often correlates with high $C$\n",
    "    * This information improves auction design\n",
    "\n",
    "\n",
    "Impact:\n",
    "\n",
    "- Billions of auctions per day\n",
    "- Millisecond decision times required\n",
    "- Random vector theory enables real-time optimization\n",
    "- LOTUS makes expectations computationally tractable\n",
    "\n",
    "Takeaway: Without random vector formalism, this system would require intractable Monte Carlo simulation for each auction!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6b003",
   "metadata": {},
   "source": [
    "## Common Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d7597",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h4>‚ö†Ô∏è Common Mistakes to Avoid</h4>\n",
    "\n",
    "1. Confusing joint and marginal distributions\n",
    "\n",
    "- Marginal ‚â† conditional\n",
    "- Integrating out vs. conditioning\n",
    "\n",
    "\n",
    "2. Forgetting that linearity works even with dependence\n",
    "\n",
    "- E[X + Y] = E[X] + E[Y] always holds\n",
    "- Don't need independence for linearity!\n",
    "\n",
    "\n",
    "3. Trying to derive intermediate distributions unnecessarily\n",
    "\n",
    "- Use LOTUS instead!\n",
    "- Don't make life harder than it needs to be\n",
    "\n",
    "\n",
    "4. Assuming independence without checking\n",
    "\n",
    "- Check covariance matrix\n",
    "- Independence: f(x,y) = f_X(x)f_Y(y)\n",
    "\n",
    "\n",
    "5. Misinterpreting covariance\n",
    "\n",
    "- Cov(X,Y) = 0 doesn't imply independence\n",
    "- (Independence ‚Üí zero covariance, but not vice versa)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3b890",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5786de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-summary\">\n",
    "<h4>üìã Key Takeaways</h4>\n",
    "Three Big Ideas:\n",
    "\n",
    "1. Random Vectors = Joint Behavior\n",
    "\n",
    "- Model multiple uncertain quantities simultaneously\n",
    "- Captures dependencies through joint distributions\n",
    "- Foundation for all multivariate statistics\n",
    "\n",
    "\n",
    "2. LOTUS = Computational Efficiency\n",
    "\n",
    "- Calculate E[g(X)] without finding distribution of g(X)\n",
    "- Essential for ML: loss functions, gradients, expectations\n",
    "- Saves enormous computational effort\n",
    "\n",
    "\n",
    "3. Linearity = Problem Decomposition\n",
    "\n",
    "- E[X + Y] = E[X] + E[Y] (even if dependent!)\n",
    "- Break complex problems into simple pieces\n",
    "- Key to analyzing indicator variables\n",
    "\n",
    "\n",
    "\n",
    "How They Connect:\n",
    "Random vectors ‚Üí Model joint distributions ‚Üí LOTUS for expectations ‚Üí Linearity simplifies calculations\n",
    "\n",
    "ML Applications Summary:\n",
    "\n",
    "- Neural networks: Weight vectors, activations, gradients\n",
    "- Dimensionality reduction: PCA uses covariance matrices\n",
    "- Classification: Multinomial models, Gaussian discriminant analysis\n",
    "- Anomaly detection: Mahalanobis distance in multivariate normal\n",
    "- Optimization: Portfolio theory, resource allocation\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probability-statistics-ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
