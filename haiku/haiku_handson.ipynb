{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29f3e8b",
   "metadata": {},
   "source": [
    "# Lab: Writing Haiku with Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/styles.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8f750",
   "metadata": {},
   "source": [
    "This lab is inspired by [*'Impractical Python Projects: playful programming activities to make you smarter'*](https://nostarch.com/impracticalpythonprojects) by Lee Vaughan. First edition. San Francisco: No Starch Press, Inc. 2019. ISBN 9781593278915"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4846dab",
   "metadata": {},
   "source": [
    "> \"The complexity of poetic interaction, the tricky dance among poet and text and reader, causes a game of hesitation. In this game, a properly programmed computer has a chance to slip in some interesting moves\" (Charles Hartman, 1996 *Virtual Muse: Experiments in Computer Poetry*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd161da1",
   "metadata": {},
   "source": [
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Haiku), **haiku** is a type of short form of Japanese poetry that traditionally consist of three lines composed of 17 syllables in a 5, 7, and 5 pattern. Tradtionally, nature - mainly the seasons - is taken as a subject.\n",
    "\n",
    "<center>\n",
    "Worker bees can leave. <br>\n",
    "Even drones can fly away. <br>\n",
    "The Queen is their slave. <br>\n",
    "\n",
    "(Chuck Palahniuk, *The Fight Club*)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa371e4f",
   "metadata": {},
   "source": [
    "## Problem Statememt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed072f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-problem\">\n",
    "Given haiku corpus, write a program that generates haiku using Markov chain model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b44bd5",
   "metadata": {},
   "source": [
    "## Load and Preprocess Haiku Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661493c",
   "metadata": {},
   "source": [
    "Let's first download our [Haiku corpus](https://github.com/rlvaugh/Impractical_Python_Projects/blob/master/Chapter_8/train.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02468bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# haiku file for training\n",
    "corpus_file = \"https://raw.githubusercontent.com/rlvaugh/Impractical_Python_Projects/master/Chapter_8/train.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4eac2d",
   "metadata": {},
   "source": [
    "This corpus from the aforementioned book contains almost 300 ancient and modern haiku, more than 200 of which were written by the masters. The haiku in the initial corpus were duplicated 18 times and randomly distributed across the file to increase the number of values per key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # URL usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(url: str) -> str:\n",
    "    \"\"\"Loads and returns training corpus of haiku.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL to the file containing the corpus\n",
    "\n",
    "    Returns:\n",
    "        corpus (str): loaded corpus\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data_raw = response.text\n",
    "        \n",
    "    return data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_set(data_raw: str) -> set:\n",
    "    \"\"\"Gets a word set from a string.\n",
    "\n",
    "    Args:\n",
    "        data_raw (str): corpus string\n",
    "\n",
    "    Returns:\n",
    "        set: a set of words\n",
    "    \"\"\"\n",
    "    data_cleaned = data_raw.replace(\"-\", \" \")\n",
    "    word_set = set(data_cleaned.split())\n",
    "    \n",
    "    return word_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw text as a string\n",
    "raw = load_corpus(corpus_file)\n",
    "print(raw)\n",
    "\n",
    "# load a set of words from the raw text\n",
    "word_set = get_word_set(raw)\n",
    "print(word_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d26e5a",
   "metadata": {},
   "source": [
    "Let's also apply some very basic preprocessing that splits our corpus into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be able to remove punctuation\n",
    "from string import punctuation\n",
    "\n",
    "def text_preprocessing(corpus: str) -> list: \n",
    "    \"\"\"Performs a basic preprocessing to the corpus text by replacing new lines and splitting to words on spaces. \n",
    "\n",
    "    Args:\n",
    "        corpus (str): Text corpus\n",
    "\n",
    "    Returns:\n",
    "        list: list of words given in the order as they appear in the corpus\n",
    "    \"\"\"\n",
    "    # lowercase the text\n",
    "    corpus = corpus.lower()\n",
    "    # remove new line\n",
    "    corpus = corpus.replace(\"\\n\", \" \")\n",
    "    # remove punctuation\n",
    "    corpus = ''.join(char for char in corpus if char not in punctuation)\n",
    "    # split into words on spaces\n",
    "    corpus_list = corpus.split()\n",
    "    \n",
    "    return corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the raw text into a list of words\n",
    "corpus_list = text_preprocessing(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5eb0c0",
   "metadata": {},
   "source": [
    "## Preliminaries: Counting Syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589113d",
   "metadata": {},
   "source": [
    "To respect haiku syllabic structure, we need to have to be able to count the number of syllables in words and phrases.\n",
    "\n",
    "<div class=\"alert alert-problem\">\n",
    "<div class=\"problem objectives\">\n",
    "Given a text input, count the number of syllables in each word, and return the total syllable count.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "To do so, we are going to use a syllable-count corpus, in particular, the [*Carnegie Mellon University Pronounciation Dictionary* (CMUdict)](http://www.speech.cs.cmu.edu/cgi-bin/cmudict) available via [NLTK (the Natural Language Toolkit)](https://www.nltk.org/). As some words may be missing from this corpus, we are going to use [`pyphen`](https://pypi.org/project/pyphen/) library allowing to hyphenate text using existing Hunspell hyphenation dictionaries. We are going to focus on English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # NLP toolkit\n",
    "from nltk.corpus import cmudict # Carnegie Mellon University Pronouncing Dictionary\n",
    "import pyphen # library to count syllables using hyphenation dictionaries\n",
    "\n",
    "nltk.download('cmudict') # download CMUdict <- takes a bit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMUdict\n",
    "cmudict = cmudict.dict()\n",
    "# hyphenation\n",
    "hyph = pyphen.Pyphen(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc6f84",
   "metadata": {},
   "source": [
    "Let's examine how the word *minds* is presented in CMU dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09911c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict['minds']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca07a55",
   "metadata": {},
   "source": [
    "The CMUdict breaks every word into set of phonemes and marks vowels using numbers for stress: 0=no stress, 1=primary, and 2=secondary.\n",
    "\n",
    "The vowels that are not pronounced are not included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb488fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict['care']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e83bc",
   "metadata": {},
   "source": [
    "When multiple and cosecutive written vowels form a single phoneme, only the latter is presented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict['mouse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab428",
   "metadata": {},
   "source": [
    "Finally, a word may have multiple distinct pronounciations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd705cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict['read']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b61366",
   "metadata": {},
   "source": [
    "We can use numbers to identify the vowels and count the number of syllables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \"mistake\"\n",
    "print(f\"Transcription of the word '{w}': {cmudict[w]}\")\n",
    "\n",
    "vow_list = [char for char in cmudict[w][0] if char[-1].isdigit()]\n",
    "print(f\"List of vowels: {vow_list}\")\n",
    "\n",
    "print(f\"Number of syllables: {len(vow_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2910749",
   "metadata": {},
   "source": [
    "Even though CMUdict is a very accurate tool, some words might be missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict['covid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1f237",
   "metadata": {},
   "source": [
    "To deal with missing words, we can use `pyphen` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ae6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyph.inserted('covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6a2b1",
   "metadata": {},
   "source": [
    "Note that `pyphen` representation is different from the one of CMUdict. In this case, in order to count the number of syllables, we can split the result based on the character `-` and count the number of elements in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyph_word = hyph.inserted(w)\n",
    "print(hyph_word)\n",
    "\n",
    "list_syl = hyph_word.split('-')\n",
    "print(f\"List of syllables of the word '{w}': {list_syl}\")\n",
    "\n",
    "print(f\"Number of syllables: {len(list_syl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2567187",
   "metadata": {},
   "source": [
    "Our main strategy is the following: for a given word, check if it is present in the CMUdict. It it is the case, then count the number of syllables. If not, use `pyphen` to count the number of syllables. \n",
    "\n",
    "We might also want to handle some special cases:\n",
    "\n",
    "1. the words ending with \"*'s*\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \"sister's\"\n",
    "print(f\"Original text: {w}\")\n",
    "if w.endswith(\"'s\") or w.endswith(\"\\u2019s\"):\n",
    "    w = w[:-2]\n",
    "print(f\"Updated text: {w}\")\n",
    "\n",
    "# curl apostophe\n",
    "w = \"sister\\u2019s\"\n",
    "print(f\"Original text: {w}\")\n",
    "if w.endswith(\"'s\") or w.endswith(\"\\u2019s\"):\n",
    "    w = w[:-2]\n",
    "print(f\"Updated text: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bf988",
   "metadata": {},
   "source": [
    "2. force a pronounciation of the words with multiple options, like the word *sake* that may refer to a Japanese drink and therefore, have 2 syllables in contrast to the pronounciation from the CMUdict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90660588",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict['sake']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a17243",
   "metadata": {},
   "source": [
    "To deal with that, we can introduce a dictionary mapping the words with the number of syllables. If a word is present in this list, then the corresponding value will be returned. Here's the code that asks a user to manually insert such exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "def make_change_exceptions_dict(missing_words: dict) -> dict:\n",
    "    \"\"\"Allows to make a manual change to missing_words and returns updated dict.\n",
    "\n",
    "    Args:\n",
    "        missing_words (dict): dictionary of word - syllables count pairs\n",
    "\n",
    "    Returns:\n",
    "        dict: updated missing_words dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Make change to dictionary before saving?\")\n",
    "    print(\"\"\"\n",
    "          0 - Exit & Save \n",
    "          1 - Add a word or Change a syllable count\n",
    "          2 - Remove a word\n",
    "          \"\"\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"Enter your choice: \")\n",
    "        if choice == '0':\n",
    "            break \n",
    "        if choice == '1':\n",
    "            w = input(\"\\nWord to add or change: \")\n",
    "            num_sylls = input(f\"Enter the number of syllables in {w}\")\n",
    "            if num_sylls.isdigit():\n",
    "                break\n",
    "            else:\n",
    "                print(\"   Not a valid answer!\", file=sys.stderr)\n",
    "            # insert a word-num syllables pair into the dictionary\n",
    "            missing_words[w] = int(num_sylls)\n",
    "            \n",
    "        elif choice == '2':\n",
    "            w = input(\"Enter word to delete: \")\n",
    "            missing_words.pop(w, None)\n",
    "        \n",
    "    return missing_words\n",
    "\n",
    "def make_exceptions_dict(exception_set: set) -> dict:\n",
    "    \"\"\"Returns a dictionary of words and corresponding number of syllables from a set of words.\n",
    "\n",
    "    Args:\n",
    "        exception_set (set): a set of words for which the number of syllables should be inserted.\n",
    "\n",
    "    Returns:\n",
    "        dict: word - syllable count dictionary\n",
    "    \"\"\"\n",
    "    missing_words = {} # resulting dictionary\n",
    "    \n",
    "    print(\"Insert the number of syllables in word.\")\n",
    "    \n",
    "    for w in exception_set:\n",
    "        while True:\n",
    "            num_sylls = input(f\"Enter the number of syllables in {w}\")\n",
    "            if num_sylls.isdigit():\n",
    "                break\n",
    "            else:\n",
    "                print(\"   Not a valid answer!\", file=sys.stderr)\n",
    "        # insert a word-num sylls pair into the dictionary\n",
    "        missing_words[w] = int(num_sylls)\n",
    "    \n",
    "    print()    \n",
    "    pprint.pprint(missing_words, width=1)\n",
    "    \n",
    "    missing_words = make_change_exceptions_dict(missing_words)\n",
    "    print(\"New words or syllable changes\")\n",
    "    pprint.pprint(missing_words, width=1)\n",
    "    \n",
    "    return missing_words\n",
    "\n",
    "def save_exceptions(missing_words: dict) -> None:\n",
    "    \"\"\"Saves word-syllables count dictionary to a JSON file\n",
    "\n",
    "    Args:\n",
    "        missing_words (dict): word-syllables count dictionary\n",
    "    \"\"\"\n",
    "    json_str = json.dumps(missing_words)\n",
    "    \n",
    "    f = open('data/missing_words.json', 'w')\n",
    "    f.write(json_str)\n",
    "    f.close()\n",
    "    \n",
    "    print(\"The dictionary has been saved to the file 'data/missing_words.json'\")\n",
    "    \n",
    "def load_exceptions() -> dict: \n",
    "    \"\"\"Loads word-syllables count dictionary from json file and returns it as a dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: word-syllables count dictionary\n",
    "    \"\"\"\n",
    "    missing_words = {}\n",
    "    \n",
    "    with open('data/missing_words.json') as f:\n",
    "        missing_words = json.load(f)\n",
    "    \n",
    "    return missing_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code if you want to update the exception list\n",
    "except_set = set(['sake'])\n",
    "mis_words = make_exceptions_dict(except_set)\n",
    "save_exceptions(mis_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionary\n",
    "missing_words = load_exceptions()\n",
    "print(missing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d82a8",
   "metadata": {},
   "source": [
    "\n",
    "Now, to be able to deal with a word or phrase, we need to: \n",
    "- lowercase or uppercase the text (`words.lower()`);\n",
    "- split a phrase into words (`words.split()`);\n",
    "- remove punctuation (`word.strip(punctuation)`);\n",
    "- apply the count procedure:\n",
    "   * handle words ending with \"'s\";\n",
    "   * check if a word is an exception from the dictionary and if it's the case, use the correspoding value of the syllables;\n",
    "   * if a word is from CMUdict, get its number of syllables (let's force the first pronounciation for simplicity);\n",
    "   * else use `pyphen`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22d3b6",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 1:</h5> Write `count_syllables()` function:\n",
    "\n",
    "```\n",
    "def count_syllables(words: str) -> int:\n",
    "    \"\"\"Counts syllables in English word or phrases.\n",
    "\n",
    "    Args:\n",
    "        words (str): a text to be analysed\n",
    "\n",
    "    Returns:\n",
    "        int: syllables count\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def count_syllables(words: str) -> int:\n",
    "    \"\"\"Counts syllables in English word or phrases.\n",
    "\n",
    "    Args:\n",
    "        words (str): a text to be analysed\n",
    "\n",
    "    Returns:\n",
    "        int: syllables count\n",
    "    \"\"\"\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e1c1d",
   "metadata": {},
   "source": [
    "Let's test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: \n",
    "    print(\"Test syllables counter\")\n",
    "    txt = input(\"Enter a word or a phrase\")\n",
    "    if txt == '':\n",
    "        sys.exit()\n",
    "    try:\n",
    "        num_sylls = count_syllables(txt)\n",
    "        print(f\"Number of syllables in '{txt}' : {num_sylls}\")\n",
    "    except KeyError:\n",
    "        print(\"Word not found. Try again.\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5658f",
   "metadata": {},
   "source": [
    "## Markov Model for Next Word Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5cb1a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-idea\">\n",
    "<h4> 💡 Idea: put the best words in the best order</h4>\n",
    "\n",
    "<div class=\"idea-steps\">\n",
    "<h5>Questions to ask:</h5>\n",
    "<ul>\n",
    "<li> How to understand what are \"the best words?\"\n",
    "\n",
    "> Need for good examples, i.e. a training corpus of haiku\n",
    "<\\li>\n",
    "<li> How to determine their \"best order\"?\n",
    "\n",
    "> Markov chains allow to predict the next (subsequent) state based on the properties of the current state. In our case, state = word.\n",
    "<\\li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "To illustrate the idea, think about the following examples. When someone says, \"*May the Force be...*\", we automatically think about \"*with you*\", or when we hear \"*Houston, we have a...*\", we end the phrase with \"*problem*\". So, certain word sequences strongly predict what comes next.\n",
    "\n",
    "Here are some **key considerations**:\n",
    "\n",
    "1. **Context creates probability distributions**: \"*Elementary, my dear ...*\" $\\rightarrow$ \"*Watson*\" has much higher probability than random words\n",
    "2. **Multiple valid completion exist**: \"*Once upon a...*\" $\\rightarrow  \\left\\{\\begin{array}{l} time \\\\ December \\\\ midnight\\end{array}\\right.$ with different probabilities\n",
    "3. **Cultural knowledge influences predictions**: \"*Luke, I am your ...*\" strongly suggests \"*father*\" due to pop culture \n",
    "4. **Transition probabilities aren't uniform**: some word pairs are far more likely than others based on training data\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<p>The <b>order</b> <strong>n</strong> (or <b>memory</b>) of a Markov chain determines how many previous states influence the prediction of the next state.</p>\n",
    "<p>In our case, a model of order <strong>n</strong> means that the next word depends on the previous <strong>n</strong> words. </p>\n",
    "<ul>\n",
    "<li>Higher order = more context = potentially better predictions</li>\n",
    "<li>Higher order = more parameters = more training data needed</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ec2a5",
   "metadata": {},
   "source": [
    "Let's see how different orders work with the phrase: \"*The quick brown fox jumps*\"\n",
    "\n",
    "|Order|Name|Context Used|Next Word Prediction|Example|\n",
    "|---|---|---|---|---|\n",
    "|n=0|Zero-order (Unigram)|No previous words|Based only on word frequency in corpus|$P(word) = count(word) / total_words$|\n",
    "|n=1|First-order (Bigram)|Previous 1 word|$P(next\\|current)$|$P(next \\| ``jumps\")$|\n",
    "|n=2|Second-order (Trigram)|Previous 2 words|$P(next\\|prev2, prev1)$|$P(next \\| ``fox\", ``jumps\")$|\n",
    "\n",
    "Thus, given a corpus, it is possible to kep a track of what is a consequent word for a gien word ($n=1$) or a pair of words ($n=2$). \n",
    "\n",
    "Let's consider a corpus of 2 haiku:\n",
    "\n",
    "|Haiku 1 | Haiku 2|\n",
    "|---|---|\n",
    "|Beneath the old oak<br/>Dark stones line the babbling brook<br/>Sunlight through green leaves | Waves crash on dark stones<br/> Salt spray kisses the moonlight<br/>Ocean's ancient song|\n",
    "\n",
    "Let $n=1$. Now, let's look what a Python's dictionary mapping each word to each subsequent word looks like:\n",
    "\n",
    "```\n",
    " 'ancient': ['song'],\n",
    " 'babbling': ['brook'],\n",
    " 'beneath': ['the'],\n",
    " 'brook': ['sunlight'],\n",
    " 'crash': ['on'],\n",
    " 'dark': ['stones', 'stones'],\n",
    " 'green': ['leaves'],\n",
    " 'kisses': ['the'],\n",
    " 'leaves': ['waves'],\n",
    " 'line': ['the'],\n",
    " 'moonlight': [\"ocean's\"],\n",
    " 'oak': ['dark'],\n",
    " \"ocean's\": ['ancient'],\n",
    " 'old': ['oak'],\n",
    " 'on': ['dark'],\n",
    " 'salt': ['spray'],\n",
    " 'song': [],\n",
    " 'spray': ['kisses'],\n",
    " 'stones': ['line', 'salt'],\n",
    " 'sunlight': ['through'],\n",
    " 'the': ['old', 'babbling', 'moonlight'],\n",
    " 'through': ['green'],\n",
    " 'waves': ['crash']\n",
    "```\n",
    "\n",
    "Note that most words have only a single-word list in the corresponding value. This is due to our corpus size. However, if you check the results for `'the'`, you can see `['old', 'babbling', 'moonlight']`.\n",
    "\n",
    "Note as well that for the key `'dark'`, we obtain the word `'stones'` twice. This is because we store every occurrence of a word as a separate, duplicate value. **Such repetitions allow to capture the statistical frequency of words.** In our case, it means that the word `'stones'` have more chances to occur after the word `'dark'` than other potential words.\n",
    "\n",
    "You can also note that the mapping continues from the first haiku to the second one, so the dictionary contains the items like `'leaves': ['waves']`.\n",
    "\n",
    "Now, let $n=2$. In this case, the keys of the dictionary will be composed of bigrams. Let's see what dictionary we can obtain:\n",
    "\n",
    "```\n",
    " 'ancient song': [],\n",
    " 'babbling brook': ['sunlight'],\n",
    " 'beneath the': ['old'],\n",
    " 'brook sunlight': ['through'],\n",
    " 'crash on': ['dark'],\n",
    " 'dark stones': ['line', 'salt'],\n",
    " 'green leaves': ['waves'],\n",
    " 'kisses the': ['moonlight'],\n",
    " 'leaves waves': ['crash'],\n",
    " 'line the': ['babbling'],\n",
    " \"moonlight ocean's\": ['ancient'],\n",
    " 'oak dark': ['stones'],\n",
    " \"ocean's ancient\": ['song'],\n",
    " 'old oak': ['dark'],\n",
    " 'on dark': ['stones'],\n",
    " 'salt spray': ['kisses'],\n",
    " 'spray kisses': ['the'],\n",
    " 'stones line': ['the'],\n",
    " 'stones salt': ['spray'],\n",
    " 'sunlight through': ['green'],\n",
    " 'the babbling': ['brook'],\n",
    " 'the moonlight': [\"ocean's\"],\n",
    " 'the old': ['oak'],\n",
    " 'through green': ['leaves'],\n",
    " 'waves crash': ['on']\n",
    "```\n",
    "\n",
    "As haiku are short and available training corpus is relatively small, we can limit $n=2$ to get some creativity and enforce some order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d67a95",
   "metadata": {},
   "source": [
    "Now let's write Python functions allowing to create such dictionaries of order 1 and 2 and apply them to the corpus containing two aforementioned haiku for testing.\n",
    "\n",
    "To do so, we are going to need to prepare our corpus using `text_preprocessing()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a69f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example\n",
    "txt_1 = \"Beneath the old oak\\nDark stones line the babbling brook\\nSunlight through green leaves.\"\n",
    "txt_2 = \"Waves crash on dark stones\\nSalt spray kisses the moonlight\\nOcean's ancient song.\"\n",
    "# concatenate the text\n",
    "txt = txt_1 + ' ' + txt_2\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text processing to our toy example \n",
    "# and split the text into a list of words\n",
    "corp_list = text_preprocessing(txt)\n",
    "print(corp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af73138",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 2:</h5> Write the function `map_word_to_word()` that maps each word from a corpus to its subsequent word and returns a dictionary.</p>\n",
    "\n",
    "```\n",
    "def map_word_to_word(corpus_list: list) -> dict[str, list]:   \n",
    "    \"\"\"Maps each word from a corpus to its subsequent word and returns a dictionary.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): a list of words representing texts to be analysed\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary mapping each word to a list of its subsequent words\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def map_word_to_word(corpus_list: list) -> dict[str, list]:   \n",
    "    \"\"\"Maps each word from a corpus to its subsequent word and returns a dictionary.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): a list of words representing texts to be analysed\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary mapping each word to a list of its subsequent words\n",
    "    \"\"\"\n",
    "        \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7888bcd",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5>QUESTION 3:</h5> Test it first on our toy example. Then, apply the function to our `corpus_list` and save the result to a variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46204f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52426cef",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5>QUESTION 4:</h5> Write the function `map_bigram_to_word()` that maps each pair of words from a corpus to its subsequent word and returns a dictionary.\n",
    "\n",
    "```\n",
    "def map_bigram_to_word(corpus_list: list) -> dict[str, list]:   \n",
    "    \"\"\"Maps each pair of words (bigram) from a corpus to its subsequent word and return a dictionary.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): a list of words representing texts to be analysed\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary mapping each pair of words (bigram) to a list of its subsequent words\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def map_bigram_to_word(corpus_list: list) -> dict[str, list]:   \n",
    "    \"\"\"Maps each pair of words (bigram) from a corpus to its subsequent word and return a dictionary.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): a list of words representing texts to be analysed\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary mapping each pair of words (bigram) to a list of its subsequent words\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6209a0",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 5: </h5> Test it first on our toy corpus. Then, apply it to `corpus_list` and save the result to a variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecef2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de51344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de461af",
   "metadata": {},
   "source": [
    "Now, let's write a function that applies our Markov models. Given a prefix (a single word / a pair of words), a mapping dictionary for order-1 or order-2 model, the current number of syllables and the target budget, get a list of all candidate words from the dictionary for a given prefix (key).\n",
    "\n",
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 6: </h5> Write a function `word_after_prefix()` that applies our Markov models. Try it with order-1 and order-2 models.\n",
    "\n",
    "```\n",
    "def word_after_prefix(prefix: str, map_dict: dict, current_syls: int, target_syls: int) -> list:\n",
    "    \"\"\"Given a prefix (a single word / a pair of words), a mapping dictionary for order-1 model, \n",
    "    the current number of syllables and the target budget, get a list of all candidate words from the dictionary.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): a single word / a pair of words used as a prefix\n",
    "        map_dict (dict): a mapping dictionary for order-1 or order-2 model\n",
    "        current_syls (int): current number of syllables in a line\n",
    "        target_syls (int): target number of syllables in a line\n",
    "\n",
    "    Returns:\n",
    "        list: a list of candidate words that fit the budget\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def word_after_prefix(prefix: str, map_dict: dict, current_syls: int, target_syls: int) -> list:\n",
    "    \"\"\"Given a prefix (a single word / a pair of words), a mapping dictionary for order-1 model, \n",
    "    the current number of syllables and the target budget, get a list of all candidate words from the dictionary.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): a single word / a pair of words used as a prefix\n",
    "        map_dict (dict): a mapping dictionary for order-1 or order-2 model\n",
    "        current_syls (int): current number of syllables in a line\n",
    "        target_syls (int): target number of syllables in a line\n",
    "\n",
    "    Returns:\n",
    "        list: a list of candidate words that fit the budget\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654a484",
   "metadata": {},
   "source": [
    "## Haiku Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d918114",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-idea\">\n",
    "<h4> 🎯 Strategy: line by line generation using Markov models</h4>\n",
    "\n",
    "Generate haiku (5-7-5 syllable pattern) using order-1 and order-2 Markov models, processing each line independently with inter-line connections.\n",
    "\n",
    "<div class=\"idea-steps\">\n",
    "<h5>Line generation process:</h5>\n",
    "<ul>\n",
    "<li>Line 1: Start with random seed word (≤4 syllables)<\\li>\n",
    "<li>Lines 2-3: Start using last pair from previous line as Markov key<\\li>\n",
    "<li>Word Selection Order:\n",
    "    <ul>\n",
    "    <li>Word 1: Random seed</li>\n",
    "    <li>Word 2: Order-1 model (bigram)</li>\n",
    "    <li>Word 3+: Order-2 model (trigram)</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Syllable Control: Only accept words that fit remaining syllable budget</li>\n",
    "<li>Fallback: If no valid words from current context, try random prefix from dictionary</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "So our algorithm can be summarised as follows:\n",
    "\n",
    "```\n",
    "For each line (target: 5/7/5 syllables):\n",
    "  1. Initialize line with starting word(s)\n",
    "  2. While syllable budget not reached:\n",
    "     a. Try appropriate model (order-1 or order-2)\n",
    "     b. Get candidate words from dictionary\n",
    "     c. Check each candidate's syllables\n",
    "     d. If fits and completes budget → finish line\n",
    "     e. If fits but incomplete → add word, continue\n",
    "     f. If no candidates fit → fallback to \"ghost prefix\"\n",
    "  3. Use last 2 words as bridge to next line\n",
    "```\n",
    "\n",
    "<center>\n",
    "<img src=\"img/haiku_flowchart_svg.svg\" alt=\"Flowchart of haiku generation\" width=\"500\" height=\"800\">\n",
    "</center>\n",
    "\n",
    "> What is a \"ghost prefix\"? \n",
    "\n",
    "A **ghost prefix** is a fallback mechanism used when the current Markov chain context cannot generate any words that fit the remaining syllable budget. The prefix is called \"ghost\" because it's not actually part of the current line being generated - it's a temporary, borrowed context used only to find suitable words. \n",
    "\n",
    "The mechanism works as follows:\n",
    "- Keep the current partial line unchanged\n",
    "- Randomly select a different word/word-pair from the Markov dictionaries\n",
    "- Use this \"ghost\" context to find candidate words\n",
    "- If a candidate fits the syllable budget, add it to the current line\n",
    "- The ghost prefix is then discarded - it never appears in the final haiku\n",
    "\n",
    "Let's illustrate the functioning with examples:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/markov_behavior_diagram.svg\" alt=\"Normal vs Ghost prefix behaviour\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490ee01",
   "metadata": {},
   "source": [
    "Let's start by picking a word at random, so that we can get a seed word to start our haiku.\n",
    "\n",
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 7: </h5> Write a function that picks a random word from the corpus list and the corresponding syllable count. If the word contains more than 4 syllables, let's retry in order to avoid one-word line or exceeding the budget. Test the function.\n",
    "\n",
    "\n",
    "```\n",
    "def random_word(corpus_list: list) -> tuple[str, int]:\n",
    "    \"\"\"Selects a random word from the corpus given as a list. If the word contains more than 4 syllables then retry. \n",
    "    Returns this word and its syllable count.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): corpus given as a list\n",
    "\n",
    "    Returns:\n",
    "        str: a word picked randomly\n",
    "        int: the number of syllables in this word\n",
    "    \"\"\"\n",
    "```\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo-random generator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def random_word(corpus_list: list) -> tuple[str, int]:\n",
    "    \"\"\"Selects a random word from the corpus given as a list. If the word contains more than 4 syllables then retry. \n",
    "    Returns this word and its syllable count.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): corpus given as a list\n",
    "\n",
    "    Returns:\n",
    "        str: a word picked randomly\n",
    "        int: the number of syllables in this word\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# test the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc410e9",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 8: </h5> Write a function that implements ghost prefix behaviour: creates a random ghost prefix, applies a Markov model for a given ghost prefix and returns a list of acceptable candidates.\n",
    "\n",
    "\n",
    "```\n",
    "def use_ghost_prefix(corpus_list: list, map_dict: dict, current_syls: int, target_syls: int) -> list:\n",
    "    \"\"\"Creates a ghost prefix and finds valid candidates\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): corpus split into a list of words\n",
    "        map_dict (dict): dictionary mapping prefixes with consecutive words\n",
    "        current_syls (int): current number of syllables \n",
    "        target_syls (int): target number of syllables\n",
    "\n",
    "    Returns:\n",
    "        list: list of valid candidate words, or empty list if none found\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def use_ghost_prefix(corpus_list: list, map_dict: dict, current_syls: int, target_syls: int) -> list:\n",
    "    \"\"\"Creates a ghost prefix and finds valid candidates\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): corpus split into a list of words\n",
    "        map_dict (dict): dictionary mapping prefixes with consecutive words\n",
    "        current_syls (int): current number of syllables \n",
    "        target_syls (int): target number of syllables\n",
    "\n",
    "    Returns:\n",
    "        list: list of valid candidate words, or empty list if none found\n",
    "    \"\"\"\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46a49d",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 9: </h5> Write a function that gets a list of acceptable candidates for a given prefix. Applies ghost prefix procedure if the initial list is empty. \n",
    "    Randomly picks a word from the resulting list and counts its number of syllables.  \n",
    "\n",
    "\n",
    "```\n",
    "def get_next_word(prefix: str, corpus_list: list, map_dict: dict, current_syls: int, target_syls: int) -> tuple[str, int]:\n",
    "    \"\"\"Gets a list of acceptable candidates for a given prefix. Applies ghost prefix procedure if the initial list is empty. \n",
    "    Randomly picks a word from the resulting list and counts its number of syllables.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): _description_\n",
    "        corpus_list (list): _description_\n",
    "        map_dict (dict): _description_\n",
    "        current_syls (int): _description_\n",
    "        target_syls (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: word, randomly picked from the candidate list\n",
    "        int: number of syllables in the resulting word\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e550bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def get_next_word(prefix: str, corpus_list: list, map_dict: dict, current_syls: int, target_syls: int) -> tuple[str, int]:\n",
    "    \"\"\"Gets a list of acceptable candidates for a given prefix. Applies ghost prefix procedure if the initial list is empty. \n",
    "    Randomly picks a word from the resulting list and counts its number of syllables.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): _description_\n",
    "        corpus_list (list): _description_\n",
    "        map_dict (dict): _description_\n",
    "        current_syls (int): _description_\n",
    "        target_syls (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: word, randomly picked from the candidate list\n",
    "        int: number of syllables in the resulting word\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a601614",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 10: </h5> Write a function that generates first two words of the first line (target number of syllables = 5). The first (seed) word is picked randomly from corpus_list. The second word is selected using order-1 model from mao_dict_1 using the seed word as prefix. \n",
    "\n",
    "\n",
    "```\n",
    "def get_first_two_words(corpus_list, map_dict_1, target_syls: int=5) -> tuple[list, int, list]:\n",
    "    \"\"\"Generates the first two words: the first (seed) word is picked randomly from corpus_list. \n",
    "    The second word is selected using order-1 model from mao_dict_1 using the seed word as prefix. \n",
    "\n",
    "    Args:\n",
    "        map_dict_1 (_type_): dictionary mapping a single word prefix with the next words\n",
    "        target_syls (int, optional): target number of syllables. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: current line containing 2 words\n",
    "        int: number of syllables in the current line\n",
    "        list: two last words of the line. Empty if the target number of syllables hasn't been reached\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def get_first_two_words(corpus_list, map_dict_1, target_syls: int=5) -> tuple[list, int, list]:\n",
    "    \"\"\"Generates the first two words: the first (seed) word is picked randomly from corpus_list. \n",
    "    The second word is selected using order-1 model from mao_dict_1 using the seed word as prefix. \n",
    "\n",
    "    Args:\n",
    "        map_dict_1 (_type_): dictionary mapping a single word prefix with the next words\n",
    "        target_syls (int, optional): target number of syllables. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: current line containing 2 words\n",
    "        int: number of syllables in the current line\n",
    "        list: two last words of the line. Empty if the target number of syllables hasn't been reached\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07697b",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 11: </h5> Write a function that continues the current line till reaching the limit. \n",
    "\n",
    "\n",
    "```\n",
    "def end_current_line(current_line: list, corpus_list, map_dict, current_syls, target_syls) -> list:\n",
    "    \"\"\"Continues the current line till reaching the limit.\n",
    "\n",
    "    Args:\n",
    "        current_line (list): initial state of the current line\n",
    "        corpus_list (_type_): corpus split into a list of words\n",
    "        map_dict (_type_): dictionary mapping a prefix with the next words\n",
    "        current_syls (_type_): initial state of the current number of syllables\n",
    "        target_syls (_type_): target number of syllables\n",
    "\n",
    "    Returns:\n",
    "        list: finalised version of the current line\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def end_current_line(current_line: list, corpus_list, map_dict, current_syls, target_syls) -> list:\n",
    "    \"\"\"Continues the current line till reaching the limit.\n",
    "\n",
    "    Args:\n",
    "        current_line (list): initial state of the current line\n",
    "        corpus_list (_type_): corpus split into a list of words\n",
    "        map_dict (_type_): dictionary mapping a prefix with the next words\n",
    "        current_syls (_type_): initial state of the current number of syllables\n",
    "        target_syls (_type_): target number of syllables\n",
    "\n",
    "    Returns:\n",
    "        list: finalised version of the current line\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b003e71",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 12: </h5> Write a function that creates a haiku line. To ensure a semantic continuity between the lines, the last two words from the previous line are kept and used for next word generation. \n",
    "\n",
    "<i>Hint:</i> Use the functions `get_first_two_words` and `end_current_line()` for simplicity.\n",
    "\n",
    "<i>Hint:</i> You can temporarily add the last word pair to the current line to ensure its use as a prefix. \n",
    "\n",
    "```\n",
    "def get_haiku_line(corpus_list: list, map_dict_1: dict, map_dict_2: dict, end_prev_line: list, target_syls: int) -> tuple[list, list]:\n",
    "    \"\"\"Creates a haiku line. The first line starts with a random seed word followed by a word obtained using order-1 Markov model. \n",
    "    For the rest, order-2 Markov model is used.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): corpus split into a list of words\n",
    "        map_dict_1 (dict): dictionary mapping a single word prefix to the next words\n",
    "        map_dict_2 (dict): dictionary mapping a word-pair prefix to the next words\n",
    "        end_prev_line (list): last pair of words from the previous line to ensure the continuity\n",
    "        target_syls (int): target number of syllables. For haiku: 5, 7, 5.\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def get_haiku_line(corpus_list: list, map_dict_1: dict, map_dict_2: dict, end_prev_line: list, target_syls: int) -> tuple[list, list]:\n",
    "    \"\"\"Creates a haiku line. The first line starts with a random seed word followed by a word obtained using order-1 Markov model. \n",
    "    For the rest, order-2 Markov model is used.\n",
    "\n",
    "    Args:\n",
    "        corpus_list (list): corpus split into a list of words\n",
    "        map_dict_1 (dict): dictionary mapping a single word prefix to the next words\n",
    "        map_dict_2 (dict): dictionary mapping a word-pair prefix to the next words\n",
    "        end_prev_line (list): last pair of words from the previous line to ensure the continuity\n",
    "        target_syls (int): target number of syllables. For haiku: 5, 7, 5.\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a55ad5",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION 13: </h5> Test your program by generating several haiku.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70108814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c2f1a83",
   "metadata": {},
   "source": [
    "**CONGRATULATIONS!!!** You have generated your first haiku."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bacf6b",
   "metadata": {},
   "source": [
    "Evaluating the quality of haiku is not a trivial task. Here are some ideas:\n",
    "\n",
    "- Generate a large number of haiku\n",
    "- \"Plagiarism\" detection: detect duplicates (exact and near) from the training set (e.g. using N-gram level detection)\n",
    "-  Rubbish haiku detection (\"rubbish haiku\" = clearly a random sequence of words): repetition penalties, function word overload (e.g. \"the\", \"and\", \"or\"), lack of content words (e.g. using POS tags likes \"NN\", \"VB\", \"JJ\", \"RB\")\n",
    "- Automatically evaluating \"good\" haikus is the most challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71df916",
   "metadata": {},
   "source": [
    "## Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6da891",
   "metadata": {},
   "source": [
    "1. [Introducing Markov Chains by HarvardX](https://www.youtube.com/watch?v=JHwyHIz6a8A)\n",
    "2. [Markov Chain Stationary Distribution : Data Science Concepts](https://www.youtube.com/watch?v=4sXiCxZDrTU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f67b06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probability-statistics-ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
