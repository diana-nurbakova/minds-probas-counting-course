{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ad9539",
   "metadata": {},
   "source": [
    "# Convergence and Limit Theorems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c196f4",
   "metadata": {},
   "source": [
    "Author & Instructor: Diana NURBAKOVA, PhD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/styles.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a59d4e",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8386b6",
   "metadata": {},
   "source": [
    "By the end of this lesson, you will be able to:\n",
    "- Distinguish between different types of convergence (in distribution, in probability, almost surely) and explain when each type matters in practice\n",
    "- Explain intuitively why the Law of Large Numbers guarantees that sample means converge to population means, and identify when this guarantee \n",
    "- Understand why the Central Limit Theorem is remarkable (universality of normality) and explain its fundamental role in statistical inference\n",
    "- Calculate required sample sizes for desired precision using CLT-based formulas: $n = (z_{\\alpha/2}\\cdot\\sigma/\\varepsilon)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "#sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e58cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the \"resources\" directory to the path\n",
    "project_root = Path().resolve().parent\n",
    "resources_path = project_root / 'resources'\n",
    "sys.path.insert(0, str(resources_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from limit_theorems import (show_transformation, demo_clt_cauchy, example_no_convergence, example_convergence_in_distribution_only, \n",
    "                            example_almost_sure_convergence, example_convergence_in_probability,\n",
    "                            casino_simulation_intro, bridge_to_formal_definition, polling_simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e80c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4>üéØ Today's Challenge: The Monte Carlo Reliability Crisis</h4>\n",
    "<p><strong>Scenario:</strong> You're building a fraud detection AI system. Your model outputs a probability score, but to get the final prediction, you need to run Monte Carlo sampling to account for model uncertainty.</p>\n",
    "\n",
    "<p><strong>Your manager asks:</strong> \"How many Monte Carlo samples do we need? Each sample costs us 10ms of compute time.\"</p>\n",
    "\n",
    "<p><strong>You try different sample sizes:</strong></p>\n",
    "<ul>\n",
    "<li><strong>10 samples (100ms):</strong> Estimate = 0.73</li>\n",
    "<li>Run again: 0.61</li>\n",
    "<li>Run again: 0.84</li>\n",
    "<li>Run again: 0.55</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>1,000 samples (10 seconds):</strong> Estimate = 0.698, then 0.702, then 0.695</p>\n",
    "\n",
    "<p><strong>100,000 samples (16 minutes):</strong> Estimate = 0.7001, then 0.7003, then 0.6999</p>\n",
    "\n",
    "<p><strong>The Million Dollar Questions:</strong></p>\n",
    "<ol>\n",
    "<li>Will we EVER get <em>exactly</em> 0.7000? Or are we chasing an impossible dream?</li>\n",
    "<li>How fast does the \"jumpiness\" decrease? Is there a mathematical pattern?</li>\n",
    "<li>Can we <em>guarantee</em> we're within 0.01 of the true value? With how many samples?</li>\n",
    "<li>Your manager needs an answer in under 1 second (max 100 samples). What do you tell them?</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Make your predictions now.</strong> By the end of today, you'll be able to answer all of these with mathematical precision.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_estimating_prob_hook(true_prob=0.7):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print(\"Live Simulation: Estimating probability = 0.7\\n\")\n",
    "\n",
    "    for n_samples in [10, 50, 100, 500, 1000, 5000]:\n",
    "        estimates = []\n",
    "        # run 5 trials\n",
    "        for trial in range(5):\n",
    "            estimate = np.random.binomial(n_samples, true_prob) / n_samples\n",
    "            estimates.append(estimate)\n",
    "        \n",
    "        print(f\"\\nWith {n_samples:5d} samples:\")\n",
    "        print(f\"  Estimates: {[f'{e:.3f}' for e in estimates]}\")\n",
    "        print(f\"  Range: {max(estimates) - min(estimates):.3f}\")\n",
    "        print(f\"  Std Dev: {np.std(estimates):.4f}\")\n",
    "\n",
    "demo_estimating_prob_hook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9527502",
   "metadata": {},
   "source": [
    "Note that if we consider the range (i.e. the difference between the max and the min elements), we can notice that the \"*jumpiness*\" is decreasing with the increase of the number of samples. The question that remains is *how fast*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760444a",
   "metadata": {},
   "source": [
    "## Types of Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cd711",
   "metadata": {},
   "source": [
    "In daily life, we say things \"*converge*\" loosely. In probability theory, we need precision.\n",
    "\n",
    "Look at these three sequences approaching 0:\n",
    "\n",
    "- Sequence A: 1, 0.5, 0.25, 0.125, ... (definitely reaching 0)\n",
    "- Sequence B: 1, 0, 1, 0, 1, 0, ... (oscillating, never settles)\n",
    "- Sequence C: 1, 0.5, 0.25, 100, 0.001, 0.0005, ... (mostly approaching, rare spikes)\n",
    "\n",
    "> Which ones \"converge to 0\"? It depends on what kind of convergence we mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three sequences\n",
    "n_terms = 50\n",
    "\n",
    "# Sequence A: 1, 0.5, 0.25, 0.125, ... (geometric: 1/2^n)\n",
    "# Definitely converges to 0\n",
    "sequence_A = np.array([1 / (2**n) for n in range(n_terms)])\n",
    "\n",
    "# Sequence B: 1, 0, 1, 0, 1, 0, ... (oscillating)\n",
    "# Never settles\n",
    "sequence_B = np.array([1 if n % 2 == 0 else 0 for n in range(n_terms)])\n",
    "\n",
    "# Sequence C: Mostly approaching 0, but with rare spikes\n",
    "# Create base sequence that decreases\n",
    "sequence_C = np.array([1 / (2**n) for n in range(n_terms)])\n",
    "# Add rare spikes at specific positions\n",
    "spike_positions = [15, 32, 47]  # Positions where spikes occur\n",
    "for pos in spike_positions:\n",
    "    if pos < n_terms:\n",
    "        sequence_C[pos] = 100 / (pos + 1)  # Spike that decreases with position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd92b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot Sequence A\n",
    "ax1 = axes[0]\n",
    "ax1.plot(range(n_terms), sequence_A, 'bo-', linewidth=2, markersize=6, alpha=0.7)\n",
    "ax1.axhline(0, color='red', linestyle='--', linewidth=2, label='Limit = 0')\n",
    "ax1.fill_between(range(n_terms), 0, sequence_A, alpha=0.3, color='blue')\n",
    "ax1.set_xlabel('n', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(r'$a_n$', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Sequence A: 1, 1/2, 1/4, 1/8, ...\\nDefinitely Converges to 0', \n",
    "              fontsize=13, fontweight='bold', color='green')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([-0.1, 1.2])\n",
    "\n",
    "# Add annotation\n",
    "ax1.annotate('Smooth decrease\\nto zero',\n",
    "            xy=(25, sequence_A[25]), xytext=(35, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=10, color='green', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "# Plot Sequence B\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(n_terms), sequence_B, 'ro-', linewidth=2, markersize=6, alpha=0.7)\n",
    "ax2.axhline(0.5, color='orange', linestyle='--', linewidth=2, \n",
    "            label='Average = 0.5', alpha=0.7)\n",
    "ax2.set_xlabel('n', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(r'$b_n$', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Sequence B: 1, 0, 1, 0, 1, 0, ...\\n Oscillates Forever (No Limit)', \n",
    "              fontsize=13, fontweight='bold', color='red')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([-0.2, 1.3])\n",
    "\n",
    "# Add annotation\n",
    "ax2.annotate('Keeps jumping\\nbetween 0 and 1',\n",
    "            xy=(30, 1), xytext=(35, 1.15),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, color='red', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='#ffebee', alpha=0.8))\n",
    "\n",
    "# Plot Sequence C\n",
    "ax3 = axes[2]\n",
    "ax3.plot(range(n_terms), sequence_C, 'go-', linewidth=2, markersize=6, alpha=0.7)\n",
    "ax3.axhline(0, color='red', linestyle='--', linewidth=2, label='Limit = 0?')\n",
    "\n",
    "# Highlight the spikes\n",
    "for pos in spike_positions:\n",
    "    if pos < n_terms:\n",
    "        ax3.plot(pos, sequence_C[pos], 'r*', markersize=20, label='Spike!' if pos == spike_positions[0] else '')\n",
    "\n",
    "ax3.set_xlabel('n', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel(r'$c_n$', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Sequence C: Mostly Decreasing, Rare Spikes\\nMostly Approaching 0, But...', \n",
    "              fontsize=13, fontweight='bold', color='orange')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([-0.5, 8])\n",
    "\n",
    "# Add annotations for spikes\n",
    "ax3.annotate('Unexpected\\nspike!',\n",
    "            xy=(spike_positions[0], sequence_C[spike_positions[0]]), \n",
    "            xytext=(spike_positions[0] - 8, 5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, color='red', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='#fff8e1', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Three Types of Sequence Behavior: Which Ones \"Converge\"?', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90dff8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Convergence in Distribution (or in Law)</h4>\n",
    "\n",
    "A sequence of random variables $X_1, X_2, ...$ with cumulative distribution functions $F_1, F_2, ...$ **converges in distribution** (or **in law**) to a random variable $X$ with cumulative distribution function $F$ if:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} F_n(x) = F(x)$$\n",
    "\n",
    "for all $x$ where $F$ is continuous.\n",
    "\n",
    "We denote this as:\n",
    "$$X_n \\xrightarrow[n\\to \\infty]{\\mathcal{d}} X$$\n",
    "\n",
    "*Intuition*: The cumulative distribution functions of $X_n$ converge point-wise to the cumulative distribution function of $X$ at all continuity points of $F$. \n",
    "\"$X_n$ **behaves like** $X$ for large $n$\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935f1ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Convergence in Probability</h4>\n",
    "\n",
    "A sequence of random variables $X_1, X_2, ...$ **converges in probability** to a random variable $X$ if for any $\\epsilon > 0$:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\mathbb{P}(|X_n - X| > \\epsilon) = 0$$\n",
    "\n",
    "We denote this as:\n",
    "$$X_n \\xrightarrow[n\\to \\infty]{\\mathbb{P}} X$$\n",
    "\n",
    "*Intuition:* As $n$ increases, the probability that $X_n$ is far from $X$ (by more than $\\epsilon$) tends to zero. In other words, $X_n$ gets arbitrarily close to $X$ with increasingly high probability. \"$X_n$ is **probably** close to $X$ for large $n$\"\n",
    "\n",
    "*Remark*: Convergence in distribution is weaker than convergence in probability. If $X_n \\xrightarrow[n\\to \\infty]{\\mathbb{P}} X$, then $X_n \\xrightarrow[n\\to \\infty]{\\mathcal{d}} X$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42fa77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Almost Sure Convergence </h4>\n",
    "\n",
    "Let $X_1, X_2, ...$ be a sequence of i.i.d. random variables with finite expectation $\\mathbb{E}(X_i) = \\mu$.\n",
    "\n",
    "Let $\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$ be the sample mean. \n",
    "\n",
    "The sequence converges **almost surely** to $X$ if:\n",
    "\n",
    "$$\\mathbb{P}\\left(\\lim_{n \\to \\infty} \\overline{X}_n = \\mu\\right) = 1$$\n",
    "\n",
    "This is also written as:\n",
    "$$\\overline{X}_n \\xrightarrow[n\\to \\infty]{a.s.} \\mu$$\n",
    "\n",
    "where \"a.s.\" stands for \"almost surely\" (convergence with probability 1).\n",
    "\n",
    "*Intuition:* The actual sequence converges (except on a set of probability 0). \"If you run the experiment, $X_n$ **will** converge to $X$ (almost certainly)\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50dfc9",
   "metadata": {},
   "source": [
    "When we say \"*all paths converge*\", we're talking about individual realizations of the random sequence. A \"*path*\" is one complete run of our random experiment from start to finish. \n",
    "\n",
    "Suppose you're estimating the probability of heads by flipping a coin repeatedly:\n",
    "\n",
    "1. Path 1: You flip and get: H, T, H, H, T, T, H, H, H, T, ...\n",
    "- Running averages: 1.00, 0.50, 0.67, 0.75, 0.60, 0.50, 0.57, 0.625, 0.67, 0.60, ...\n",
    "\n",
    "2. Path 2: Your friend flips (different random outcomes): T, T, H, T, H, H, T, H, T, H, ...\n",
    "- Running averages: 0.00, 0.00, 0.33, 0.25, 0.40, 0.50, 0.43, 0.50, 0.44, 0.50, ...\n",
    "\n",
    "3. Path 3: Another person flips: H, H, H, T, T, H, T, H, H, H, ...\n",
    "- Running averages: 1.00, 1.00, 1.00, 0.75, 0.60, 0.67, 0.57, 0.625, 0.67, 0.70, ...\n",
    "\n",
    "Each sequence of flips is a *path* or *sample path* or *realization* or *trajectory*.\n",
    "\n",
    "If we compare convergence in probability and almost sure convergence:\n",
    "\n",
    "1. Convergence in probability says: \"For large $n$, MOST paths will be close to 0.5\"\n",
    "- Some paths might wander away temporarily\n",
    "- We only care that the probability of being far is small\n",
    "\n",
    "2. Almost sure convergence says: \"If you actually run the experiment, YOUR specific path WILL converge to 0.5\"\n",
    "- Not just \"probably\" - it actually happens\n",
    "- Only fails on a set of measure zero (impossible events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63497d0",
   "metadata": {},
   "source": [
    "> If bad events keep happening, but become rarer and rarer, will they eventually stop happening?\n",
    "</br>\n",
    "\n",
    "Consider an analogy of the casino. We are tracking \"bad days\":\n",
    "- Day 1: Probability of losing = 0.5\n",
    "- Day 10: Probability of losing = 0.1\n",
    "- Day 100: Probability of losing = 0.01\n",
    "- Day 1000: Probability of losing = 0.001\n",
    "\n",
    "*Question*: Will you eventually stop having losing days?\n",
    "\n",
    "Borel-Cantelli says: It depends on whether the sum of probabilities converges:\n",
    "\n",
    "- If $0.5 + 0.1 + 0.01 + 0.001 + ...$ converges (finite sum) ‚Üí You'll only have finitely many losing days\n",
    "- If the sum diverges (infinite) ‚Üí You'll have infinitely many losing days\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Borel-Cantelli Lemma</h4>\n",
    "\n",
    "Let $A_1, A_2, A_3, ...$ be a sequence of events.\n",
    "\n",
    "If $\\sum P(A_n) < \\infty$ (the sum of probabilities converges), then $P(A_n\\text{ occurs infinitely often}) = 0$\n",
    "\n",
    "In other words: With probability 1, only finitely many of the events $A_n$ will occur.\n",
    "\n",
    "Notation: $P(A_n \\text{ i.o.}) = 0$ where \"*i.o.*\" means \"*infinitely often*\"\n",
    "\n",
    "Formally: $A_n \\text{ i.o.} = \\cap_{m=1}^{\\infty} \\cup_{n=m}^{\\infty}A_n$ (Events that occur for infinitely many $n$)\n",
    "</div>\n",
    "\n",
    "**Connection to Almost Sure Convergence**\n",
    "\n",
    "How We Use It:\n",
    "\n",
    "To prove $X_n \\rightarrow X$ almost surely, we define \"bad events\":\n",
    "$A_n = {|X_n - X| > \\varepsilon}$  (the event that we're more than Œµ away from the limit)\n",
    "\n",
    "Goal: Show that $P(A_n \\text{ i.o.}) = 0$\n",
    "\n",
    "That is, we want to show: \"With probability 1, we're only far from $X$ finitely many times, and then we stay close forever.\"\n",
    "\n",
    "**Borel-Cantelli Strategy:**\n",
    "\n",
    "1. Show that $Œ£ P(|X_n - X| > \\varepsilon) < \\infty$\n",
    "2. Conclude that $P(|X_n - X| > \\varepsilon \\text{ infinitely often}) = 0$\n",
    "3. Therefore, eventually $|X_n - X| \\leq \\varepsilon$ for all sufficiently large $n$ (with probability 1)\n",
    "4. This holds for every $\\varepsilon > 0$, so $X_n \\rightarrow X$ almost surely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cb04e",
   "metadata": {},
   "source": [
    "<a id=\"borel-cantelli-ex\"></a>\n",
    "<div class=\"alert alert-example\">\n",
    "<h4>Calculated Example: Almost Sure Convergence and Borel-Cantelli Lemma</h4>\n",
    "\n",
    "Let $Z_1, Z_2, Z_3, ...$ be i.i.d. with $E[Z_i] = 0$ and $Var(Z_i) = \\sigma^2 = 1 < \\infty$.\n",
    "\n",
    "Let $X_n = \\sum_{i=1}^n Z_i/n^2$.\n",
    "\n",
    "1. Step 1: Define bad event\n",
    "\n",
    "$$A_n = {|X_n| > \\varepsilon}$$\n",
    "\n",
    "2. Step 2: Bound probability using Chebyshev's inequality\n",
    "\n",
    "$$P(|X_n| > \\varepsilon) \\leq \\frac{Var(X_n)}{\\varepsilon^2}$$\n",
    "\n",
    "Let's find the variance of $X_n$:\n",
    "$$Var(X_n) = Var\\bigg(\\sum_{i=1}^n Z_i/n^2\\bigg) = 1/n^4 Var\\bigg(\\sum_{i=1}^n Z_i\\bigg) = [\\text{by indep.}] = n/n^4\\times Var(Z_1) = 1/n^3$$\n",
    "So $P(|X_n| > \\varepsilon) \\leq \\frac{1}{n^3\\varepsilon^2}$\n",
    "\n",
    "3. Step 3: Check if sum converges\n",
    "\n",
    "$$\\sum_{n=1}^\\infty P(|X_n| > \\varepsilon) \\leq \\sum_{n=1}^\\infty \\frac{1}{n^3\\varepsilon^2} = (1/\\varepsilon^2)\\times \\sum_{n=1}^\\infty \\frac{1}{n^3}$$\n",
    "\n",
    "The series $\\sum_{n=1}^\\infty \\frac{1}{n^3}$ [converges to a finite value](https://en.wikipedia.org/wiki/Convergence_tests#Examples) ($p$-series with $p=3 > 1$; intuitive sense: when $p$ is large, $1/n^p$ becomes tiny very quickly, so we're essentially just adding $1 + \\text{(tiny terms)}$), so the sum is finite.\n",
    "\n",
    "4. Step 4: Apply Borel-Cantelli\n",
    "\n",
    "Since $\\sum_{n=1}^\\infty P(A_n) < \\infty$, we have $P(A_n \\text{ i.o.}) = 0$\n",
    "\n",
    "Therefore $P(X_n \\rightarrow 0) = 1$, i.e. $X_n$ converges almost surely to 0.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d1064",
   "metadata": {},
   "source": [
    "### Example Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f0bef",
   "metadata": {},
   "source": [
    "We'll explore four scenarios:\n",
    "\n",
    "1. No Convergence: Sequence stays random forever\n",
    "2. Convergence in Distribution Only: CDFs converge, but values don't\n",
    "3. Convergence in Probability + Distribution: Values probably get close\n",
    "4. Almost Sure + Probability + Distribution: Values actually converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d2b5d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Scenario 1: No Convergence - The Oscillating Sequence</h4>\n",
    "\n",
    "Consider  \n",
    "$$X_n = (-1)^n \\cdot n$$\n",
    "\n",
    "\n",
    "*Behavior*: $X_1 = -1, X_2 = 2, X_3 = -3, X_4 = 4, X_5 = -5, ...$\n",
    "\n",
    "*Mathematical Analysis:*\n",
    "\n",
    "- Does NOT converge to any value: oscillates and grows\n",
    "- Not convergent in distribution:> CDFs don't stabilize\n",
    "- Not convergent in probability: $P(|X_n - X| < \\varepsilon)$ doesn't go to 1 for any $X$\n",
    "- Not almost surely convergent: the sequence diverges\n",
    "\n",
    "\n",
    "*Why it fails:* No matter what value you pick as a \"limit,\" the sequence keeps moving away from it.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe80b1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal mathematical proof</summary>\n",
    "\n",
    "Our claim: $X_n$ does not converge to any $X$\n",
    "\n",
    "Let's apply the strategy of proof by contradiction.\n",
    "\n",
    "Suppose $X_n \\rightarrow X$ for some $X \\in \\mathbb{R}$. \n",
    "\n",
    "Then for $\\varepsilon = 1$:\n",
    "    $P(|X_n - X| < 1)\\text{ should} \\rightarrow 1$\n",
    "    \n",
    "But:\n",
    "- For even $n$: $X_n = n \\rightarrow +\\infty$\n",
    "- For odd $n$:  $X_n = -n \\rightarrow -\\infty$\n",
    "    \n",
    "So $|X_n - X| \\rightarrow \\infty$ for ANY $X$.\n",
    "    \n",
    "Therefore: NO convergence (not in distribution, not in probability, not almost surely)\n",
    "\n",
    "**Numerical evidence:**\n",
    "\n",
    "|$X_i$|Distance from 0|\n",
    "|:--:|:--:|\n",
    "|$X_5 = -5$ |  5  |\n",
    "|$X_{10} = 10$ |  10 |\n",
    "|$X_{20} = 20$ |  20 |\n",
    "|$X_{50} = 50$ |  50 |\n",
    "\n",
    "*Key Observations*\n",
    "1. Sign alternates: Values flip between positive and negative\n",
    "2. Magnitude grows: The absolute values keep increasing\n",
    "3. No stabilization: Distance from any proposed limit increases without bound\n",
    "4. Pattern persists: No matter how large $n$ gets, the oscillation and growth continue\n",
    "\n",
    "*Interpretation*:\n",
    "\n",
    "This sequence does NOT converge. It violates the most basic requirement: the values don't approach any fixed number. This is a sign of:\n",
    "- Unstable process\n",
    "- In ML context: divergent training, exploding gradients\n",
    "- Action needed: fundamental fix required (reduce learning rate, add clipping, check for bugs)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric verification\n",
    "for n in [5, 10, 20, 50]:\n",
    "    val = ((-1) ** n) * n\n",
    "    print(f\"X_{n:2d} = {val:6.0f}   |  Distance from 0: {abs(val):6.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c706d58",
   "metadata": {},
   "source": [
    "\n",
    "*Practical interpretation:*\n",
    "- This is like a training process that's unstable\n",
    "- If $X_n$ is a metric, it oscillates wildly and never settles\n",
    "- In ML: divergent training, need to fix hyperparameters (or check for bugs)\n",
    "- In statistics: an estimator that doesn't converge is useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "example_no_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9d01e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Scenario 2: Convergence in Distribution ONLY</h4>\n",
    "\n",
    "Consider:\n",
    "$$X_n = Z \\cdot (-1)^n\\text{, where } Z \\sim N(0,1)$$\n",
    "\n",
    "*Behavior*: ${X_n}$ flips sign each time, but maintains same distribution\n",
    "\n",
    "*Mathematical Analysis:*\n",
    "- ‚úì Converges in distribution: $X_n \\sim N(0,1)$ for all $n$ (same CDF!)\n",
    "- ‚úó Does NOT converge in probability: $P(|X_n - X_{n+1}| < \\varepsilon) = P(|2Z| < \\varepsilon) \\neq 1$\n",
    "- ‚úó Does NOT converge almost surely: sequence oscillates forever\n",
    "</ul>\n",
    "\n",
    "*Key Insight*: The *distribution* stays the same, but individual *values* keep jumping around.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e69143",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal mathematical proof</summary>\n",
    "\n",
    "1. First, let's check convergence in distribution\n",
    "    \n",
    "For all $n$: $X_n = Z \\cdot (-1)^n$ where $Z \\sim N(0,1)$\n",
    "    \n",
    "Since $(-1)^n \\in {-1, 1}$ and $Z \\sim N(0,1)$:\n",
    "- Both $Z$ and $-Z$ have same distribution\n",
    "- Therefore $X_n \\sim N(0,1)$ for all $n$\n",
    "    \n",
    "In this case, $X_n$ has CDF of standard normal distribution: \n",
    "\n",
    "$F_n(x) = P(X_n \\leq x) = \\Phi(x)$\n",
    "    \n",
    "    \n",
    "Hence: $\\lim\\limits_{n\\to +\\infty}F_n(x) = \\Phi(x)$ for all $n$\n",
    "\n",
    "So: $X_n  \\xrightarrow[n\\to+\\infty]{d} X \\sim N(0,1)$\n",
    "\n",
    "$X_n$ converges in distribution to $X\\sim N(0,1)$\n",
    "    \n",
    "2. Second, let's check convergence in probability\n",
    "    \n",
    "$$X_{n+1} - X_n = Z¬∑(-1)^{n+1} - Z¬∑(-1)^n = -2Z¬∑(-1)^n$$\n",
    "    \n",
    "$$|X_{n+1} - X_n| = 2|Z|$$\n",
    "    \n",
    "$$P(|X_{n+1} - X_n| < \\varepsilon) = P(|Z| < \\varepsilon/2) \\neq 1\\text{ as } n \\rightarrow \\infty$$\n",
    "    \n",
    "So, we note that the values keep jumping.\n",
    "\n",
    "Therefore, there is no convergence in probability.\n",
    "\n",
    "**Numerical Verification:**\n",
    "\n",
    "Let $Z = 0.496714$\n",
    "\n",
    "|$X_i$ |oscillates between ¬±0.496714|\n",
    "|:--:|:--:|\n",
    "|$X_1 = -0.496714$   | YES | \n",
    "|$X_2 = 0.496714$   | YES |\n",
    "|$X_3 = -0.496714$   | YES |\n",
    "|$X_4 = 0.496714$   | YES |\n",
    "|$X_5 = -0.496714$   | YES |\n",
    "|$X_{10} = 0.496714$   | YES |\n",
    "|$X_{50} = 0.496714$   | YES |\n",
    "|$X_{100} = 0.496714$   | YES |\n",
    "\n",
    "Key Observations:\n",
    "1. Perfect oscillation: Values jump between exactly +Z and -Z\n",
    "2. No trend toward zero: Distance remains constant forever\n",
    "3. Distribution is stable: If you collect many samples at any n, you get N(0,1)\n",
    "4. Individual values don't converge: Each path keeps oscillating indefinitely\n",
    "\n",
    "*Interpretation*:\n",
    "\n",
    "Distribution converges, but values don't. This tells us:\n",
    "- The \"average behavior\" is predictable (distribution is $N(0,1)$)\n",
    "- But individual realizations are unreliable\n",
    "- In ML context: Your model's uncertainty distribution is well-calibrated, but individual predictions are noisy\n",
    "- Solution: Average multiple predictions, or decrease learning rate to get value convergence\n",
    "\n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical verification\n",
    "np.random.seed(42)\n",
    "# Generate sequence - ONE realization\n",
    "Z = np.random.normal(0, 1)  # Draw once\n",
    "print(f\"Z = {Z:.6f}\")\n",
    "for n in [1, 2, 3, 4, 5, 10, 50, 100]:\n",
    "    val = Z * ((-1) ** n)\n",
    "    print(f\"X_{n:3d} = {val:8.6f}   (oscillates between ¬±{abs(Z):.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01078c",
   "metadata": {},
   "source": [
    "Practical interpretation:\n",
    "- ‚úì Distribution convergence: 'On average, behavior is stable'\n",
    "    * Example: Your model's prediction distribution stays constant\n",
    "    * Example: Error bars remain the same size\n",
    "- ‚úó No value convergence: 'Individual predictions still jump around'\n",
    "    * Example: Predictions oscillate even though distribution is stable\n",
    "    * Example: SGD with constant high learning rate - loss distribution stays the same but actual loss values keep jumping\n",
    "- When you see this in ML:\n",
    "    * Model uncertainty quantification: distribution is calibrated \n",
    "    * But individual predictions are unreliable (high variance)\n",
    "    * Need to average multiple predictions for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "example_convergence_in_distribution_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652087d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Scenario 3: Convergence in Probability</h4>\n",
    "\n",
    "Consider:\n",
    "$X_n = Z / n \\text{, where } Z \\sim N(0,1)$\n",
    "\n",
    "*Behavior*: ${X_n} \\rightarrow 0$ as $n \\rightarrow \\infty$\n",
    "\n",
    "*Mathematical Analysis:*\n",
    "- ‚úì Converges in probability: $P(|X_n - 0| > \\varepsilon) = P(|Z| > n\\varepsilon) \\rightarrow 0$\n",
    "- ‚úì Converges in distribution: $X_n \\xrightarrow[n\\to+\\infty]{d} \\delta_0$ (point mass at 0)\n",
    "- ‚úó Does NOT converge almost surely: Individual paths may oscillate\n",
    "\n",
    "</ul>\n",
    "\n",
    "*Key Insight*: For large $n$, $X_n$ is *probably* close to 0, but there's always a small chance it's not (because $Z$ could be large).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73ce86",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal mathematical proof</summary>\n",
    "\n",
    "1. First, let's focus on the convergence in probability\n",
    "    \n",
    "  What we have: $X_n = Z/n$ where $Z \\sim N(0,1)$\n",
    "    \n",
    "  For any $\\varepsilon > 0$:\n",
    "  $$P(|X_n - 0| > \\varepsilon) = P(|Z/n| > \\varepsilon) = P(|Z| > n\\varepsilon) = 2¬∑P(Z > n\\varepsilon) = 2¬∑[1 - \\Phi(n\\varepsilon)] \\rightarrow 0 \\text{ as } n \\rightarrow \\infty$$\n",
    "    \n",
    "  Since $\\Phi(n\\varepsilon) \\rightarrow 1$ as $n \\rightarrow \\infty$, therefore: $X_n \\xrightarrow[n \\rightarrow \\infty]{p} 0$ \n",
    "   \n",
    "2. Second, let's check convergence in distribution\n",
    "    \n",
    "  Proof:\n",
    "  X‚Çô ~ N(0, 1/n¬≤)\n",
    "    \n",
    "  CDF: F‚Çô(x) = Œ¶(x¬∑n)\n",
    "  - For x < 0: F‚Çô(x) = Œ¶(x¬∑n) ‚Üí 0\n",
    "  - For x > 0: F‚Çô(x) = Œ¶(x¬∑n) ‚Üí 1\n",
    "  - For x = 0: F‚Çô(0) = 0.5 for all n\n",
    "    \n",
    "  Limit is point mass at 0: Œ¥‚ÇÄ\n",
    "  Therefore: X‚Çô ‚Üí·µà Œ¥‚ÇÄ ‚úì\n",
    "    \n",
    "  Note: Convergence in probability ‚üπ Convergence in distribution\n",
    "\n",
    "*Numerical Verification*\n",
    "\n",
    "Let $Z = 2.5$ (a particular realization)\n",
    "\n",
    "| $X_i$ | $P(|X_n| > 0.1)$|\n",
    "|:--:|:--:|\n",
    "| $X_1 =  2.50000$   |    0.920344 |\n",
    "|$X_2 =  1.25000$   |   0.841481|\n",
    "|$X_5 =  0.50000$   |   0.617075|\n",
    "|$X_{10} =  0.25000$   |   0.317311|\n",
    "|$X_{20} =  0.12500$   |   0.045500|\n",
    "|$X_{50} =  0.05000$   |   0.000001|\n",
    "|$X_{100} =  0.02500$   |   0.000000|\n",
    "| $X_{200} =  0.01250$   |   0.000000|\n",
    "\n",
    "*Key Observations*\n",
    "1. Monotonic decrease: Values consistently get smaller\n",
    "2. Approaching zero: Clear trend toward 0\n",
    "3. Predictable rate: Each doubling of $n$ halves the value ($1/n$ pattern)\n",
    "4. Not quite zero: Never exactly reaches 0, but gets arbitrarily close\n",
    "5. Depends on $Z$: If $Z$ is large, takes longer to get close to 0\n",
    "\n",
    "*Interpretation*\n",
    "\n",
    "Values probably get close to 0. This means:\n",
    "- For large $n$, $X_n$ is likely very close to 0\n",
    "- But there's always a small chance it's not (if $Z$ happens to be large)\n",
    "- Different random draws of $Z$ give different paths, but all trend toward 0\n",
    "- In ML context: Your algorithm usually converges, good enough for practice\n",
    "- The specific path depends on random initialization ($Z$), but outcome is predictable\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b79a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Verification\n",
    "print(\"Drawing Z = 2.5 (a particular realization):\")\n",
    "Z = 2.5\n",
    "for n in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    val = Z / n\n",
    "    prob_outside = 2 * (1 - stats.norm.cdf(n * 0.1))  # P(|X‚Çô| > 0.1)\n",
    "    print(f\"X_{n:3d} = {val:8.5f}   |  P(|X‚Çô| > 0.1) = {prob_outside:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ac721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Drawing Z = 500 (a particular realization):\")\n",
    "Z = 500\n",
    "for n in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    val = Z / n\n",
    "    prob_outside = 2 * (1 - stats.norm.cdf(n * 0.1))  # P(|X‚Çô| > 0.1)\n",
    "    print(f\"X_{n:3d} = {val:8.5f}   |  P(|X‚Çô| > 0.1) = {prob_outside:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef0790",
   "metadata": {},
   "source": [
    "*Real-world examples:*\n",
    "1. Sample mean with fixed data:\n",
    "$\\bar{X_n} = (X_1 + ... + X_n)/n \\rightarrow \\mu$ (Law of Large Numbers)\n",
    "    \n",
    "2. Regularization in ML:\n",
    "- $Loss_n = MSE + \\lambda/n¬∑||w||^2 \\rightarrow MSE$ as $n\\rightarrow \\infty$ (regularization term $\\lambda/n¬∑||w||^2$ vanishes)\n",
    "    \n",
    "3. Learning rate decay:\n",
    "- $\\theta_{n+1} = \\theta_n - (1/n)\\cdot\\nabla L(\\theta_n)$\n",
    "- Step sizes ‚Üí 0, likely to converge\n",
    "    \n",
    "4. Monte Carlo estimates:vill\n",
    "- Estimate with $n$ samples ‚Üí true value\n",
    "    \n",
    "*What it does NOT guarantee:*\n",
    "- Individual sequences may still have occasional jumps\n",
    "- Not every realization converges (just 'most' do)\n",
    "- Requires infinite samples for perfect convergence\n",
    "\n",
    "*Practical Interpretation:*\n",
    "    \n",
    "Convergence in probability means:\n",
    "- For large $n$, $X_n$ is PROBABLY very close to the limit\n",
    "- The probability of being far away goes to zero\n",
    "- But there's always a tiny chance of deviation (if $Z$ is large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "example_convergence_in_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaed765",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Scenario 4: Almost Sure Convergence (Strongest Type)</h4>\n",
    "\n",
    "Consider:\n",
    "$X_n = \\sum_{i=1}^\\infty Z_i / n^2 \\text{, where } Z_i \\sim N(0,1) \\text{ i.i.d.}$\n",
    "\n",
    "*Behavior*: ${X_n} \\rightarrow 0$ almost surely (with probability 1)\n",
    "\n",
    "*Mathematical Analysis:*\n",
    "- ‚úì Converges almost surely: $P(X_n \\rightarrow 0) = 1$\n",
    "- ‚úì Converges in probability: implied by a.s. convergence\n",
    "- ‚úì Converges in distribution: implied by convergence in probability\n",
    "</ul>\n",
    "\n",
    "*Key Insight*: The actual sequence values converge to 0 (not just probabilistically, but the paths themselves).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ec188",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal mathematical proof</summary>\n",
    "\n",
    "We'd like to prove almost sure convergence.\n",
    "\n",
    "Proof outline (detailed proof has been discussed in the section [Borel-Cantelli Lemma](#borel-cantelli-ex)):\n",
    "    \n",
    "Let $X_n = (Z_1 + Z_2 + ... + Z_n)/n^2$ where $Z_i \\sim N(0,1)$.\n",
    "    \n",
    "The expected value $E[X_n] = E[(Z_1 + Z_2 + ... + Z_n)/n^2] = n/2^2E[Z_i] =  0$\n",
    "\n",
    "Variance:     \n",
    "$$Var(X_n) = Var\\bigg(\\sum_{i=1}^n Z_i/n^2\\bigg) = 1/n^4 Var\\bigg(\\sum_{i=1}^n Z_i\\bigg) = [\\text{by indep.}] = n/n^4\\times Var(Z_1) = 1/n^3$$\n",
    "\n",
    "We can bound the probability of $P(|X_n| > \\varepsilon)$ using Chebyshev's inequality $\\varepsilon > 0$:\n",
    "    $$P(|X_n| > \\varepsilon) ‚â§ Var(X_n)/\\varepsilon^2 = 1/(n^3\\varepsilon^2)$$\n",
    "    \n",
    "Key point: $$\\sum_{n=1}^\\infty P(|X_n| > \\varepsilon) \\leq \\sum_{n=1}^\\infty 1/(n^3\\varepsilon^2) = (1/\\varepsilon^2)\\cdot \\sum_{n=1}^\\infty 1/n^3 < \\infty \\text{ (p-series, p=3 > 1)}$$\n",
    "    \n",
    "By Borel-Cantelli Lemma:\n",
    "    $P(|X_n| > \\varepsilon \\text{ infinitely often}) = 0$\n",
    "    \n",
    "Therefore: $P(X_n \\rightarrow 0) = 1$\n",
    "    \n",
    "This implies:\n",
    "- Convergence in probability\n",
    "- Convergence in distribution\n",
    "- Convergence almost surely\n",
    "\n",
    "**Numerical verification:**\n",
    "\n",
    "| $X_i$ | Realization 1| Realization 2 | Realization 3 | Realization 4 | Realization 5 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| $X_{10}$   | 0.0448061   | -0.0137659 | 0.0487307 | 0.0199982  | -0.0281947 |\n",
    "| $X_{50}$   |  -0.0045095 | -0.0051722 | 0.0065431 | 0.0002167  | -0.0029837 |\n",
    "|  $X_{100}$ | -0.0010385  | -0.0011531 | 0.0016017 | -0.0009915 | -0.0011160 |\n",
    "|  $X_{200}$ | -0.0002039  | -0.0002280 | 0.0006682 | 0.0001646  | -0.0003293 |\n",
    "|  $X_{500}$ |  0.0000137  | 0.0000637  | 0.0002170 | 0.0000664  | -0.0000230 |\n",
    "\n",
    "*Key Observations*: \n",
    "\n",
    "1. All paths converge: Every realization trends toward 0\n",
    "2. Fast convergence: Values become tiny very quickly\n",
    "3. Different paths, same destination: Each realization takes a different route but all reach 0\n",
    "4. Accelerating convergence: The rate of decrease speeds up (due to n¬≤ in denominator)\n",
    "\n",
    "*Interpretation*\n",
    "\n",
    "Actually converges to 0. This is the strongest guarantee:\n",
    "\n",
    "- Not just \"probably\" - the paths themselves converge\n",
    "- Every run will reach the limit (with probability 1)\n",
    "- Convergence is fast and reliable\n",
    "- In ML context: Training will definitely converge, can trust stopping criteria\n",
    "- Different random seeds all lead to the same outcome (convergence to optimum)\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17caaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "    \n",
    "# Numerical verification\n",
    "n_max = 500\n",
    "# Generating 5 independent realizations:\n",
    "for trial in range(5):\n",
    "    Z = np.random.normal(0, 1, n_max)\n",
    "    cumsum_Z = np.cumsum(Z)\n",
    "        \n",
    "    print(f\"\\nRealization {trial + 1}:\")\n",
    "    for n in [10, 50, 100, 200, 500]:\n",
    "        if n <= n_max:\n",
    "            val = cumsum_Z[n-1] / (n ** 2)\n",
    "            print(f\"  X_{n:3d} = {val:10.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bfce7",
   "metadata": {},
   "source": [
    "*Practical Interpretation:*\n",
    "- Almost sure convergence is the STRONGEST type:\n",
    "- The actual sequence WILL converge (with probability 1)\n",
    "- Not just 'probably' close, but paths actually reach the limit\n",
    "- Only excludes a set of probability 0 (pathological cases)\n",
    "\n",
    "*Real-world examples:*\n",
    "1. Stochastic Gradient Descent with decreasing learning rate:\n",
    "    \n",
    "If $\\eta_n = 1/n^2$ and $\\sum\\eta_n^2 < \\infty$, then $\\theta_n \\xrightarrow[n\\to+\\infty]{a.s.} \\theta^*$ ([Robbins-Monro conditions](https://en.wikipedia.org/wiki/Stochastic_approximation))\n",
    "\n",
    "2. Sample mean (Strong Law of Large Numbers):\n",
    "    \n",
    "$\\bar{X_n} = (X_1 + ... + X_n)/n \\xrightarrow[n\\to+\\infty]{a.s.}\\mu$, i.e. your estimate WILL converge to true mean\n",
    "    \n",
    "3. Monte Carlo integration with proper variance control:\n",
    "    \n",
    "$\\int f(x)dx$ estimated by $(1/n)\\sum f(X_i) \\xrightarrow[n\\to+\\infty]{a.s.}\\text{ true integral}$\n",
    "\n",
    "4. Online learning with vanishing step sizes:\n",
    "    \n",
    "Parameter updates converge to optimal value\n",
    "    \n",
    "\n",
    "*Why it matters:*\n",
    "- Strongest theoretical guarantee\n",
    "- Individual runs will converge (not just on average)\n",
    "- Required for many theoretical proofs in ML\n",
    "- Justifies stopping criteria in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0048e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "example_almost_sure_convergence(py=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc52339",
   "metadata": {},
   "source": [
    "**Hierarchy of convergence:**\n",
    "  \n",
    "$$\\text{Almost Sure} \\Rightarrow \\text{In Probability} \\Rightarrow \\text{In Distribution}$$\n",
    "\n",
    "(Each arrow is strict: $\\Leftarrow$ does NOT hold in general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47059793",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4>Comparison: Types of Convergence</h4>\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse; background: white; margin: 10px 0; font-size: 0.95em;\">\n",
    "<thead>\n",
    "<tr style=\"background: #e3f2fd;\">\n",
    "<th style=\"padding: 12px; border: 1px solid #ccc;\">Property</th>\n",
    "<th style=\"padding: 12px; border: 1px solid #ccc;\">Example 1:<br>No Conv.</th>\n",
    "<th style=\"padding: 12px; border: 1px solid #ccc;\">Example 2:<br>Dist. Only</th>\n",
    "<th style=\"padding: 12px; border: 1px solid #ccc;\">Example 3:<br>In Prob.</th>\n",
    "<th style=\"padding: 12px; border: 1px solid #ccc;\">Example 4:<br>Almost Sure</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Sequence</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">\n",
    "\n",
    "$(-1)^n¬∑n$</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">\n",
    "\n",
    "$Z¬∑(-1)‚Åø$</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">\n",
    "\n",
    "$Z/n$</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">\n",
    "\n",
    "$\\sum_i Z_i/n^2$</td>\n",
    "</tr>\n",
    "<tr style=\"background: #f5f5f5;\">\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Limit (if exists)</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">None</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">N(0,1) (dist)</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">0</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>In Distribution?</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #c62828;\">‚ùå No</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #2e7d32;\">‚úì Yes</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #2e7d32;\">‚úì Yes</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #2e7d32;\">‚úì Yes</td>\n",
    "</tr>\n",
    "<tr style=\"background: #f5f5f5;\">\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>In Probability?</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #c62828;\">‚ùå No</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #c62828;\">‚ùå No</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #2e7d32;\">‚úì Yes</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #2e7d32;\">‚úì Yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Almost Surely?</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #c62828;\">‚ùå No</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #c62828;\">‚ùå No</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #c62828;\">‚ùå No</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc; color: #2e7d32;\">‚úì Yes</td>\n",
    "</tr>\n",
    "<tr style=\"background: #f5f5f5;\">\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Path Behavior</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Diverges</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Oscillates forever</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Usually converges</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Actually converges</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>ML Example</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Divergent training</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">SGD w/ const. high LR</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">LR decay: 1/n</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">LR decay: 1/n¬≤</td>\n",
    "</tr>\n",
    "<tr style=\"background: #f5f5f5;\">\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Practical Meaning</strong></td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Unstable, useless</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Stable dist., noisy values</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Probably close</td>\n",
    "<td style=\"padding: 10px; border: 1px solid #ccc;\">Actually converges</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p style=\"background: #fff8e1; padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "<strong>Key Insight:</strong> The hierarchy is strict:<br>\n",
    "<strong>Almost Sure ‚üπ In Probability ‚üπ In Distribution</strong><br>\n",
    "But the reverse implications do NOT hold!\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cbb71d",
   "metadata": {},
   "source": [
    "### Practical Implications: What Does Each Type Really Mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdbfb0",
   "metadata": {},
   "source": [
    "| | No Convergence | Convergence in Distribution | Convergence in Probability | Almost Sure Convergence |\n",
    "|---|----|---|----|----|\n",
    "| **What it means** |<ul><li>Your process is unstable or diverging</li><li>No meaningful limit exists</li><li>Cannot make reliable predictions</li></ul> |<ul> <li>The <em>distribution</em> of outcomes is stable</li>    <li>But individual values keep jumping around</li> <li>\"On average\" behavior is predictable</li> <li>Individual predictions are still noisy </li></ul>| <ul><li>For large n, values are <em>probably</em> close to the limit</li><li>Probability of being far away ‚Üí 0</li><li>Most runs converge, but occasional deviations possible</li>    <li>Good enough for most practical purposes</li></ul>| <ul><li>The actual sequence <em>will</em> converge (with probability 1)</li><li>Not just \"probably close\" - paths actually reach the limit</li><li>Strongest possible guarantee</li><li>Only fails on a set of measure zero (impossible events)</li> </ul>|\n",
    "| **When you see it (ML)** |<ul><li>Training loss oscillating wildly and growing</li><li>Learning rate too high</li><li>Bug in code (gradient explosion)</li><li>Wrong optimization algorithm for the problem</li></ul>| <ul> <li>SGD with constant learning rate: loss distribution stable, but values oscillate</li> <li>Monte Carlo sampling: histogram shape stable, but samples vary</li><li>Ensemble predictions: distribution is calibrated, but individual model outputs vary</li><li>Uncertainty quantification: predictive distribution correct, point estimates noisy</li></ul> |<ul><li>Sample mean converging to population mean (Weak LLN)</li><li>SGD with learning rate decay $\\eta_n = 1/n$</li> <li>Monte Carlo estimates with increasing samples</li><li>Stochastic approximation algorithms</li><li>Online learning with diminishing step sizes</li></ul> |<ul><li>Sample mean (Strong LLN): XÃÑ‚Çô ‚Üí Œº almost surely</li> <li>SGD with aggressive learning rate decay: $\\eta_n = 1/n^2$</li><li>Robbins-Monro stochastic approximation (when $\\sum\\eta_n^2 < \\infty$)</li> <li>Well-designed online learning algorithms</li> <li>Martingale convergence theorems</li> </ul> |\n",
    "| **What to do**|<ul><li>Stop and debug!</li><li>Reduce learning rate</li><li>Add gradient clipping</li><li>Check for numerical instabilities</li></ul> |<ul><li>For distribution estimation: you're done (if that's your goal)</li><li>For point estimates: average multiple samples</li> <li>Use ensemble methods to reduce variance</li><li>Consider moving to probability or a.s. convergence (e.g., decrease LR)</li></ul> | <ul><li>This is usually sufficient for ML applications</li><li>Can use this to set stopping criteria</li><li>Compute confidence intervals for reliability</li><li>Monitor convergence with validation metrics</li></ul>|<ul><li>Best possible scenario</li><li>Can rely on convergence for theoretical analysis</li><li>Use in proofs of algorithm correctness</li><li>Justifies stopping when change becomes small</li></ul> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b642f9d",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"img/convergence-decision-tree.svg\" alt=\"Convergence Decision Tree\" width=\"1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfeb720",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-summary\">\n",
    "<h4>üîë Key Takeaways for ML Practitioners</h4>\n",
    "    \n",
    "<ol>\n",
    "<li><strong>Check which type you have:</strong>\n",
    "        <ul>\n",
    "        <li>Plot training curves - do they stabilize?</li>\n",
    "        <li>Run multiple random seeds - do all converge?</li>\n",
    "        <li>Monitor variance - does it decrease?</li>\n",
    "        </ul>\n",
    "</li>\n",
    "    \n",
    "<li><strong>Match convergence type to your needs:</strong>\n",
    "        <ul>\n",
    "        <li>Distribution only: OK for uncertainty quantification</li>\n",
    "        <li>In probability: OK for most ML tasks</li>\n",
    "        <li>Almost sure: Needed for theoretical guarantees</li>\n",
    "        </ul>\n",
    "</li>\n",
    "    \n",
    "<li><strong>Tune accordingly:</strong>\n",
    "        <ul>\n",
    "        <li>No convergence ‚Üí reduce LR, add regularization</li>\n",
    "        <li>Dist. only ‚Üí decrease LR over time if you need point convergence</li>\n",
    "        <li>Probability ‚Üí already good for practice!</li>\n",
    "        <li>Almost sure ‚Üí optimal setup for theory</li>\n",
    "        </ul>\n",
    "</li>\n",
    "    \n",
    "<li><strong>Use the right stopping criterion:</strong>\n",
    "        <ul>\n",
    "        <li>In probability: Stop when P(close) is high enough</li>\n",
    "        <li>Almost sure: Stop when actual changes are small</li>\n",
    "        <li>Dist. only: Can't use simple stopping criterion</li>\n",
    "        </ul>\n",
    "</li>\n",
    "</ol>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b595c",
   "metadata": {},
   "source": [
    "<h5>üéì Key Theorems Using Each Type</h5>\n",
    "\n",
    "<ul>\n",
    "<li><strong>Central Limit Theorem:</strong> Convergence in distribution</li>\n",
    "<li><strong>Weak Law of Large Numbers:</strong> Convergence in probability</li>\n",
    "<li><strong>Strong Law of Large Numbers:</strong> Almost sure convergence</li>\n",
    "<li><strong>Slutsky's Theorem:</strong> Uses convergence in probability</li>\n",
    "<li><strong>Continuous Mapping Theorem:</strong> Preserves type of convergence</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"background: #fff8e1; padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "<strong>Remember:</strong> Understanding convergence types helps you:<br>\n",
    "- Choose appropriate algorithms<br>\n",
    "- Set correct stopping criteria<br>\n",
    "- Interpret your results properly<br>\n",
    "- Prove theoretical guarantees<br>\n",
    "- Debug when things go wrong\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639a236",
   "metadata": {},
   "source": [
    "## Law of Large Numbers (LLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee8631",
   "metadata": {},
   "source": [
    "**SCENARIO 1.** Imagine you own a **casino**. You have a simple game:\n",
    "\n",
    "- Player pays $1 to play\n",
    "- Roll a fair die (6 sides)\n",
    "- If it lands on 6: player wins $5 (you lose $4)\n",
    "- If it lands on 1-5: player wins nothing (you keep $1)\n",
    "\n",
    "On average, you expect to make **0.17 per game** (let's verify: $-4 \\times 1/6 + 1 \\times 5/6 = 1/6 \\approx 0.17$).\n",
    "\n",
    "But there's uncertainty. \n",
    "\n",
    "Consider what happens:\n",
    "- After 1 game: You might be up $1 or down $4 (wild swings)\n",
    "- After 10 games: Still quite variable\n",
    "- After 100 games: What do you expect?\n",
    "- After 10,000 games: What about now?\n",
    "\n",
    "> Should you be worried about going bankrupt on a lucky day for players?\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cadab",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION:</h5> \n",
    "\n",
    "Simulate the casino scenario:\n",
    "\n",
    "1. Calculate the theoretical profit of the casino\n",
    "2. Simulate up to 10000 games (rolling a fair dice):\n",
    "    - for each game, calculate the casino profit\n",
    "    - calculate cumulative sum of profits\n",
    "3. Calculate running average\n",
    "\n",
    "```\n",
    "def simulate_casino_scenario(n_games:int=10000, cost_to_play:float=1.0, payout_on_six:float=5.0) -> tuple[float, np.array]:\n",
    "    \"\"\"\n",
    "    Simulates casino scenario. Calculates the theoretical expected profit of the casino. Then \"plays\" n_games and calculates the running average of profits.\n",
    "\n",
    "    Args:\n",
    "        n_games (int, optional): Number of games (trials) to consider in simulations. Defaults to 10000.\n",
    "        cost_to_play (float, optional): Cost a player pays to play. Defaults to 1.0.\n",
    "        payout_on_six (float, optional): Gain in case of winning (die rolls 6). Defaults to 5.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, np.array]: expected casino profit and running average of profits.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "4. Display the theoretical value and empirical values after 50, 500, 5000, and 10000 games.\n",
    "5. Visualise the result: casino profit as a function of number of games\n",
    "\n",
    "*Hint*: to calculate a cumulative sum, you can use [`np.cumsum()`](https://numpy.org/doc/2.3/reference/generated/numpy.cumsum.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def simulate_casino_scenario(n_games:int=10000, cost_to_play:float=1.0, payout_on_six:float=5.0) -> tuple[float, np.array]:\n",
    "    \"\"\"\n",
    "    Simulates casino scenario. Calculates the theoretical expected profit of the casino. Then \"plays\" n_games and calculates the running average of profits.\n",
    "\n",
    "    Args:\n",
    "        n_games (int, optional): Number of games (trials) to consider in simulations. Defaults to 10000.\n",
    "        cost_to_play (float, optional): Cost a player pays to play. Defaults to 1.0.\n",
    "        payout_on_six (float, optional): Gain in case of winning (die rolls 6). Defaults to 5.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, np.array]: expected casino profit and running average of profits.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71623009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "running_avg, expected, fig = casino_simulation_intro(py=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd754a8",
   "metadata": {},
   "source": [
    "**Key insight**:\n",
    "- Each individual game is RANDOM and UNPREDICTABLE\n",
    "- But the AVERAGE over many games becomes PREDICTABLE\n",
    "- The more games you play, the closer you get to the expected value\n",
    "- This is why casinos always make money in the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6b9d2",
   "metadata": {},
   "source": [
    "**SCENARIO 2.** Let's consider another scenario. This time we are talking about **election poll**. \n",
    "\n",
    "You're conducting a poll before an election. \n",
    "\n",
    "The Truth (unknown to pollsters):\n",
    "<ul>\n",
    "<li>52% of the population supports Candidate A</li>\n",
    "<li>48% supports Candidate B</li>\n",
    "</ul>\n",
    "\n",
    "Your Task: Estimate the support by randomly surveying people\n",
    "\n",
    "> How many people do you need to survey to get a reliable estimate?\n",
    "</br>\n",
    "\n",
    "10 people? 50 people? 100 people? 1000 people?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064966b6",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION:</h5> \n",
    "\n",
    "Simulate the election poll scenario:\n",
    "\n",
    "1. Calculate the theoretical profit of the casino\n",
    "2. Simulate up to 5000 participants (`max_surveys`): \n",
    "- Here, we consider that the choice is only between 2 candidates: candidate A and candidate B. So, we can use binomial distribution to model responses\n",
    "- Calculate cumulative support of candidate A   \n",
    "3. Calculate running average and errors (difference between the estimate and the true value)\n",
    "\n",
    "```\n",
    "def simulate_election_poll_scenario(true_support:float= 0.52, max_surveys:int=5000) -> tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Simulate the Election Poll Scenario. Two candidates are competing. \n",
    "    That's why binomial distribution with p=true_support is used. \n",
    "    \n",
    "    Calculates running average and errors (difference of the estimates with true value).\n",
    "\n",
    "    Args:\n",
    "        true_support (float, optional): True support of the first candidate (or Candidate A). Defaults to 0.52.\n",
    "        max_surveys (int, optional): Max number of participants of the survey. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        running_estimate (np.array): Running average (estimate) of the support of candidate A\n",
    "        errors (np.array): Difference between the estimates and the true value\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "4. Display the values for the following poll sizes: 10, 50, 100, 500, 1000, 5000.\n",
    "5. Visualise the results\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb975ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def simulate_election_poll_scenario(true_support:float= 0.52, max_surveys:int=5000) -> tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Simulate the Election Poll Scenario. Two candidates are competing. \n",
    "    That's why binomial distribution with p=true_support is used. \n",
    "    \n",
    "    Calculates running average and errors (difference of the estimates with true value).\n",
    "\n",
    "    Args:\n",
    "        true_support (float, optional): True support of the first candidate (or Candidate A). Defaults to 0.52.\n",
    "        max_surveys (int, optional): Max number of participants of the survey. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        running_estimate (np.array): Running average (estimate) of the support of candidate A\n",
    "        errors (np.array): Difference between the estimates and the true value\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3814f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298db065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "polling_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aad66e",
   "metadata": {},
   "source": [
    "**Key insight:**\n",
    "- Small polls ($n=50$): Estimates vary widely from poll to poll\n",
    "- Large polls ($n=1000$): Estimates cluster tightly around true value\n",
    "- As sample size increases, estimate converges to true population value\n",
    "- This is why professional polls typically survey 1000-1500 people. You don't need to survey everyone - just a large enough sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d5ceb",
   "metadata": {},
   "source": [
    "**Common Thread:**\n",
    "<ul>\n",
    "<li>Each individual observation is RANDOM and UNPREDICTABLE</li>\n",
    "<li>But the AVERAGE of many observations becomes PREDICTABLE</li>\n",
    "<li>The more observations, the closer we get to the \"true value\"</li>\n",
    "<li>This convergence happens REGARDLESS of the distribution!</li>\n",
    "</ul>\n",
    "\n",
    "This is the **Law of Large Numbers** (LLN).\n",
    "\n",
    "<p style=\"background: #fff; padding: 15px; border-radius: 8px; border-left: 5px solid #ff9800; margin: 15px 0;\">\n",
    "<strong>Informal Statement:</strong><br>\n",
    "When you repeat a random experiment many times and take the average, that average will get closer and closer to the expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58092edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insights \n",
    "bridge_to_formal_definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969b70a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: The Law of Large Numbers</h4>\n",
    "\n",
    "Setup: Let $X_1, X_2, X_3, ...$ be independent, identically distributed (i.i.d.) random variables with:</p>\n",
    "- Expected value: $E[X_i] = \\mu$\n",
    "- Variance: $Var(X_i) = \\sigma^2 < \\infty$ \n",
    "\n",
    "The sample mean is defined as: $$\\bar{X_n} = (X_1 + X_2 + ... + X_n) / n$$\n",
    "\n",
    "<h5>Weak Law of Large Numbers (WLLN):</h5>\n",
    "\n",
    "$$\\bar{X_n} \\xrightarrow[n\\rightarrow\\infty]{p} \\mu \\text{ (converges in probability)}$$\n",
    "\n",
    "*Formally*: For any $\\varepsilon > 0$,\n",
    "$$P(|\\bar{X_n} - \\mu| > \\varepsilon) \\rightarrow 0\\text{ as } n \\rightarrow \\infty$$\n",
    "\n",
    "*In words*: The probability that the sample mean is \"far\" from $\\mu$ goes to zero.\n",
    "\n",
    "<h5>Strong Law of Large Numbers (SLLN):</h5>\n",
    "\n",
    "$$\\bar{X_n} \\xrightarrow[n\\rightarrow\\infty]{a.s.} \\mu \\text{ (converges almost surely)}$$\n",
    "\n",
    "*Formally:*\n",
    "\n",
    "$$P(\\bar{X_n} \\rightarrow \\mu \\text{ as } n \\rightarrow \\infty) = 1$$\n",
    "\n",
    "*In words*: If you actually run the experiment, the sequence $\\bar{X_1}, \\bar{X_2}, \\bar{X_3}, ...$ will converge to $\\mu$ (with probability 1).\n",
    "\n",
    "\n",
    "**Intuitive Interpretation:**\n",
    "\n",
    "1. WLLN: \"For large $n$, $\\bar{X_n}$ is *probably* close to $\\mu$\"\n",
    "2. SLLN: \"If you run the experiment, $\\bar{X_n}$ *will* converge to $\\mu$ (almost certainly)\"\n",
    "\n",
    "\n",
    "**Key Requirements:**\n",
    "\n",
    "- Independence: Samples must be independent\n",
    "- Identical distribution: All from the same distribution\n",
    "- Finite mean: $E[X]$ must exist\n",
    "- Finite variance: $Var(X) < \\infty$ (for WLLN; SLLN only needs finite mean)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22010ce5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-idea\">\n",
    "<h4>üí°Key Insight</h4>\n",
    "\n",
    "In every case, averaging many independent random observations gives us a reliable estimate of the expected value.\n",
    "\n",
    "Why This Matters:\n",
    "<ul>\n",
    "<li><strong>Statistics:</strong> Justifies using sample means to estimate population means</li>\n",
    "<li><strong>Machine Learning:</strong> Explains why Monte Carlo methods work</li>\n",
    "<li><strong>Probability Theory:</strong> Foundation for Central Limit Theorem and inference</li>\n",
    "<li><strong>Real World:</strong> Why casinos make money, why polls work, why averaging reduces noise</li>\n",
    "</ul>\n",
    "\n",
    "**What LLN Does NOT Tell Us**\n",
    "\n",
    "<p>The Law of Large Numbers guarantees convergence but is <strong>silent on the rate</strong>.</p>\n",
    "\n",
    "Questions LLN Cannot Answer:\n",
    "<ul>\n",
    "<li>How many samples do we need to be within Œµ of Œº?</li>\n",
    "<li>How much does the error decrease when we double the sample size?</li>\n",
    "<li>What's the distribution of XÃÑ‚Çô - Œº for finite n?</li>\n",
    "</ul>\n",
    "\n",
    "For these answers, we need the Central Limit Theorem.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc6cf9",
   "metadata": {},
   "source": [
    "## Central Limit Theorem (CLT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabd5bf",
   "metadata": {},
   "source": [
    "<div class=\"alert-exercise\">\n",
    "<h5> QUESTION:</h5> \n",
    "\n",
    "We have several probability distributions with very different shapes:\n",
    "\n",
    "- $Uniform(0, 1)$: Flat, every value equally likely\n",
    "- $Exponential(\\lambda=1)$: Heavily right-skewed\n",
    "- $Normal(\\mu=0, \\sigma^2=1)$: Standard normal with bell-shaped PDF\n",
    "- $Binomial(n=10, p=0.5)$: Discrete, symmetric but not continuous\n",
    "- $Poisson(\\lambda=5)$: Discrete\n",
    "\n",
    "The Experiment:\n",
    "\n",
    "1. Pick a distribution (start with any one you like)\n",
    "2. Draw $n$ samples from it (e.g., $n=5$, $n=30$, $n=100$)\n",
    "3. Calculate the **mean** of those $n$ samples\n",
    "4. Repeat step 2-3 many times (e.g., 1000 times)\n",
    "5. Plot a histogram of all those sample means\n",
    "6. Try different values of $n$ and different distributions\n",
    "\n",
    "Questions to Investigate:\n",
    "\n",
    "1. What shape does the histogram of sample means have?\n",
    "2. Does this shape depend on which distribution you started with?\n",
    "3. How does the shape change as you increase $n$ (sample size)?\n",
    "4. How does the spread (width) of the histogram change as $n$ increases?\n",
    "5. Where is the center of the sample means distribution?\n",
    "\n",
    "</div>\n",
    "\n",
    "*Hints:*\n",
    "\n",
    "You'll need these tools:\n",
    "\n",
    "```\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "```\n",
    "\n",
    "1. Step 1: Create a distribution object\n",
    "\n",
    "```\n",
    "# Example: Exponential distribution\n",
    "dist = stats.expon(scale=1)\n",
    "\n",
    "# You can also try:\n",
    "# dist = stats.uniform(loc=0, scale=1)\n",
    "# dist = stats.binom(n=10, p=0.5)\n",
    "```\n",
    "\n",
    "2. Step 2: Generate ONE sample and compute its mean\n",
    "\n",
    "```\n",
    "sample_size = 30  # Try 5, 10, 30, 100\n",
    "one_sample = dist.rvs(size=sample_size)\n",
    "sample_mean = np.mean(one_sample)\n",
    "print(f\"One sample mean: {sample_mean}\")\n",
    "```\n",
    "\n",
    "3. Step 3: Repeat many times to collect sample means\n",
    "\n",
    "```\n",
    "n_repetitions = 1000\n",
    "sample_means = []\n",
    "\n",
    "for i in range(n_repetitions):\n",
    "    sample = dist.rvs(size=sample_size)\n",
    "    sample_means.append(np.mean(sample))\n",
    "\n",
    "# Convert to numpy array\n",
    "sample_means = np.array(sample_means)\n",
    "```\n",
    "\n",
    "4. Step 4: Visualize\n",
    "\n",
    "```\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sample_means, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Distribution of Sample Means (n={sample_size})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "You can create a 2√ó2 grid comparing different sample sizes ($n=5$, $n=30$, $n=100$, $n=1000$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def clt_solution(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Simplified solution showing CLT for multiple distributions and sample sizes.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int, optional): Number of sample means to generate. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        fig: matplotlib figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c07dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the solution\n",
    "clt_solution(n_samples=10)\n",
    "clt_solution(n_samples=100)\n",
    "clt_solution(n_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063bad14",
   "metadata": {},
   "source": [
    "**KEY OBSERVATIONS:**\n",
    "\n",
    "1. SHAPE BECOMES NORMAL\n",
    "    -   No matter which distribution you started with\n",
    "    -   The histogram of sample means looks bell-shaped (normal)\n",
    "    \n",
    "2. SAMPLE SIZE MATTERS\n",
    "    -  Left to right: as n increases, the shape becomes MORE normal\n",
    "    -  With $n=5$: still somewhat irregular\n",
    "    -  With $n=30$ or more: very close to normal shape\n",
    "    \n",
    "3. SPREAD DECREASES\n",
    "    -  Left to right: the histogram gets NARROWER\n",
    "    - This happens in a predictable way: $SE = \\sigma/\\sqrt{n}$\n",
    "    - Larger n ‚Üí smaller standard error ‚Üí more precise estimates\n",
    "    \n",
    "4. CENTER STAYS THE SAME\n",
    "    - The center (mean) of $\\text{sample means} \\approx \\text{population mean}$\n",
    "    - This is true regardless of $n$\n",
    "    - Sample mean is an unbiased estimator!\n",
    "    \n",
    "5. THE PATTERN IS UNIVERSAL\n",
    "    - Works for continuous distributions (Uniform, Exponential, Normal)\n",
    "    - Works for discrete distributions (Binomial, Poisson)\n",
    "    - Works for symmetric distributions (Binomial, Uniform)\n",
    "    - Works for skewed distributions (Exponential)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e174ba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-summary\">\n",
    "<h4>scipy.stats Cheat Sheet</h4>\n",
    "\n",
    "<h5>1Ô∏è‚É£ Creating a Distribution Object</h5>\n",
    "\n",
    "```\n",
    "from scipy import stats\n",
    "\n",
    "# Continuous distributions\n",
    "dist = stats.norm(loc=0, scale=1)           # Normal\n",
    "dist = stats.uniform(loc=0, scale=1)        # Uniform\n",
    "dist = stats.expon(scale=1)                 # Exponential\n",
    "dist = stats.gamma(a=2, scale=2)            # Gamma\n",
    "dist = stats.beta(a=2, b=5)                 # Beta\n",
    "\n",
    "# Discrete distributions\n",
    "dist = stats.binom(n=10, p=0.5)             # Binomial\n",
    "dist = stats.poisson(mu=5)                  # Poisson\n",
    "\n",
    "```\n",
    "\n",
    "<h5>2Ô∏è‚É£ Generating Random Samples</h5>\n",
    "\n",
    "```\n",
    "# Generate n random samples\n",
    "samples = dist.rvs(size=100)\n",
    "\n",
    "# With random seed for reproducibility\n",
    "samples = dist.rvs(size=100, random_state=42)\n",
    "\n",
    "# Generate multiple samples (for CLT demo)\n",
    "sample_means = [dist.rvs(size=30).mean() for _ in range(1000)]\n",
    "\n",
    "```\n",
    "\n",
    "<h5>3Ô∏è‚É£ Getting Distribution Parameters</h5>\n",
    "\n",
    "```\n",
    "mean = dist.mean()                          # Population mean\n",
    "std = dist.std()                            # Population std dev\n",
    "var = dist.var()                            # Population variance\n",
    "median = dist.median()                      # Median\n",
    "\n",
    "# All moments at once\n",
    "mean, var, skew, kurt = dist.stats(moments='mvsk')\n",
    "```\n",
    "\n",
    "\n",
    "<h5>4Ô∏è‚É£ Probability Functions</h5>\n",
    "\n",
    "```\n",
    "# PDF (continuous) or PMF (discrete)\n",
    "prob = dist.pdf(x)                          # For continuous\n",
    "prob = dist.pmf(x)                          # For discrete\n",
    "\n",
    "# CDF\n",
    "cumulative = dist.cdf(x)                    # P(X ‚â§ x)\n",
    "\n",
    "# Survival function\n",
    "survival = dist.sf(x)                       # P(X > x) = 1 - CDF(x)\n",
    "\n",
    "# Percent point function (inverse CDF)\n",
    "quantile = dist.ppf(0.95)                   # 95th percentile\n",
    "\n",
    "```\n",
    "\n",
    "<h5>5Ô∏è‚É£ CLT Demonstration Template</h5>\n",
    "\n",
    "```\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create distribution\n",
    "dist = stats.expon(scale=2)\n",
    "\n",
    "# 2. Generate sample means\n",
    "n_samples = 1000\n",
    "sample_size = 30\n",
    "sample_means = [dist.rvs(size=sample_size).mean() \n",
    "                for _ in range(n_samples)]\n",
    "\n",
    "# 3. Get theoretical values\n",
    "mu = dist.mean()\n",
    "sigma = dist.std()\n",
    "se = sigma / np.sqrt(sample_size)\n",
    "\n",
    "# 4. Plot\n",
    "plt.hist(sample_means, bins=50, density=True, alpha=0.7)\n",
    "x = np.linspace(min(sample_means), max(sample_means), 100)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, se), 'r-', linewidth=2)\n",
    "plt.title(f'CLT: Sample Means ~ N({mu:.2f}, {se:.4f}¬≤)')\n",
    "plt.show()\n",
    "\n",
    "# 5. Verify normality\n",
    "from scipy.stats import shapiro\n",
    "stat, p_value = shapiro(sample_means)\n",
    "print(f'Shapiro-Wilk test: p-value = {p_value:.4f}')\n",
    "\n",
    "```\n",
    "\n",
    "<h5>6Ô∏è‚É£ Useful Tips</h5>\n",
    "<ul>\n",
    "<li><strong>Freeze distribution:</strong> Create once, use many times for efficiency</li>\n",
    "<li><strong>Random state:</strong> Always set random_state for reproducibility</li>\n",
    "<li><strong>Vectorization:</strong> Pass arrays to .pdf(), .cdf(), etc. for speed</li>\n",
    "<li><strong>Documentation:</strong> Use <code>help(stats.norm)</code> or <code>stats.norm?</code> in Jupyter</li>\n",
    "<li><strong>List all distributions:</strong> <code>[d for d in dir(stats) if isinstance(getattr(stats, d), type)]</code></li>\n",
    "</ul>\n",
    "\n",
    "<h5>7Ô∏è‚É£ Complete Distribution List</h5>\n",
    "<p><strong>Continuous:</strong> norm, uniform, expon, gamma, beta, chi2, t, f, lognorm, weibull_min, pareto, cauchy, laplace, logistic, gumbel_r, rayleigh, and 80+ more!</p>\n",
    "<p><strong>Discrete:</strong> binom, poisson, geom, nbinom, hypergeom, zipf, and 10+ more!</p>\n",
    "\n",
    "<p><strong>Documentation:</strong> <a href=\"https://docs.scipy.org/doc/scipy/reference/stats.html\" target=\"_blank\">scipy.stats official docs</a></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "show_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b05d0",
   "metadata": {},
   "source": [
    "We've just discovered the Central Limit Theorem empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c71f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Central Limit Theorem (CLT)</h4>\n",
    "\n",
    "<p><strong>The Central Limit Theorem states:</strong></p>\n",
    "\n",
    "When you take random samples of size $n$ from ANY distribution (with finite mean $\\mu$ and variance $\\sigma^2$), and calculate their means, those sample means will follow an approximately **normal distribution** with mean $\\mu$ and standard deviation $\\sigma/\\sqrt{n}$.\n",
    "\n",
    "\n",
    "<p><strong>In mathematical notation:</strong></p>\n",
    "\n",
    "\n",
    "Let $X_1, X_2, ..., X_n$ be i.i.d. with mean $\\mu<\\infty$ and std $\\sigma<\\infty$. Then:\n",
    "$$\\bar{X_n} \\sim N(\\mu, \\sigma^2/n) \\text{ as } n \\rightarrow \\infty$$\n",
    "\n",
    "Another form:\n",
    "\n",
    "Let $S_n = \\sum_{i}^n X_i$. Then, using the standardisation procedure:\n",
    "$$\\frac{S_n-n\\mu}{\\sigma \\sqrt{n}} \\xrightarrow[n\\rightarrow + \\infty]{\\mathcal{d}} Y \\sim {N}(0,1)$$\n",
    "\n",
    "**Key properties:**\n",
    "\n",
    "- Universality: Works for ANY starting distribution\n",
    "- Normality: Result is always approximately normal\n",
    "- Predictability: We can predict the standard error: $SE = \\sigma/\\sqrt{n}$\n",
    "- Practicality: Explains why normal distribution appears everywhere in nature and ML</li>\n",
    "\n",
    "</div>\n",
    "\n",
    "<p><strong>Why this matters for Machine Learning:</strong></p>\n",
    "<ul>\n",
    "<li><strong>Monte Carlo methods:</strong> We can quantify uncertainty in our estimates</li>\n",
    "<li><strong>Gradient descent:</strong> Understand noise in mini-batch gradients</li>\n",
    "<li><strong>Statistical inference:</strong> Build confidence intervals and hypothesis tests</li>\n",
    "<li><strong>Model evaluation:</strong> Understand variability in performance metrics</li>\n",
    "<li><strong>A/B testing:</strong> Determine required sample sizes</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>The ‚àön rule:</strong> This is THE fundamental rate in statistics and ML</p>\n",
    "<ul>\n",
    "<li>To halve your error, you need 4√ó more samples</li>\n",
    "<li>To get 10√ó better accuracy, you need 100√ó more data</li>\n",
    "<li>This explains why \"big data\" is so important</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee755b3",
   "metadata": {},
   "source": [
    "> Why finite variance is crucial? </br>\n",
    "\n",
    "Consider the formula $\\frac{S_n-n\\mu}{\\sigma \\sqrt{n}} \\xrightarrow[n\\rightarrow + \\infty]{\\mathcal{d}} Y \\sim {N}(0,1)$. Notice that $\\sigma$ appears in the denominator. If $\\sigma^2 = \\infty$, this formula becomes meaningless:\n",
    "- We're dividing by infinity\n",
    "- The rate of convergence ($1/\\sqrt{n}$) depends on $\\sigma$ being finite\n",
    "- The limiting distribution's variance comes from $\\sigma^2/n$\n",
    "\n",
    "*Intuition*: Variance measures \"typical deviation from the mean.\" If variance is infinite, there's no \"typical\" scale - extreme values are so common that averaging doesn't help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed30447",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h4>‚ö†Ô∏è Common Mistake: When CLT Fails</h4>\n",
    "\n",
    "<p><strong>CLT requires finite variance!</strong> If œÉ¬≤ = ‚àû, CLT does not apply.</p>\n",
    "\n",
    "<p><strong>Example: Cauchy Distribution</strong></p>\n",
    "<ul>\n",
    "<li>Has undefined mean and infinite variance</li>\n",
    "<li>Sample means do NOT converge to normal</li>\n",
    "<li>Sample means follow the SAME Cauchy distribution (wild!)</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>ML Implications:</strong></p>\n",
    "<ul>\n",
    "<li><strong>Heavy-tailed losses:</strong> Some loss functions (e.g., with outliers) might have infinite variance</li>\n",
    "<li><strong>Learning rates:</strong> If gradients have infinite variance, standard convergence theory breaks</li>\n",
    "<li><strong>Robust statistics:</strong> Use median instead of mean for heavy-tailed data</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Rule of thumb:</strong> CLT works well when n ‚â• 30 for most distributions. For heavily skewed distributions, might need n ‚â• 100.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo Cauchy distribution\n",
    "demo_clt_cauchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b8f85",
   "metadata": {},
   "source": [
    "The Cauchy distribution is the canonical example of infinite variance. its PDF: $f_X(x) = \\frac{1}{\\pi(1 + x^2)}$.\n",
    "\n",
    "Properties:\n",
    "- Mean: undefined (integral doesn't converge)\n",
    "- Variance: infinite ($\\sigma^2 = \\infty$)\n",
    "- Heavy tails: $P(|X| > x) \\sim 1/x$ (much heavier than normal)\n",
    "\n",
    "The critical difference between Normal and Cauchy distribution is in the TAILS, which are hard to see in a histogram, especially in a linear scale (that's why in the demo above a log-scale is used).\n",
    "\n",
    "1. Cauchy has such heavy tails that extreme values dominate\n",
    "2. One extremely large value can completely change the mean\n",
    "3. As $n$ grows, you're increasingly likely to hit an extreme value\n",
    "4. These extremes keep the variance infinite\n",
    "5. Averaging doesn't reduce variability\n",
    "\n",
    "*Mathematical property* (stability under averaging):\n",
    "The Cauchy distribution is \"stable\" - the sum (or mean) of Cauchy random variables is still Cauchy. This is extremely unusual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd0ae1",
   "metadata": {},
   "source": [
    "> What is the difference between LLN and CLT? </br>\n",
    "\n",
    "**Law of Large Numbers (LLN)**:\n",
    "\n",
    "LLN tells you WHERE the sample mean goes (it converges to Œº), but not HOW FAST or what the distribution looks like.\n",
    "\n",
    "In practical terms: *\"Your estimate will be close to the truth with enough data\"*\n",
    "\n",
    "**Central Limit Theorem (CLT)**:\n",
    "\n",
    "CLT tells you HOW FAST the sample mean converges (at rate 1/‚àön) and WHAT SHAPE the distribution has (approximately normal), allowing you to quantify uncertainty.\n",
    "\n",
    "In practical terms: *\"Here's how close, with what probability, and how much data you need\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf6f28",
   "metadata": {},
   "source": [
    "## Problem-Solving Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfc083",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-idea\">\n",
    "<h4>üí° Problem-Solving Strategy: How to Apply These Concepts</h4>\n",
    "<span class=\"idea-type\">General Framework</span>\n",
    "\n",
    "<div class=\"idea-steps\">\n",
    "<h5>Step-by-Step Approach:</h5>\n",
    "<ol>\n",
    "<li><strong>Identify the estimator:</strong> What are you averaging? (Sample mean, gradient, Monte Carlo estimate, etc.)</li>\n",
    "\n",
    "<li><strong>Check LLN applicability:</strong>\n",
    "   <ul>\n",
    "   <li>Are samples independent?</li>\n",
    "   <li>Identically distributed?</li>\n",
    "   <li>Finite mean?</li>\n",
    "   </ul>\n",
    "   If YES ‚Üí Estimator converges to true value</li>\n",
    "\n",
    "<li><strong>Check CLT applicability:</strong>\n",
    "   <ul>\n",
    "   <li>All LLN conditions PLUS</li>\n",
    "   <li>Finite variance?</li>\n",
    "   </ul>\n",
    "   If YES ‚Üí Can quantify uncertainty and convergence rate</li>\n",
    "\n",
    "<li><strong>Apply the ‚àön rule:</strong>\n",
    "   <ul>\n",
    "   <li>Standard error: SE = œÉ/‚àön</li>\n",
    "   <li>95% CI width: ‚âà 4 √ó SE = 4œÉ/‚àön</li>\n",
    "   <li>To improve accuracy by factor k: need k¬≤ times more samples</li>\n",
    "   </ul>\n",
    "</li>\n",
    "\n",
    "<li><strong>Estimate required sample size:</strong>\n",
    "   <ul>\n",
    "   <li>Desired accuracy: Œµ</li>\n",
    "   <li>For 95% confidence: need SE ‚âà Œµ/2</li>\n",
    "   <li>Solve: œÉ/‚àön = Œµ/2 ‚Üí n = (2œÉ/Œµ)¬≤</li>\n",
    "   </ul>\n",
    "</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<p><strong>Common ML Applications:</strong></p>\n",
    "<ul>\n",
    "<li><strong>Monte Carlo methods:</strong> Use this framework to determine number of samples</li>\n",
    "<li><strong>Gradient estimation:</strong> Understand mini-batch size vs noise trade-off</li>\n",
    "<li><strong>Bootstrap:</strong> Determine number of bootstrap iterations</li>\n",
    "<li><strong>A/B testing:</strong> Calculate required sample size for desired statistical power (coming in Week 8!)</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9a441",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-exercise\">\n",
    "<h4>Integration Exercise: Monte-Carlo Method for œÄ Estimation</h4>\n",
    "\n",
    "*Scenario*: You're implementing a Monte Carlo method to estimate œÄ using the classic quarter-circle method.\n",
    "\n",
    "*Method*: Generate random points $(x,y)$ in $[0,1]\\times[0,1]$. Let $X_1, X_2, ... , X_n$ and $Y_1, Y_2, ..., Y_n$ be independent variables with distribution $\\mathcal{U}[0; 1]$.\n",
    "Count how many fall inside the quarter circle ($x^2 + y^2 \\leq 1$). \n",
    "The ratio estimates $\\pi/4$.</p>\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Let $P_n = \\frac{4}{n}\\sum_{i=1}^{n}Z_i$ where $Z_i = \\mathbf{1}_{X^2_i + Y^2_i\\leq 1} \\sim Bernoulli(p=\\pi/4)$. Show that $P_n$ converges almost surely to $\\pi$\n",
    "2. Let $\\alpha > 0$. Using Chebyshev's inequality, determine $n_\\alpha$ such that for all $n$\n",
    "greater than $n_\\alpha$, $\\mathbb{P}(|P_n - \\pi| > \\alpha) \\leq 0.05$\n",
    "3. Repeat the previous question using CLT\n",
    "4. Use the implementation of the estimator\n",
    "5. Run it with different sample sizes: 100, 1000, 10000, 100000\n",
    "6. For each sample size, repeat 1000 times to see the distribution\n",
    "7. Verify that:\n",
    "   - Estimates converge to œÄ (Law of Large Numbers)\n",
    "   - Distribution of estimates is approximately normal (Central Limit Theorem)\n",
    "   - Standard error decreases as 1/‚àön (CLT rate)\n",
    "8. Determine: How many samples needed to estimate œÄ within ¬±0.01 with 95% confidence?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi(n_samples):\n",
    "    \"\"\"Single estimate of œÄ using Monte Carlo\"\"\"\n",
    "    x = np.random.uniform(0, 1, n_samples)\n",
    "    y = np.random.uniform(0, 1, n_samples)\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "    pi_estimate = 4 * np.mean(inside_circle)\n",
    "    return pi_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018eb630",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal Solution</summary>\n",
    "\n",
    "1. Almost Sure Convergence\n",
    "\n",
    "Let's denote $R^2_i = X^2_i + Y^2_i$.\n",
    "\n",
    "*Independence*\n",
    "\n",
    "**Lemma:** Let $n$ be a natural number, $X_1$, $X_2$, ... , $X_n$ be independent random variables and $k$ be an integer between $1$ and $n$. Let $\\varphi: \\mathbb{R}^k \\rightarrow \\mathbb{R}$ and $\\psi :  \\mathbb{R}^{n-k} \\rightarrow \\mathbb{R}$ be measurable mappings. Then the variables $Y = \\varphi(X_1, ... , X_k)$ and $Z = \\psi(X_{k+1}, ... , X_n)$ are independent. \n",
    "\n",
    "Consequently, the variables $R^2_i = X^2_i + Y^2_i$ are independent and $Z_i = \\mathbf{1}_{R^2_i\\leq 1}$ too. \n",
    "\n",
    "*Identical distribution*\n",
    "\n",
    "The variables $R_i$ come from the same distribution. The condition $R_i^2 \\leq 1$ is the same for all $Z_i$. Then $Z_i$ come from the same distribution.\n",
    "\n",
    "According to the problem statement, $Z_i = \\mathbf{1}_{R^2_i\\leq 1} \\sim \\mathcal{B}(\\pi/4)$. The expectation of $Z_i$ is therefore $m=\\pi/4$.\n",
    "\n",
    "According to the **strong law of large numbers**: $$\\lim_{n\\rightarrow +\\infty}\\overline{X}_n = m$$\n",
    "\n",
    "In our case, \n",
    "\n",
    "$$\\lim_{n\\rightarrow +\\infty}\\overline{Z}_n =\\frac{1}{n}\\sum_{i=1}^{n}Z_i = m = \\pi/4$$\n",
    "\n",
    "The expression in question on the left is multiplied by 4 compared to that of the law, so: \n",
    "\n",
    "$$\\frac{4}{n}\\sum_{i=1}^{n}Z_i = 4m = 4\\cdot (\\pi/4) = \\pi$$\n",
    "\n",
    "Hence, the convergence almost surely.\n",
    "\n",
    "2. Finding $n_\\alpha$ using Chebyshev's inequality\n",
    "\n",
    "According to **Chebyshev's inequality**: $\\forall \\alpha \\in \\mathbb{R}, \\alpha>0$:\n",
    "\n",
    "$$\\mathbb{P}(|X - m| \\geq \\alpha) \\leq \\frac{\\sigma^2}{\\alpha^2}$$\n",
    "\n",
    "In our case, $Z_i \\sim \\mathcal{B}(\\pi/4)$, $P_n = \\frac{4}{n}\\sum_{i=1}^{n}Z_i$ which converges almost surely to $\\pi$, its expectation $m = \\pi$ and variance:\n",
    "\n",
    "$$\\sigma^2 = \\sum_{i=1}^{n} \\frac{4}{n}Var(Z_i) =\\bigg(\\frac{4}{n}\\bigg)^2 \\sum_{i=1}^{n} Var(Z_i) = \\bigg(\\frac{4}{n}\\bigg)^2 np(1-p) = \\frac{16}{n^2}\\frac{n\\pi}{4}(1-\\pi/4) = \\frac{4\\pi}{n}(1-\\pi/4)$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\mathbb{P}(|X - m| \\geq \\alpha) = \\mathbb{P}(|P_n - m| \\geq \\alpha) = \\mathbb{P}(|P_n - \\pi| \\geq \\alpha) \\leq \\frac{\\frac{4\\pi}{n}(1-\\pi/4)}{\\alpha^2}$$\n",
    "\n",
    "Let's take up the right part. According to the problem statement it must be less than 0.05:\n",
    "$$\\frac{\\frac{4\\pi}{n}(1-\\pi/4)}{\\alpha^2} \\leq 0.05$$\n",
    "\n",
    "$$\\frac{4\\pi}{n}(1-\\pi/4) \\leq 0.05\\alpha^2$$\n",
    "\n",
    "$$\\frac{4\\pi(1-\\pi/4)}{0.05\\alpha^2} \\leq n$$\n",
    "\n",
    "3. Finding $n_\\alpha$ using CLT\n",
    "\n",
    "According to the CLT:\n",
    "\n",
    "$$\\frac{S_n-nm}{\\sigma \\sqrt{n}} \\xrightarrow[n\\rightarrow + \\infty]{\\mathcal{d}} Y \\sim\\mathcal{N}(0,1)$$\n",
    "where $S_n=\\sum_{i=1}^{n}X_i$.\n",
    "\n",
    "In our case, (the standardization) \n",
    "\n",
    "$$U = \\frac{X-m}{\\sigma} = \\frac{P_n - \\pi}{\\sqrt{\\frac{4\\pi}{n}(1-\\pi/4)}} \\xrightarrow[n\\rightarrow + \\infty]{\\mathcal{L}} \\mathcal{N}(0,1)$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\mathbb{P}(|P_n - \\pi| \\geq \\alpha) = \\mathbb{P}\\left(|U| \\geq \\frac{\\alpha}{\\sqrt{\\frac{4\\pi}{n}(1-\\pi/4)}}\\right) \\leq 0.05$$\n",
    "Note that the expression under the probability contains the absolute value $|U|$. \n",
    "\n",
    "Recall the symmetry of the normal distribution:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/normal-absolute-value.png\" alt=\"Symmetry of Normal distribution\" width=\"400px\">\n",
    "</center>\n",
    "\n",
    "That's why, we need to consider the value $0.025$. Using the `scipy.stats.norm.ppf` method, we obtain $u=1.96$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\alpha}{\\sqrt{\\frac{4\\pi}{n}(1-\\pi/4)}} \\geq 1.96$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\frac{\\alpha^2}{\\frac{4\\pi}{n}(1-\\pi/4)} \\geq 1.96^2$$\n",
    "\n",
    "$$\\frac{n\\alpha^2}{4\\pi(1-\\pi/4)} \\geq 1.96^2$$\n",
    "\n",
    "$$n \\geq \\frac{1.96^2 \\cdot 4\\pi(1-\\pi/4)}{\\alpha^2}$$\n",
    "\n",
    "Let's introduce the effective base variance that is independent of $n$ as:\n",
    "$$\\sigma_0^2 ‚Äã = \\frac{Var(4\\sum_{i=1}^n ‚ÄãZ_i‚Äã)}{n} = \\frac{16\\cdot \\sum_{i=1}^n Var(Z_i‚Äã)}{n} = \\frac{16\\cdot n Var(Z_i‚Äã)}{n} = 16 Var(Z_i‚Äã) = 16 \\pi/4 (1 ‚àí \\pi/4) = 4\\pi(1-\\pi/4)$$\n",
    "\n",
    "In this case, the inequality becomes:\n",
    "\n",
    "$$n \\geq \\frac{1.96^2 \\cdot \\sigma_0^2}{\\alpha^2}$$\n",
    "\n",
    "Let's compare the obtained results:\n",
    "\n",
    "|error| Chebychev's inequality | CLT |\n",
    "|---:|:---:|:---:|\n",
    "|general case| $$n \\geq \\frac{4\\pi(1-\\pi/4)}{0.05\\alpha^2}$$ | $$n \\geq \\frac{1.96^2 \\cdot 4\\pi(1-\\pi/4)}{\\alpha^2}$$ |\n",
    "|$\\alpha = 0.01$ | $$n \\geq \\frac{4\\pi(1-\\pi/4)}{0.05\\cdot 0.01^2}\\approx 539,354$$ | $$n \\geq \\frac{1.96^2 \\cdot 4\\pi(1-\\pi/4)}{0.01^2}\\approx 103,599$$ | \n",
    "\n",
    "We note that there is a factor of 0.05 with the $n_\\alpha$ calculated in the previous question. The central limit theorem gives a finer and presumably more accurate value.\n",
    "We just need to make sure it is high enough so that we can consider the approximation given by the CLT valid.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ff22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value of the CDF of standard normal distribution for u=0.025\n",
    "U = stats.norm(loc=0, scale=1)\n",
    "print(U.ppf(0.025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_term = 4*np.pi*(1-np.pi/4)\n",
    "alpha = 0.01\n",
    "\n",
    "chebyshev_est = common_term / (0.05 * alpha**2)\n",
    "clt_est = 1.96**2 * common_term / alpha**2\n",
    "\n",
    "print(f\"Using Chebyshev's inequality: {chebyshev_est}\")\n",
    "print(f\"Using CLT: {clt_est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3b206",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Formula: Required Sample Size for Given Precision</h4>\n",
    "\n",
    "Let $X_i$ be i.i.d. with mean $\\mu$.\n",
    "\n",
    "To estimate a mean $\\mu$ with a margin of error $\\pm \\varepsilon$, confidence level $(1 - \\alpha)$ (typically 95%, so $\\alpha=0.05$), and population standard deviation $\\sigma$, a required sample size is given by:\n",
    "\n",
    "$$n = \\bigg(\\frac{z_{\\alpha/2}\\cdot \\sigma}{\\varepsilon}\\bigg)^2$$\n",
    "\n",
    "where $z_{\\alpha/2}$ is the critical value from standard normal distribution $N(0, 1)$.\n",
    "\n",
    "Some typical critical values:\n",
    "- For 95% confidence: $z_{0.025} = 1.96 \\approx 2$\n",
    "- For 99% confidence: $z_{0.005} = 2.576 \\approx 2.6$\n",
    "- For 90% confidence: $z_{0.05} = 1.645 \\approx 1.6$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "sample_sizes = [100, 1000, 10000, 100000, 1000000]\n",
    "n_trials = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_convergence_analysis(sample_sizes, n_trials=1000):    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    true_pi = np.pi\n",
    "    \n",
    "    for idx, n in enumerate(sample_sizes):\n",
    "        row, col = idx // 3, idx % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Generate multiple estimates\n",
    "        estimates = [estimate_pi(n) for _ in range(n_trials)]\n",
    "        estimates = np.array(estimates)\n",
    "        \n",
    "        # Plot histogram\n",
    "        ax.hist(estimates, bins=50, density=True, alpha=0.7, \n",
    "                color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Overlay theoretical normal\n",
    "        mean_est = np.mean(estimates)\n",
    "        std_est = np.std(estimates)\n",
    "        x = np.linspace(estimates.min(), estimates.max(), 1000)\n",
    "        y = stats.norm.pdf(x, mean_est, std_est)\n",
    "        ax.plot(x, y, 'r-', linewidth=2, label=f'N({mean_est:.3f}, {std_est:.3f}¬≤)')\n",
    "        \n",
    "        # Mark true value\n",
    "        ax.axvline(true_pi, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'True œÄ = {true_pi:.4f}')\n",
    "        \n",
    "        ax.set_title(f'n = {n:,}\\nMean: {mean_est:.4f}, SE: {std_est:.4f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('œÄ Estimate')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print statistics\n",
    "        error = abs(mean_est - true_pi)\n",
    "        within_01 = np.mean(np.abs(estimates - true_pi) < 0.01) * 100\n",
    "        print(f\"n = {n:6d}: Mean = {mean_est:.5f}, SE = {std_est:.5f}, \"\n",
    "              f\"Error = {error:.5f}, Within ¬±0.01: {within_01:.1f}%\")\n",
    "    \n",
    "    plt.suptitle('Monte Carlo Estimation of œÄ: Convergence Analysis', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_convergence_analysis(sample_sizes=sample_sizes, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_se(sample_sizes, n_trials=1000):\n",
    "    # Plot SE vs n (log-log scale)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Collect observed standard errors\n",
    "    std_errors = []\n",
    "    for n in sample_sizes:\n",
    "        estimates = [estimate_pi(n) for _ in range(n_trials)]\n",
    "        # as each estimate is a sample mean according to Monte-Carlo method,\n",
    "        # the std of these sample means is, by def. the standard error\n",
    "        std_errors.append(np.std(estimates))\n",
    "    \n",
    "    # Plot 1: SE vs n\n",
    "    ax1.plot(sample_sizes, std_errors, 'bo-', linewidth=2, markersize=8, label='Observed SE')\n",
    "    \n",
    "    # estimate sigma from one sample size (recover sigma from observed SE for sample size n0)\n",
    "    sigma_estimate = std_errors[0] * np.sqrt(sample_sizes[0])\n",
    "    # Theoretical line (1/‚àön behavior)\n",
    "    theoretical_se = sigma_estimate / np.sqrt(np.array(sample_sizes))\n",
    "    ax1.plot(sample_sizes, theoretical_se, 'r--', linewidth=2, label='Theoretical: ' + r\"$\\propto$\" + '1/‚àön')\n",
    "    \n",
    "    ax1.set_xlabel('Sample Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Standard Error', fontsize=12)\n",
    "    ax1.set_title('Standard Error vs Sample Size', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    # Plot 2: Required samples for different error tolerances\n",
    "    target_errors = np.array([0.1, 0.05, 0.01, 0.005, 0.001])\n",
    "    conf_level = 0.95\n",
    "    alpha = 1 - conf_level\n",
    "    z_alpha_05 = stats.norm.ppf(alpha/2, loc=0, scale=1)\n",
    "    # Using rule: for 95% CI, need SE ‚âà error/2 (1.96 ‚âà 2)\n",
    "    # SE = sigma/‚àön, so n = (sigma/SE)¬≤ = (sigma/(error/2))¬≤ = (2sigma/error)¬≤\n",
    "    \n",
    "    # Estimate sigma from our data (recover sigma from observed SE for sample size n0)\n",
    "    sigma_estimate = std_errors[0] * np.sqrt(sample_sizes[0])\n",
    "    required_samples = (z_alpha_05 * sigma_estimate / target_errors) ** 2\n",
    "    \n",
    "    ax2.barh(range(len(target_errors)), required_samples, color='coral', edgecolor='black')\n",
    "    ax2.set_yticks(range(len(target_errors)))\n",
    "    ax2.set_yticklabels([f'¬±{e:.3f}' for e in target_errors])\n",
    "    ax2.set_xlabel('Required Sample Size', fontsize=12)\n",
    "    ax2.set_ylabel('Desired Accuracy (95% CI)', fontsize=12)\n",
    "    ax2.set_title('Samples Needed for Different Accuracy Levels', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(required_samples):\n",
    "        ax2.text(v, i, f' {int(v):,}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_se(sample_sizes=sample_sizes, n_trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf690e0",
   "metadata": {},
   "source": [
    "## Return to the Opening Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd1b29",
   "metadata": {},
   "source": [
    "Let's answer all four questions now.\n",
    "\n",
    "<div>\n",
    "<p><strong>Question 1: Will we EVER get exactly 0.7000?</strong></p>\n",
    "<p><strong>Answer:</strong> Almost certainly NO (probability = 0). But by the <strong>Strong Law of Large Numbers</strong>, \n",
    "we'll converge to 0.7 almost surely. That is, P(estimate ‚Üí 0.7) = 1.</p>\n",
    "\n",
    "<p style=\"background: white; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "<em>LLN tells us: We'll get arbitrarily close, but not exactly equal.</em>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><strong>Question 2: How fast does the \"jumpiness\" decrease?</strong></p>\n",
    "<p><strong>Answer:</strong> By the <strong>Central Limit Theorem</strong>, the standard error decreases as <strong>1/‚àön</strong>.</p>\n",
    "\n",
    "\n",
    "$$SE(estimate) = \\sigma / \\sqrt{n} \\approx 0.458 / \\sqrt{n}$$\n",
    "\n",
    "(where $\\sigma = \\sqrt{p(1-p)} = \\sqrt{0.7 \\times 0.3} \\approx 0.458$)\n",
    "\n",
    "\n",
    "<table style=\"margin: 10px auto; border-collapse: collapse; background: white;\">\n",
    "<tr style=\"background: #e3f2fd;\"><th style=\"padding: 8px; border: 1px solid #ccc;\">Samples (n)</th><th style=\"padding: 8px; border: 1px solid #ccc;\">SE</th><th style=\"padding: 8px; border: 1px solid #ccc;\">95% CI Width</th></tr>\n",
    "<tr><td style=\"padding: 8px; border: 1px solid #ccc;\">100</td><td style=\"padding: 8px; border: 1px solid #ccc;\">0.046</td><td style=\"padding: 8px; border: 1px solid #ccc;\">¬±0.090</td></tr>\n",
    "<tr><td style=\"padding: 8px; border: 1px solid #ccc;\">1,000</td><td style=\"padding: 8px; border: 1px solid #ccc;\">0.014</td><td style=\"padding: 8px; border: 1px solid #ccc;\">¬±0.028</td></tr>\n",
    "<tr><td style=\"padding: 8px; border: 1px solid #ccc;\">10,000</td><td style=\"padding: 8px; border: 1px solid #ccc;\">0.0046</td><td style=\"padding: 8px; border: 1px solid #ccc;\">¬±0.009</td></tr>\n",
    "<tr><td style=\"padding: 8px; border: 1px solid #ccc;\">100,000</td><td style=\"padding: 8px; border: 1px solid #ccc;\">0.0014</td><td style=\"padding: 8px; border: 1px solid #ccc;\">¬±0.003</td></tr>\n",
    "</table>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><strong>Question 3: Can we guarantee we're within 0.01 of the true value?</strong></p>\n",
    "<p><strong>Answer:</strong> YES! Using CLT, we can construct confidence intervals.</p>\n",
    "\n",
    "\n",
    "For 95% confidence with margin of error ¬±0.01:<br>\n",
    "$n = (1.96 \\times \\sigma / 0.01)¬≤ = (1.96 \\times 0.458 / 0.01)^2 \\approx 8,068\\text{ samples}$\n",
    "\n",
    "\n",
    "<p><em>With 8,068 samples, we're 95% confident our estimate is within ¬±0.01 of 0.7</em></p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><strong>Question 4: What if we only have 100 samples (1 second)?</strong></p>\n",
    "<p><strong>Answer:</strong> Tell your manager the uncertainty:</p>\n",
    "\n",
    "With 100 samples, our estimate is $\\hat{p}_{100} = \\frac{1}{100}\\sum_{i=1}^100X_i$ where $X_i\\sim Bernoulli(p=0.7)$. \n",
    "\n",
    "Properties of this estimator:\n",
    "- Mean $E[\\hat{p}_{100}] = 0.7$\n",
    "- Variance $Var(\\hat{p}_{100}) = \\frac{p(1-p)}{n} = \\frac{0.7(1-0.7)}{100} = \\frac{0.21}{100} = 0.0021$\n",
    "- Standard Error: $SE = \\sqrt{0.0021} = 0.0458$\n",
    "\n",
    "By CLT, for $n=100$:\n",
    "$$\\hat{p}_{100} \\sim N(0.7, 0.0458^2)$$\n",
    "\n",
    "For a 95% confidence interval: $\\hat{p}_{100} \\pm z_{0.025}\\times SE$ where $z_{0.025} = 1.96$ (from Standard Normal CDF).\n",
    "\n",
    "Margin of error: $z_{0.025}\\times SE = 1.96 \\times 0.0458 = 0.0898 \\approx 0.09$\n",
    "\n",
    "Therefore, our estimate $0.7 \\pm 0.09$ (95\\% CI). \n",
    "\n",
    "That means the true probability could be anywhere from 0.61 to 0.79. <br>\n",
    "For a ¬±0.01 precision, we need 8,000 samples (80 seconds).\n",
    "\n",
    "\n",
    "<p><strong>Trade-off:</strong> Speed vs Precision - a fundamental constraint in probabilistic systems</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5a699",
   "metadata": {},
   "source": [
    "## Common Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cac137",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h5>‚ö†Ô∏è Common Pitfalls</h5>\n",
    "\n",
    "<ol>\n",
    "<li><strong>Assuming fast convergence:</strong> ‚àön is slow! 100√ó better accuracy needs 10,000√ó more data</li>\n",
    "<li><strong>Ignoring independence:</strong> LLN/CLT require independent samples</li>\n",
    "<li><strong>Infinite variance:</strong> CLT fails for heavy-tailed distributions (check your data!)</li>\n",
    "<li><strong>Small sample sizes:</strong> CLT is asymptotic; n ‚â• 30 is rule of thumb</li>\n",
    "<li><strong>Confusing convergence types:</strong> Almost sure ‚â† in probability ‚â† in distribution</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de46c67",
   "metadata": {},
   "source": [
    "## ML Application Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25338f4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-summary\">\n",
    "<h4>ü§ñ ML Applications Summary</h4>\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse; background: white; margin: 10px 0;\">\n",
    "<tr style=\"background: #e8f5e8;\">\n",
    "<th style=\"padding: 10px; border: 1px solid #ccc;\">Application</th>\n",
    "<th style=\"padding: 10px; border: 1px solid #ccc;\">How Convergence Concepts Apply</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\"><strong>Monte Carlo</strong></td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Error ‚àù 1/‚àön; use CLT for confidence intervals</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\"><strong>SGD/Mini-batch</strong></td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Gradient noise ‚àù 1/‚àöB; batch size trade-offs</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\"><strong>Bootstrap</strong></td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Bootstrap SE ‚àù 1/‚àöB; determines # iterations</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\"><strong>Model Ensembles</strong></td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Prediction variance ‚àù 1/M (M models)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\"><strong>A/B Testing</strong></td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Sample size calculation using CLT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\"><strong>Dropout Uncertainty</strong></td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Average over T passes; uncertainty ‚àù 1/‚àöT</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8994d",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1448d8b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-summary\">\n",
    "<h4>üéØ Key Takeaways</h4>\n",
    "\n",
    "<p><strong>1. Three Types of Convergence</strong></p>\n",
    "<ul>\n",
    "<li><strong>In Probability:</strong> P(|X‚Çô - X| > Œµ) ‚Üí 0 (weakest)</li>\n",
    "<li><strong>Almost Sure:</strong> P(X‚Çô ‚Üí X) = 1 (stronger)</li>\n",
    "<li><strong>In Distribution:</strong> CDFs converge (different flavor)</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>2. Law of Large Numbers</strong></p>\n",
    "<ul>\n",
    "<li><strong>What:</strong> Sample means converge to population mean</li>\n",
    "<li><strong>When:</strong> i.i.d. samples with finite mean (and variance for WLLN)</li>\n",
    "<li><strong>Limitation:</strong> Doesn't specify convergence rate</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>3. Central Limit Theorem</strong></p>\n",
    "<ul>\n",
    "<li><strong>What:</strong> Sample means are approximately normal: XÃÑ‚Çô ~ N(Œº, œÉ¬≤/n)</li>\n",
    "<li><strong>Magic:</strong> Works for ANY distribution (with finite variance)</li>\n",
    "<li><strong>Rate:</strong> Standard error = œÉ/‚àön (the famous ‚àön rule!)</li>\n",
    "<li><strong>Power:</strong> Enables confidence intervals and hypothesis tests</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h5>üîë Essential Formulas</h5>\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse; background: white; margin: 10px 0;\">\n",
    "<tr style=\"background: #e3f2fd;\">\n",
    "<th style=\"padding: 10px; border: 1px solid #ccc;\">Concept</th>\n",
    "<th style=\"padding: 10px; border: 1px solid #ccc;\">Formula</th>\n",
    "<th style=\"padding: 10px; border: 1px solid #ccc;\">Interpretation</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Sample Mean</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">XÃÑ‚Çô = (X‚ÇÅ+...+X‚Çô)/n</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Average of n samples</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Expected Value</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">E[XÃÑ‚Çô] = Œº</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Unbiased estimator</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Variance</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Var(XÃÑ‚Çô) = œÉ¬≤/n</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Decreases with n</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Standard Error</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">SE = œÉ/‚àön</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Std dev of estimate</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">95% CI</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">XÃÑ ¬± 1.96¬∑œÉ/‚àön</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Confidence interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">Sample Size</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">n = (z¬∑œÉ/Œµ)¬≤</td>\n",
    "<td style=\"padding: 8px; border: 1px solid #ccc;\">For margin of error Œµ</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c85b07",
   "metadata": {},
   "source": [
    "## Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261afa6e",
   "metadata": {},
   "source": [
    "- [The Central Limit Theorem, Clearly Explained!!! by StatQuest](https://www.youtube.com/watch?v=YAlJCEDH2uY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probability-statistics-ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
