{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4a1fa8",
   "metadata": {},
   "source": [
    "# Point Estimation of Population Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4396e",
   "metadata": {},
   "source": [
    "Author & Instructor: Diana NURBAKOVA, PhD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/styles.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be567b",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243efe7e",
   "metadata": {},
   "source": [
    "By the end of this lesson, you will be able to:\n",
    "- Define and distinguish point estimators\n",
    "- Evaluate estimator properties (bias, variance, MSE)\n",
    "- Derive MLE for standard distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca013cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_style(\"whitegrid\")\n",
    "#sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da65c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider, Dropdown\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Segoe UI Emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a03a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the \"resources\" directory to the path\n",
    "project_root = Path().resolve().parent\n",
    "resources_path = project_root / 'resources'\n",
    "sys.path.insert(0, str(resources_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56326c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimators import (generate_hook_data, explore_sampling_distribution, demonstrate_estimator_concept, visualize_bias_variance_tradeoff, plot_heads_tails, \n",
    "                        demonstrate_likelihood_concept, plot_binomial, demonstrate_prior_importance, compare_mle_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d575e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4>üéØ Today's Challenge: The Neural Network Initialization Mystery</h4>\n",
    "\n",
    "You're training a neural network for image classification. A critical hyperparameter is the weight initialization scale $\\sigma$.\n",
    "\n",
    "- Too small ‚Üí vanishing gradients, slow learning\n",
    "- Too large ‚Üí exploding gradients, instability\n",
    "- Just right ‚Üí optimal convergence\n",
    "\n",
    "You run 50 training experiments and record the final validation accuracy for different $\\sigma$ values. Your best result shows $\\hat{\\sigma} = 0.15$ gave 94.2% accuracy.\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "1. Is $\\sigma = 0.15$ the \"true\" optimal value?\n",
    "2. If you had run 500 experiments instead of 50, would you get the same estimate?\n",
    "3. How do you quantify how \"wrong\" your estimate might be?\n",
    "4. Your colleague claims $\\sigma = 0.12$ is better. Who's right?\n",
    "\n",
    "By the end of today: You'll have a mathematical framework to answer all these questions with confidence.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "generate_hook_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf422f0",
   "metadata": {},
   "source": [
    "> If we run 50 MORE experiments, will we get exactly $\\hat{\\sigma} = 0.15$ again? (Yes / No / Maybe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11866518",
   "metadata": {},
   "source": [
    "## What is a Statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0ccc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Statistic</h4>\n",
    "\n",
    "A **statistic** is any function of the sample data that doesn't depend on unknown parameters.\n",
    "\n",
    "If $X_1, X_2, ..., X_n$ is a random sample, then $T(X_1, X_2, ..., X_n)$ is a statistic.\n",
    "\n",
    "*Examples of Statistics:*\n",
    "- Sample mean: $\\bar{X} = (1/n)\\sum_i^n X_i$\n",
    "- Sample variance: $s^2 = (1/(n-1))\\sum_i^n(X_i - \\bar{X})^2$\n",
    "- Sample median: middle value when data is sorted\n",
    "- Sample maximum: $max(X_1, ..., X_n)$\n",
    "- Sample range: $max(X_i) - min(X_i)$\n",
    "\n",
    "*Not statistics (depend on unknown parameters)*:\n",
    "- $(\\hat{X} - \\mu)/\\sigma$ where $\\mu, \\sigma$ are unknown \n",
    "- $P(X > \\theta)$ where $\\theta$ is unknown \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353f84a",
   "metadata": {},
   "source": [
    "### Two Types of Statistics for Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac2810",
   "metadata": {},
   "source": [
    "| | Point Statistic (Point Estimator) | Interval (Range) Statistic (Interval Estimator) |\n",
    "|--|----|----|\n",
    "|**Definition**| A **single number** that estimates the parameter| A **range of plausible values** for the parameter|\n",
    "|**Notation**| $\\hat{\\theta}$ (theta-hat) represents a point estimate of $\\theta$| interval containing the point estimate and the margin of error, e.g. $\\mu ¬± E$ where $E$ is a margin of error| \n",
    "|**Characteristics:**|- Simple, easy to interpret</br>- No information about uncertainty</br>- \"Best guess\" at the true value|- Provides range of uncertainty</br>- Comes with confidence level (e.g., 95%)</br>- More informative than point estimate</br>- Accounts for sampling variability|\n",
    "|**Interpretation**||*\"We are 95% confident that the true parameter lies in this interval\"*| \n",
    "|**Examples**|- Estimating population mean $\\mu$: use $\\bar{X} = 94.3$</br>- Estimating population variance $\\sigma^2$: use $s^2 = 15.7$</br>- Estimating success probability $p$: use $\\hat{p} = 0.65$|- Estimating $\\mu$: [92.1, 96.5] (95% confidence interval)</br>- Estimating $\\sigma^2$: [12.3, 21.4]</br>- Estimating $p$: [0.58, 0.72]|\n",
    "| **Use in ML**|- Model parameters (weights, biases)</br>- Performance metrics (accuracy = 0.87)</br>- Quick decision-making|- Model comparison: \"Is model A really better than B?\"</br>- Performance ranges: accuracy $\\in$ [0.84, 0.90]</br>- Risk assessment</br>- Communicating uncertainty to stakeholders|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46002e44",
   "metadata": {},
   "source": [
    "## What is an Estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae0bc7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Estimator</h4>\n",
    "\n",
    "An **estimator** is a rule (function) that takes data and produces an estimate of an unknown parameter.\n",
    "\n",
    "**Notation:**\n",
    "- $\\theta$ = true (unknown) parameter\n",
    "- $X_1, X_2, ..., X_n$ = observed data (random sample)\n",
    "- $\\hat{\\theta} = \\hat{\\theta}(X_1, ..., X_n)$ = estimator (a function of the data)\n",
    "\n",
    "**Key insight:** An estimator is itself a random variable (because it depends on random data)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4f6a8",
   "metadata": {},
   "source": [
    "Let's consider the darts analogy for a better understanding estimators.\n",
    "\n",
    "Imagine throwing darts at a target. The **bullseye** represents the **true parameter value** $\\theta$, and each **dart** represents an **estimate** $\\hat{\\theta}$ from a different sample.\n",
    "\n",
    "We simulate the same estimation process many times (like throwing many darts), each time with a different random sample. This shows us the **sampling distribution** of our estimator‚Äîhow the estimates vary from sample to sample.\n",
    "\n",
    "What we see:\n",
    "- Red star (‚òÖ): The true parameter value (bullseye)\n",
    "- Blue dots: Individual estimates from different samples (dart throws)\n",
    "- Spread pattern: How the estimator behaves across many samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# darts analogy\n",
    "demonstrate_estimator_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc87fe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application Spotlight: Where Do We Use Estimators?</h4>\n",
    "\n",
    "Everywhere in machine learning:\n",
    "\n",
    "- Neural Networks: Estimating optimal weights from training data\n",
    "- Clustering: Estimating cluster centers (k-means uses sample means)\n",
    "- Gaussian Mixture Models: Estimating $\\mu$ and $\\sigma$ for each component\n",
    "- Reinforcement Learning: Estimating value functions from rewards\n",
    "- Generative Models: Estimating distribution parameters (GANs, VAEs)\n",
    "\n",
    "The fundamental question: Given data, what's our best guess for the model parameters?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba359374",
   "metadata": {},
   "source": [
    "Let's consider the following setup.\n",
    "\n",
    "We repeatedly:\n",
    "1. Draw a random sample of size $n$ from a population\n",
    "2. Compute an estimate $\\hat{\\theta}$ from that sample\n",
    "3. Plot the estimate as a point\n",
    "4. Build up a histogram of all estimates\n",
    "\n",
    "For the sake of comparison, we will use the true values of the distribution parameters (e.g. true $\\mu = 5$ and $\\sigma = 2$).\n",
    "\n",
    "As we increase the number of samples:\n",
    "\n",
    "1. Distribution Shape Emerges\n",
    "   - Initially: just a few scattered points\n",
    "   - Eventually: a clear bell-shaped curve (often normal by CLT)\n",
    "   - The shape tells us about the estimator's behavior\n",
    "\n",
    "2. Center (Bias)\n",
    "   - Where is the histogram centered?\n",
    "   - If centered on the green line (true $\\theta$): **unbiased** \n",
    "   - If shifted left or right: **biased** \n",
    "\n",
    "3. Spread (Variance)\n",
    "   - How wide is the histogram?\n",
    "   - Narrow spread: **low variance** (precise) \n",
    "   - Wide spread: **high variance** (imprecise) \n",
    "\n",
    "4. Convergence\n",
    "   - With more samples, the histogram stabilizes\n",
    "   - This demonstrates the Law of Large Numbers\n",
    "   - The empirical distribution ‚Üí theoretical distribution\n",
    "\n",
    "\n",
    "Parameters to explore:\n",
    "\n",
    "1. Sample Size (n):\n",
    "- Small n ‚Üí wider sampling distribution (more uncertainty)\n",
    "- Large n ‚Üí narrower sampling distribution (less uncertainty)\n",
    "- This is why \"more data is better\"\n",
    "\n",
    "2. Number of Samples:\n",
    "- More samples ‚Üí smoother histogram\n",
    "- Shows the sampling distribution more clearly\n",
    "- In practice, we only get ONE sample, but this helps us understand uncertainty\n",
    "\n",
    "3. Different Estimators:\n",
    "- Compare sample mean vs. sample median\n",
    "- See which has lower variance\n",
    "- Understand when each is preferable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive exploration of sampling distribution\n",
    "interact(explore_sampling_distribution,\n",
    "         true_mean=FloatSlider(min=0, max=10, step=0.5, value=5, description='True Œº:'),\n",
    "         true_std=FloatSlider(min=0.5, max=5, step=0.5, value=2, description='True œÉ:'),\n",
    "         sample_size=IntSlider(min=10, max=200, step=10, value=30, description='Sample size:'),\n",
    "         n_experiments=IntSlider(min=100, max=2000, step=100, value=1000, description='# Samples:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ddbd8",
   "metadata": {},
   "source": [
    "Key Insights from This Exploration\n",
    "\n",
    "1. Every estimate has uncertainty\n",
    "   - A single sample gives one estimate\n",
    "   - Different samples give different estimates\n",
    "   - The sampling distribution quantifies this variability\n",
    "\n",
    "2. Sample size matters enormously\n",
    "   - Larger n ‚Üí estimates closer to truth\n",
    "   - This is the foundation of statistical inference\n",
    "\n",
    "3. Not all estimators are equal\n",
    "   - Some are centered better (less bias)\n",
    "   - Some are more consistent (less variance)\n",
    "   - We need both properties!\n",
    "\n",
    "4. Statistics is about distributions, not just numbers\n",
    "   - Don't just report $\\hat{\\theta} = 5.3$\n",
    "   - Think about: \"How would this estimate change with different data?\"\n",
    "   - The sampling distribution answers this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44439dfb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Key Insight: Estimators are Random Variables</h4>\n",
    "\n",
    "Because estimators depend on random data, they have their own probability distributions (called sampling distributions).\n",
    "\n",
    "This means:\n",
    "\n",
    "- Different samples ‚Üí different estimates\n",
    "- We need to understand the distribution of our estimator\n",
    "- Properties like bias and variance characterize estimator quality\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88427f64",
   "metadata": {},
   "source": [
    "## Properties of Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c4cde",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Three Critical Properties of Estimators</h4>\n",
    "\n",
    "Given an estimator $\\hat{\\theta}$ for parameter $\\theta$:\n",
    "\n",
    "1. **Bias**:\n",
    "\n",
    "$$Bias(\\hat{\\theta}, \\theta) = E[\\hat{\\theta}] - \\theta$$\n",
    "\n",
    "- Measures systematic error\n",
    "\n",
    "- $\\hat{\\theta}$ is **unbiased** if $E[\\hat{\\theta}] = \\theta$, i.e. $Bias(\\hat{\\theta}, \\theta) = 0$\n",
    "\n",
    "2. **Variance**:\n",
    "\n",
    "$$Var(Bias(\\hat{\\theta}, \\theta)) = E[(Bias(\\hat{\\theta}, \\theta) - E[Bias(\\hat{\\theta}, \\theta)])^2]$$\n",
    "\n",
    "- Measures spread/uncertainty\n",
    "\n",
    "- Lower variance ‚Üí more consistent estimates\n",
    "\n",
    "3. **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$MSE(\\hat{\\theta}, \\theta) = E[(\\hat{\\theta} - Œ∏)^2]$$\n",
    "\n",
    "Combines both: $$MSE(\\hat{\\theta}, \\theta) = Bias^2(\\hat{\\theta}, \\theta) + Var(\\hat{\\theta})$$\n",
    "\n",
    "- Measures the dispersion of the results around the true value\n",
    "\n",
    "- Overall measure of estimator quality\n",
    "\n",
    "- If $MSE(\\hat{\\theta}, \\theta) \\xrightarrow[n\\rightarrow \\infty]{} 0$, then the estimator $\\hat{\\theta}$ converges to $\\theta$ \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79d742",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-exercise\">\n",
    "<h4>Calculated Example: Bias of and Estimator of Poisson Distribution Parameter ùúÜ</h4>\n",
    "\n",
    "1. Calculate the bias of the estimator $\\hat{m}_n = \\bar{X}$ of the parameter $\\lambda$ of Poisson distribution $\\mathcal{P}(\\lambda)$, i.e. $X_i \\sim \\mathcal{P}(\\lambda)$.\n",
    "2. Calculate MSE of the estimator $\\hat{m}_n = \\bar{X}$\n",
    "\n",
    "*Reminder*: $P(X = k) = e^{-k}\\frac{\\lambda^k}{k!}$, $\\mathbb{E}X = \\lambda$, $Var(X) = \\lambda$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89454777",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Reveal solution</summary>\n",
    "\n",
    "1. Finding Bias\n",
    "\n",
    "$$Bias(\\hat{\\theta}, \\theta) = E[\\hat{\\theta}] - \\theta$$\n",
    "\n",
    "In our case: $$\\left\\{\\begin{array}{ll}\\theta = \\lambda\\\\ \\hat{\\theta} = \\bar{X}\\end{array}\\right.$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$Bias(\\bar{X}, \\lambda) = E[\\bar{X}] - \\lambda = \\bigg[\\text{by def. } \\bar{X} = \\frac{1}{n}\\sum_i^nX_i\\bigg] = E\\bigg[\\frac{1}{n}\\sum_i^nX_i\\bigg] - \\lambda =$$\n",
    "\n",
    "$$= \\bigg[\\text{by propr. of E } E[aX + b] = aE[X] + b\\bigg] = \\frac{1}{n}E\\bigg[\\sum_i^nX_i\\bigg] - \\lambda = \\bigg[\\text{by propr. of E } E[X + Y] = E[X] + E[Y]\\bigg] = \\frac{1}{n}\\sum_i^n E[X_i] - \\lambda =$$\n",
    "\n",
    "$$= \\bigg[\\text{as } X_i \\sim \\mathcal{P}(\\lambda) \\text{ and } EX = \\lambda \\bigg] = \\frac{1}{n}\\sum_i^n \\lambda - \\lambda = \\frac{1}{n}n \\lambda - \\lambda = \\mathbf{0}$$\n",
    "\n",
    "As $Bias = 0$, this is *unbiased estimator*.\n",
    "\n",
    "2. Finding MSE\n",
    "\n",
    "$$MSE(\\hat{\\theta}, \\theta) = E[(\\hat{\\theta} - Œ∏)^2] = Bias^2(\\hat{\\theta}, \\theta) + Var(\\hat{\\theta})$$\n",
    "\n",
    "In our case, $Bias(\\hat{\\theta}, \\theta) = 0$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$MSE(\\bar{X}, \\lambda) = 0 + Var(\\bar{X}) = \\bigg[\\text{by def. } \\bar{X} = \\frac{1}{n}\\sum_i^nX_i\\bigg] = Var\\bigg(\\frac{1}{n}\\sum_i^nX_i\\bigg) = $$\n",
    "\n",
    "$$= \\bigg[\\text{by propr. of Var } Var(aX + b) = a^2Var(X)\\bigg] =  \\bigg(\\frac{1}{n}\\bigg)^2 Var\\bigg(\\sum_i^nX_i\\bigg) =$$\n",
    "\n",
    "$$= \\bigg[\\text{by propr. of Var for indep. r.v.} Var(X + Y) = Var(X) + Var(Y)\\bigg] =  \\frac{1}{n^2} \\sum_i^n Var(X_i) =$$\n",
    "\n",
    "$$= \\bigg[\\text{as } X_i \\sim \\mathcal{P}(\\lambda) \\text{ and } Var(X) = \\lambda \\bigg] = \\frac{1}{n^2} n\\lambda = \\frac{\\lambda}{n} \\xrightarrow[n\\rightarrow \\infty]{} 0$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be91776",
   "metadata": {},
   "source": [
    "Thus, every estimator can be characterized along two independent dimensions:\n",
    "\n",
    "|| **BIAS (Systematic Error)**| **VARIANCE (Random Error)**|\n",
    "|---|----|---|\n",
    "|**Question**| \"On average, does the estimator hit the target?\"| \"How much do estimates vary from sample to sample?\"\n",
    "**Formula**| $Bias = E[\\hat{\\theta}] - \\theta$ | $Variance = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]$|\n",
    "**Visual cue**| Where is the cluster of estimates *centered*?|How *spread out* is the cluster of estimates?|\n",
    "|**Low value**| Estimates centered on true value (target)|Tight cluster of estimates (consistent)|\n",
    "|**High value**| Estimates consistently shifted away from true value|Wide scatter of estimates (inconsistent)|\n",
    "|**Reduces with mode data?**| NO | YES |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec27f5",
   "metadata": {},
   "source": [
    "Let's get back to our darts analogy and consider four scenarios:\n",
    "\n",
    "1. **Low Bias, Low Variance** (Excellent)\n",
    "   - Darts cluster tightly around bullseye\n",
    "   - Consistently accurate (low bias) and consistently precise (low variance)\n",
    "   - *Example*: Using $\\bar{X}$ to estimate $\\mu$ for normal data\n",
    "\n",
    "2. **Low Bias, High Variance** (Good)\n",
    "   - Darts scattered but centered on bullseye\n",
    "   - Unbiased on average\n",
    "   - But individual throws are unreliable\n",
    "   - Need more data to reduce \n",
    "   - Note: better than biased estimators\n",
    "   - *Example*: Median with small sample size\n",
    "\n",
    "3. **High Bias, Low Variance** (Problematic)\n",
    "   - Darts cluster tightly but miss the target\n",
    "   - Consistently wrong in the same direction\n",
    "   - Precision without accuracy\n",
    "   - More data won't help (bias doesn't decrease with $n$)\n",
    "   - Note: sometimes accepted if variance reduction is dramatic\n",
    "   - *Example*: Biased coin estimator that always adds 0.1\n",
    "\n",
    "4. **High Bias, High Variance** (Worst)\n",
    "   - Darts scattered AND off-target\n",
    "   - Neither accurate nor precise\n",
    "   - *Example*: Using a terrible estimator like \"first observation + 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance \n",
    "visualize_bias_variance_tradeoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c4f42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Key Insights: Bias and Variance</h4>\n",
    "\n",
    "**Bias = Systematic error**\n",
    "- Where is your dart cluster *centered*?\n",
    "- Low bias: centered on target (bullseye)\n",
    "- High bias: consistently off to one side\n",
    "\n",
    "**Variance = Random error**\n",
    "- How *spread out* are your darts?\n",
    "- Low variance: tight cluster\n",
    "- High variance: wide scatter\n",
    "\n",
    "**The Goal**: Low bias AND low variance\n",
    "- Hit the bullseye consistently\n",
    "- This means: $E[\\hat{\\theta}] = \\theta$ (unbiased) AND $Var(\\hat{\\theta})$ is small\n",
    "\n",
    "**The Trade-off**: Sometimes we accept a little bias to get much lower variance\n",
    "- Like a slightly off-center but very tight dart cluster\n",
    "- This is the bias-variance trade-off in machine learning\n",
    "- *Example*: Ridge regression accepts small bias for lower variance\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019025e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application Spotlight: Bias-Variance in Model Selection</h4>\n",
    "\n",
    "This is THE fundamental tradeoff in machine learning.\n",
    "\n",
    "**Underfitting (High Bias)**:\n",
    "\n",
    "- Model too simple\n",
    "- Systematically misses patterns\n",
    "- Low training AND test accuracy\n",
    "\n",
    "**Overfitting (High Variance):**\n",
    "\n",
    "- Model too complex\n",
    "- Learns noise in training data\n",
    "- High training accuracy, poor test accuracy\n",
    "\n",
    "*Goal*: Find the sweet spot that minimizes MSE = Bias¬≤ + Variance\n",
    "\n",
    "*Example*: Polynomial regression degree selection\n",
    "\n",
    "- Degree 1: High bias (too simple)\n",
    "- Degree 20: High variance (too complex)\n",
    "- Degree 3-5: Just right for most problems\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1262658",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-exercise\">\n",
    "<h4>Question: Mini-Exercise on Estimator Properties</h4>\n",
    "\n",
    "Given data from $N(\\mu=10, \\sigma^2=3^2)$, analyze the three estimators below: sample mean, sample mean of first half of the data + 5, and sample median.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Run 1000 experiments with n=50 samples each\n",
    "2. For each estimator, calculate:\n",
    "\n",
    "- Empirical bias\n",
    "- Empirical variance\n",
    "- Empirical MSE\n",
    "\n",
    "3. Which estimator is best? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6567907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define estimators\n",
    "def estimator_A(data):\n",
    "    \"\"\"Sample mean\"\"\"\n",
    "    return np.mean(data)\n",
    "    \n",
    "def estimator_B(data):\n",
    "    \"\"\"Mean of first half + 5\"\"\"\n",
    "    return np.mean(data[:len(data)//2]) + 5\n",
    "    \n",
    "def estimator_C(data):\n",
    "    \"\"\"Median\"\"\"\n",
    "    return np.median(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ffa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def solution_mini_exercise_estimator_properties(true_mean = 10, true_std = 3, n = 50, n_trials = 1000):\n",
    "    \"\"\"\n",
    "    Solution for mini-exercise: Comparing Three Estimators\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Run simulation\n",
    "    estimates_A, estimates_B, estimates_C = [], [], []\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbcf3ae",
   "metadata": {},
   "source": [
    "COMMENTS: <span style=\"color:red\">YOUR COMMENTS HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bbedb2",
   "metadata": {},
   "source": [
    "| When to use Median | When to use Mean |\n",
    "|------|------|\n",
    "| Data has outliers (robust to extreme values) | No significant outliers |\n",
    "| Skewed distributions (median = 'typical' value)</br> Heavy-tailed distributions </br> When a few extreme values shouldn't influence result | Data is (approximately) normally distributed | \n",
    "| | Want most efficient estimator (lowest variance) | \n",
    "| | Need to use established statistical tests (t-test, etc.) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cecbdd5",
   "metadata": {},
   "source": [
    "Let's explore the case with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mean = 10\n",
    "true_std = 3\n",
    "n = 50 \n",
    "n_trials = 1000\n",
    "\n",
    "# Generate data with outliers\n",
    "n_outlier_experiments = 1000\n",
    "sample_size_outlier = 50\n",
    "contamination_rate = 0.1  # 10% outliers\n",
    "    \n",
    "estimates_mean_outlier = []\n",
    "estimates_median_outlier = []\n",
    "    \n",
    "for _ in range(n_outlier_experiments):\n",
    "    # Generate mostly normal data\n",
    "    sample = np.random.normal(true_mean, true_std, sample_size_outlier)\n",
    "        \n",
    "    # Add some outliers\n",
    "    n_outliers = int(contamination_rate * sample_size_outlier)\n",
    "    outlier_indices = np.random.choice(sample_size_outlier, n_outliers, replace=False)\n",
    "    sample[outlier_indices] = np.random.uniform(30, 50, n_outliers)  # Extreme values\n",
    "        \n",
    "    estimates_mean_outlier.append(np.mean(sample))\n",
    "    estimates_median_outlier.append(np.median(sample))\n",
    "    \n",
    "estimates_mean_outlier = np.array(estimates_mean_outlier)\n",
    "estimates_median_outlier = np.array(estimates_median_outlier)\n",
    "    \n",
    "# Calculate MSE with outliers\n",
    "mse_mean_outlier = np.mean((estimates_mean_outlier - true_mean)**2)\n",
    "mse_median_outlier = np.mean((estimates_median_outlier - true_mean)**2)\n",
    "    \n",
    "print(f\"With {contamination_rate:.0%} outliers:\")\n",
    "print(f\"  MSE (Mean):   {mse_mean_outlier:.4f}\")\n",
    "print(f\"  MSE (Median): {mse_median_outlier:.4f}\")\n",
    "\n",
    "if mse_median_outlier < mse_mean_outlier:\n",
    "    print(\"Median WINS! It's robust to outliers.\")\n",
    "    print(f\"Median reduces MSE by {(1 - mse_median_outlier/mse_mean_outlier)*100:.1f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation \n",
    "fig, axes = plt.subplots(ncols=3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Distributions of all three estimators\n",
    "ax = axes[0]\n",
    "\n",
    "ax.hist(estimates_A, bins=40, alpha=0.6, color='steelblue', \n",
    "       edgecolor='black', density=True, label='A: Mean')\n",
    "ax.hist(estimates_B, bins=40, alpha=0.6, color='red', \n",
    "       edgecolor='black', density=True, label='B: Half+5')\n",
    "ax.hist(estimates_C, bins=40, alpha=0.6, color='green', \n",
    "       edgecolor='black', density=True, label='C: Median')\n",
    "\n",
    "ax.axvline(true_mean, color='gold', linewidth=4, linestyle='--',\n",
    "          label=f'True Œº = {true_mean}', zorder=10)\n",
    "\n",
    "ax.set_xlabel('Estimate', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Sampling Distributions of Three Estimators\\n(No Outliers)', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bias-Variance decomposition\n",
    "ax = axes[1]\n",
    "\n",
    "names = list(results.keys())\n",
    "biases_sq = [results[name]['bias']**2 for name in names]\n",
    "variances = [results[name]['variance'] for name in names]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x, biases_sq, width, label='Bias¬≤', color='red', alpha=0.7)\n",
    "bars2 = ax.bar(x, variances, width, bottom=biases_sq, label='Variance', \n",
    "               color='blue', alpha=0.7)\n",
    "\n",
    "# Add MSE values on top\n",
    "for i, name in enumerate(names):\n",
    "    mse = results[name]['mse']\n",
    "    ax.text(i, mse + 0.1, f'MSE={mse:.2f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Bias-Variance Decomposition: MSE = Bias¬≤ + Variance', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Mean', 'Half+5', 'Median'], fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: With outliers comparison\n",
    "ax = axes[2]\n",
    "\n",
    "ax.hist(estimates_mean_outlier, bins=40, alpha=0.6, color='steelblue',\n",
    "       edgecolor='black', density=True, label='Mean (affected by outliers)')\n",
    "ax.hist(estimates_median_outlier, bins=40, alpha=0.6, color='green',\n",
    "       edgecolor='black', density=True, label='Median (robust)')\n",
    "\n",
    "ax.axvline(true_mean, color='gold', linewidth=4, linestyle='--',\n",
    "          label=f'True Œº = {true_mean}', zorder=10)\n",
    "\n",
    "ax.set_xlabel('Estimate', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Effect of Outliers: Median vs Mean\\n(10% contamination)', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103de5cc",
   "metadata": {},
   "source": [
    "### Is My Variance High?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d488a",
   "metadata": {},
   "source": [
    "> How to evaluate if variance is high? </br>\n",
    "\n",
    "Note that variance bu itself is just a number (e.g. $Var(\\hat{\\theta} = 0.1)$). To be able to assess if its value is high or low, we need more information, e.g.:\n",
    "- If $\\theta \\approx 1000$, then variance of 0.1 is tiny (very precise)\n",
    "- If $\\theta \\approx 0.01$, then variance of 0.1 is huge (very imprecise)\n",
    "\n",
    "1. **Method 1: Coefficient of Variation**\n",
    "\n",
    "The most common approach is to compare variance to the magnitude of what is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb7c06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Coefficient of Variation</h4>\n",
    "\n",
    "**Coefficient of Variation** (CV) is a standardized measure of dispersion of a probability distribution or frequency distribution. It is defined as the ratio of the standard deviation \n",
    "$\\sigma$ to the mean $\\mu$ (or its absolute value, $|\\mu |$). It shows the extent of variability in relation to the mean of the population.\n",
    "\n",
    "$$CV(X) = \\frac{\\sigma}{\\mu} = \\frac{\\sqrt{Var(X)}}{|E[X]|}$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- CV < 0.1 (10%): Low variance, very precise\n",
    "- CV ‚âà 0.1 - 0.3 (10-30%): Moderate variance, acceptable\n",
    "- CV > 0.3 (30%): High variance, imprecise \n",
    "- CV > 1 (100%): Very high variance, unreliable\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient of variation\n",
    "estimates = np.array([98, 102, 95, 103, 97, 101])\n",
    "mean_estimate = np.mean(estimates)  # 99.33\n",
    "std_estimate = np.std(estimates, ddof=1)  # 3.08\n",
    "\n",
    "CV = std_estimate / mean_estimate\n",
    "print(f\"CV = {CV:.3f} = {CV*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f1708",
   "metadata": {},
   "source": [
    "2. **Method 2: Relative to Bias (MSE Context)**\n",
    "\n",
    "**Question**: Is variance large compared to bias?\n",
    "\n",
    "**Rule**: \n",
    "- If Variance >> Bias¬≤: High variance problem (need more data)\n",
    "- If Bias¬≤ >> Variance: High bias problem (need better model)\n",
    "- If Bias¬≤ ‚âà Variance: Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06533386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "true_param = 10\n",
    "estimates = np.array([15.1, 15.3, 14.9, 15.2, 15.0])\n",
    "\n",
    "mean_est = np.mean(estimates)  # 15.1\n",
    "bias = mean_est - true_param   # 5.1\n",
    "variance = np.var(estimates, ddof=1)  # 0.02\n",
    "\n",
    "print(f\"Bias¬≤ = {bias**2:.3f}\")      # 26.01\n",
    "print(f\"Variance = {variance:.3f}\")  # 0.02\n",
    "\n",
    "if variance < bias**2:\n",
    "    print(\"Bias¬≤ >> Variance ‚Üí HIGH BIAS problem\")\n",
    "    print(\"Variance is actually LOW relative to bias\")\n",
    "elif variance > bias**2:\n",
    "    print(\"Bias¬≤ << Variance ‚Üí HIGH VARIANCE problem (need more data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001f11e",
   "metadata": {},
   "source": [
    "3. **Method 3: Relative to Sample Size**\n",
    "\n",
    "**Expected behavior**: Var(Œ∏ÃÇ) should decrease as n increases\n",
    "\n",
    "For most estimators:\n",
    "$$Var(\\hat{\\theta}) \\propto 1/n$$\n",
    "\n",
    "**If variance doesn't decrease with n**: Something is wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cc921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# Simulate variance at different sample sizes\n",
    "sample_sizes = [10, 50, 100, 500, 1000]\n",
    "variances = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    estimates = []\n",
    "    for _ in range(1000):\n",
    "        sample = np.random.normal(10, 3, n)\n",
    "        estimates.append(np.mean(sample))\n",
    "    variances.append(np.var(estimates))\n",
    "\n",
    "# Check if variance decreases as 1/n\n",
    "for n, var in zip(sample_sizes, variances):\n",
    "    print(f\"n={n:4d}: Var={var:.4f}, n√óVar={n*var:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbae2c",
   "metadata": {},
   "source": [
    "4. **Method 4: Relative to Other Estimators (Efficiency)**\n",
    "\n",
    "**Relative Efficiency**: Compare variance of two estimators for same parameter\n",
    "\n",
    "$$\\text{Efficiency of } \\hat{\\theta}_1 \\text{ relative to } \\hat{\\theta}_2 = Var(\\hat{\\theta}_2) / Var(\\hat{\\theta}_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# Compare mean vs median for normal data\n",
    "n_trials = 1000\n",
    "n = 50\n",
    "\n",
    "estimates_mean = []\n",
    "estimates_median = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    sample = np.random.normal(10, 3, n)\n",
    "    estimates_mean.append(np.mean(sample))\n",
    "    estimates_median.append(np.median(sample))\n",
    "\n",
    "var_mean = np.var(estimates_mean, ddof=1)\n",
    "var_median = np.var(estimates_median, ddof=1)\n",
    "\n",
    "efficiency = var_median / var_mean\n",
    "print(f\"Var(mean) = {var_mean:.4f}\")\n",
    "print(f\"Var(median) = {var_median:.4f}\")\n",
    "print(f\"Efficiency = {efficiency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec991f",
   "metadata": {},
   "source": [
    "In the example above, median ($\\hat{\\theta}_2$) has 57% MORE variance than mean. Hence, mean ($\\hat{\\theta}_1$) has \"lower\" variance (more efficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b721c9",
   "metadata": {},
   "source": [
    "5. **Method 5: Confidence Interval Width**\n",
    "\n",
    "**Practical perspective**: Is the uncertainty acceptable for your application?\n",
    "\n",
    "**95% Confidence Interval**: $\\hat{\\theta} \\pm 1.96 \\times \\sqrt{Var(\\hat{\\theta})}$\n",
    "\n",
    "$$Width = 2 \\times 1.96 \\times \\sqrt{Var(\\hat{\\theta})} \\approx 4 \\times SD(\\hat{\\theta})$$\n",
    "\n",
    "Note that depending on the application domain, the same value may be acceptable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72bbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# Estimating model accuracy\n",
    "mean_accuracy = 0.85\n",
    "std_accuracy = 0.05\n",
    "\n",
    "# 95% CI\n",
    "ci_lower = mean_accuracy - 1.96 * std_accuracy\n",
    "ci_upper = mean_accuracy + 1.96 * std_accuracy\n",
    "ci_width = ci_upper - ci_lower\n",
    "\n",
    "print(f\"Accuracy: {mean_accuracy:.2f} ¬± {1.96*std_accuracy:.2f}\")\n",
    "print(f\"95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"CI width: {ci_width:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb283dfe",
   "metadata": {},
   "source": [
    "In the example above, the width is 20% which is NOT acceptable for critical medical application (too wide) but can be acceptable for exploratory analysis.\n",
    "\n",
    "**Rules of thumb for CI width**:\n",
    "- Width < 0.1: Low variance (precise) \n",
    "- Width 0.1 - 0.3: Moderate variance (acceptable)\n",
    "- Width > 0.3: High variance (imprecise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9928a8",
   "metadata": {},
   "source": [
    "6. **Method 6: Domain Knowledge**\n",
    "\n",
    "**Context matters**: What's acceptable depends on your problem.\n",
    "\n",
    "||| Scenario 1 | Scenario 2|\n",
    "|--|--|--|---|\n",
    "|**ML Model Accuracy** | **Domain** | **Medical diagnosis** | **Movie recommendation**|\n",
    "||**Accuracy**| 0.90 ¬± 0.10 | 0.70 ¬± 0.10 | \n",
    "||**CI**|[0.80, 1.00]|[0.60, 0.80]|\n",
    "||**Assessment**|HIGH VARIANCE (unacceptable)|Moderate variance (acceptable)|\n",
    "||**Why?**|80% vs 90% is a huge difference in medicine| Small accuracy variations are OK here|\n",
    "|**Parameter Estimation**|**Domain**| **Bridge engineering** | **Marketing campaign reach** |\n",
    "||**Parameter**|Estimated load capacity|Estimated viewers|\n",
    "||**Value**|1000 tons ¬± 200 tons|1M ¬± 200K|\n",
    "||**CV**|20%|20%|\n",
    "||**Assessment**|HIGH VARIANCE (unacceptable)|Low variance (acceptable)|\n",
    "||**Why?**|Safety critical, need precision|Rough estimates are sufficient|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94982b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Key Insights: Variance Assessment: Decision Framework</h4>\n",
    "\n",
    "**Step 1**: Calculate $CV = SD(\\hat{\\theta}) / |E[\\hat{\\theta}]|$\n",
    "- CV < 10%: Low variance \n",
    "- CV 10-30%: Moderate variance\n",
    "- CV > 30%: High variance \n",
    "\n",
    "**Step 2**: Check MSE decomposition\n",
    "- If Variance >> Bias¬≤: Variance is the problem\n",
    "- If Bias¬≤ >> Variance: Bias is the problem\n",
    "\n",
    "**Step 3**: Compare to other estimators\n",
    "- Is there a lower-variance alternative?\n",
    "- What's the efficiency loss?\n",
    "\n",
    "**Step 4**: Consider practical implications\n",
    "- Is the CI width acceptable?\n",
    "- Does it meet your application requirements?\n",
    "\n",
    "**Step 5**: Use domain knowledge\n",
    "- What precision does your problem actually need?\n",
    "- What are the consequences of uncertainty?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bcce2",
   "metadata": {},
   "source": [
    "## Parameter Estimation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8bb3d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>Problem Statement: Parameter Estimation Task</h4>\n",
    "\n",
    "Our observed data: $X_1, X_2, ..., X_n$\n",
    "\n",
    "We believe they come from a distribution with parameter $\\theta$ (e.g., $N(Œ∏, 1)$)\n",
    "\n",
    "**Question**: Which value of $\\theta$ makes our observed data \"most likely\"?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9c585",
   "metadata": {},
   "source": [
    "## Method of Moments (MoM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce19ddb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Methods of Moments (MoM)</h4>\n",
    "\n",
    "**Match sample moments to population moments**, then solve for parameters.\n",
    "\n",
    "**Population moments**: $E[X]$, $E[X^2]$, $E[X^3]$, ... (depend on unknown parameters $\\theta$)\n",
    "\n",
    "**Sample moments**: $\\bar{X}$, $(1/n)\\sum_{i=1}^n X_i^2$, $(1/n)\\sum_i{i=1}^n X_i^3$, ... (computed from data)\n",
    "\n",
    "**Method**: Set sample moments equal to population moments, solve for $\\theta$.\n",
    "\n",
    "<h5> Algorithm</h5>\n",
    "\n",
    "For a distribution with $k$ parameters $\\theta = (\\theta_1, ..., \\theta_k)$:\n",
    "\n",
    "1. Write first $k$ population moments in terms of $\\theta$:\n",
    "   - $\\mu_1 = E[X] = g_1(\\theta)$\n",
    "   - $\\mu_2 = E[X^2] = g_2(\\theta)$\n",
    "   - ...\n",
    "   - $\\mu_k = E[X^k] = g_k(\\theta)$\n",
    "\n",
    "2. Compute corresponding sample moments:\n",
    "   - $m_1 = \\bar{X} = (1/n)\\sum_i^n X_i$\n",
    "   - $m_2 = (1/n)\\sum_{i=1}^n X_i^2$\n",
    "   - ...\n",
    "   - $m_k = (1/n)\\sum_{i=1}^n X_i^k$\n",
    "\n",
    "3. Solve the system of equations:\n",
    "   - $m_1 = g_1(\\hat{\\theta})$\n",
    "   - $m_2 = g_2(\\hat{\\theta})$\n",
    "   - ...\n",
    "   - $m_k = g_k(\\hat{\\theta})$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401f9e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Calculated Example: MoM for Normal Distribution, N(Œº, œÉ¬≤)</h3>\n",
    "\n",
    "Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ has two parameters $\\mu, \\sigma^2$ ‚Üí need two moments\n",
    "\n",
    "**Population moments:**\n",
    "- $E[X] = \\mu$\n",
    "- $E[X¬≤] = \\sigma^2 + \\mu^2$\n",
    "\n",
    "**Sample moments:**\n",
    "- $m_1 = \\bar{X}$\n",
    "- $m_2 = (1/n)\\sum_{i=1}^n X_i^2$\n",
    "\n",
    "**Method of Moments equations:**\n",
    "- $\\hat{\\mu} = \\bar{X}$\n",
    "- $\\hat{\\sigma}^2 + \\hat{\\mu}^2 = (1/n)\\sum_{i=1}^n X_i^2$\n",
    "\n",
    "**Solving:**\n",
    "- $\\hat{\\mu}_{MoM} = \\bar{X}$\n",
    "- $\\hat{\\sigma}^2_{MoM} = (1/n)\\sum_{i=1}^n X_i^2 - \\bar{X}^2 = (1/n)\\sum_{i=1}^n(X_i - \\bar{X})^2$\n",
    "\n",
    "**Note**: This gives the *biased* variance estimator (divides by $n$, not $n-1$)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac067cc5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-exercise\">\n",
    "<h4>Calculated Example: MoM for Exponential Distribution, Exp(Œª)</h4>\n",
    "\n",
    "One parameter ($\\lambda$) ‚Üí need one moment\n",
    "\n",
    "**Population moment:**\n",
    "- $E[X] = 1/\\lambda$\n",
    "\n",
    "**Sample moment:**\n",
    "- $m_1 = \\bar{X}$\n",
    "\n",
    "**Method of Moments equation:**\n",
    "- $1/\\hat{\\lambda} = \\bar{X}$\n",
    "\n",
    "**Solving:**\n",
    "- $\\hat{\\lambda}_{MoM} = 1/\\bar{X}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a0a3",
   "metadata": {},
   "source": [
    "When to Use Method of Moments\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to compute (just solve equations)\n",
    "- No optimization needed\n",
    "- Good starting values for numerical MLE\n",
    "- Works when MLE is intractable\n",
    "\n",
    "**Disadvantages:**\n",
    "- Less efficient than MLE\n",
    "- May give biased estimates\n",
    "- Ignores likelihood structure\n",
    "- Can give invalid estimates (e.g., negative variance)\n",
    "\n",
    "**Practical use:**\n",
    "- Quick initial estimates\n",
    "- Starting point for iterative MLE\n",
    "- When MLE is computationally expensive\n",
    "- Sanity check for MLE results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75697b89",
   "metadata": {},
   "source": [
    "Let $X_1, X_2, ..., X_n$ be independent realisations of r.v. $X$ with $\\mathbb{E}X = \\mu$ and $Var(X) = \\sigma^2$. \n",
    "\n",
    "We define the moment estimators as follows: \n",
    "$$\\begin{array}{l}\\hat{\\mu} = \\bar{X}\\\\\\hat{\\sigma}^2 = s^2\\end{array}$$\n",
    "\n",
    "> Is there quality satisfactory?</br>\n",
    "\n",
    "**Part I: Quality of the estimator** $\\hat{\\mu} = \\bar{X}$\n",
    "\n",
    "1. Let's calculate bias of the estimator $\\hat{\\mu} = \\bar{X}$\n",
    "\n",
    "$$Bias(\\hat{\\theta},\\theta) = \\mathbb{E}[\\hat{\\theta}] ‚àí \\theta = Bias(\\hat{\\mu}, \\mu) = \\mathbb{E}[\\hat{\\mu}] ‚àí \\mu = $$\n",
    "\n",
    "$$= \\mathbb{E}[\\bar{X}] ‚àí \\mu = \\mathbb{E}\\bigg[1/n \\sum_{ùëñ=1}^n X_i\\bigg] ‚àí \\mu = [\\text{by propr. of  E,} E[aX + b] = ùëéE[X] + b] = 1/n \\mathbb{E}[\\sum_{i=1}^n X_i] ‚àí \\mu $$\n",
    "\n",
    "$$= [\\text{by propr. of E,} E[X + Y] = E[X] + E[Y]] = 1/n \\sum_{i=1}^n E[X_i] ‚àí \\mu = 1/n n \\mu ‚àí \\mu = \\mathbf{0}$$\n",
    "\n",
    "So, the estimator $\\hat{\\mu} = \\bar{X}$ is **unbiased**.\n",
    "\n",
    "2. Let's calculate the variance of the estimator $\\hat{\\mu} = \\bar{X}$\n",
    "\n",
    "$$Var(\\hat{\\mu}) = Var(\\bar{X}) = Var\\bigg(1/n \\sum_{i=1}^n X_i\\bigg) = [\\text{by propr. of Var, } Var(aX + b) = a^2Var(X)] = \\frac{1}{n^2}Var\\bigg(\\sum_{i=1}^n X_i\\bigg) = $$\n",
    "$$= [\\text{by propr. of Var of indep. X and Y, } Var(X + Y) = Var(X) + Var(Y)] = \\frac{1}{n^2}\\sum_{i=1}^n Var(X_i) = \\frac{1}{n^2}n\\sigma^2 = \\frac{\\sigma^2}{n} \\xrightarrow[n\\rightarrow \\infty]{} 0$$\n",
    "\n",
    "3. Calculate MSE of the estimator $\\hat{\\mu} = \\bar{X}$\n",
    "\n",
    "$$MSE(\\hat{\\mu}, \\mu) = Bias(\\hat{\\mu}, \\mu)^2 + Var(\\hat{\\mu}) = \\frac{\\sigma^2}{n} \\xrightarrow[n\\rightarrow \\infty]{} 0$$\n",
    "\n",
    "So, it is a convergent estimator in the mean square sense.\n",
    "\n",
    "**Part II: Quality of the estimator** $\\hat{\\sigma}^2 = s^2$\n",
    "\n",
    "1. Let's calculate bias of the estimator $\\hat{\\sigma}^2 = s^2$\n",
    "\n",
    "$$Bias(\\hat{\\theta},\\theta) = \\mathbb{E}[\\hat{\\theta}] ‚àí \\theta = Bias(\\hat{\\sigma}^2, \\sigma^2) = \\mathbb{E}[\\hat{\\sigma}^2] ‚àí \\sigma^2 = \\mathbb{E}[s^2] ‚àí \\sigma^2 =$$\n",
    "$$= \\mathbb{E}\\bigg[\\frac{1}{n}\\sum_{i=1}^n(X_i^2 - \\bar{X})\\bigg] - \\sigma^2 = \\bigg[\\text{by propr. of E, } E[X + Y] = EX + EY\\bigg] = \\mathbb{E}\\bigg[\\frac{1}{n}\\sum_{i=1}^nX_i^2\\bigg] - \\mathbb{E}[\\bar{X}^2] - \\sigma^2 = $$\n",
    "\n",
    "$$\\bigg[\\text{by propr. of E, } E[aX + b] = aEX + b\\bigg] = \\frac{1}{n}\\mathbb{E}\\bigg[\\sum_{i=1}^nX_i^2\\bigg] - \\mathbb{E}[\\bar{X}^2] - \\sigma^2 =$$\n",
    "\n",
    "$$\\bigg[\\text{by propr. of E, } E[X + Y] = EX + EY\\bigg] = \\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}[X_i^2] - \\mathbb{E}[\\bar{X}^2] - \\sigma^2 = \\frac{1}{n}n\\mathbb{E}[X^2] - \\mathbb{\\hat{\\mu}^2} - \\sigma^2 =$$\n",
    "\n",
    "$$= \\mathbb{E}[X^2] - \\mathbb{\\hat{\\mu}^2} - \\sigma^2 = \\bigg[\\text{by def. } Var(X) = E[X^2] - (EX)^2 \\Rightarrow E[X^2] = Var(X) + (EX)^2\\bigg] = $$\n",
    "\n",
    "$$= (Var(X) + (EX)^2) - (Var(\\hat{\\mu}) + (\\mathbb{E}[\\hat{\\mu}])^2) - \\sigma^2 = (\\sigma^2 + \\mu^2) - \\bigg(\\frac{\\sigma^2}{n} + \\mu^2\\bigg) - \\sigma^2 =$$\n",
    "\n",
    "$$= \\bigg(1 - \\frac{1}{n}\\bigg)\\sigma^2 - \\sigma^2 = -\\frac{\\sigma^2}{n} \\mathbf{\\neq 0}$$\n",
    "\n",
    "So, the estimator $\\hat{\\sigma}^2 = s^2$ is **biased**.\n",
    "\n",
    "To make it *unbiased*, we can apply so called **Bessel's correction**:\n",
    "\n",
    "$$\\hat{\\sigma}'^2 = s'^2 = \\frac{n}{n-1}s^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6443a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Bessel's Correction</h4>\n",
    "\n",
    "Unbiased estimator of variance is given by:\n",
    "\n",
    "$$\\hat{\\sigma}'^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X})^2$$\n",
    "\n",
    "*Note:* we divide by $(n-1)$ instead of $n$ here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173a4d4",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92707b3c",
   "metadata": {},
   "source": [
    "Imagine you flip a coin 10 times and get HTHHHTHHHH:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/coins.svg\" width=\"800px\" alt=\"10 coins: HTHHHTHHHH\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b089099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plot_heads_tails()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92674e",
   "metadata": {},
   "source": [
    "Our goal is to find the optimal way to fit a distribution to the data in order to facilitate the work and generalise the observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac9390",
   "metadata": {},
   "source": [
    "Here, we are dealing with a sequence of Bernoulli trials with parameter $p$ of having a head in a single coin flip. As we observed HTHHHTHHHH, we can calculate the probability to get this exact sequence with a given parameter $p$ as the joint probability:\n",
    "\n",
    "$$P(HTHHHTHHHH|p) = p\\times (1-p)\\times p\\times p\\times p\\times p\\times (1-p) \\times p\\times p\\times p\\times p = p^7\\times (1-p)^3$$\n",
    "\n",
    "In a more general way, we think the data follows Binomial distribution with parameter $p$ of having a head in a single coin flip. The probability mass function is given by $P(X=k | n, p) = \\begin{pmatrix}n \\\\ k\\end{pmatrix}p^k q^{n-k} = \\frac{n!}{k!(n-k)!}p^k q^{n-k}$ and for $n=10$ it looks something like that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise binomial distribution mass function for n=10\n",
    "plot_binomial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c6c17",
   "metadata": {},
   "source": [
    "For instance, the probability that we get 8 heads out of 10 flips of a fair coin is:\n",
    "$$P(X=8 | n=10, p=0.5) = \\frac{10!}{8!(10-8)!}0.5^8 0.5^{10-8} = \\frac{10!}{8!2!}0.5^8 0.5^{2} = \\frac{9\\times 10}{2}\\frac{1}{2^{10}} = 9 \\times 5 \\times \\frac{1}{2^{10}} \\approx 0.044$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71114",
   "metadata": {},
   "source": [
    "So our question is then:\n",
    "\n",
    "> Which value of $p$ makes our observed data most probable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db478ca0",
   "metadata": {},
   "source": [
    "If we want to calculate the likelihood of $p = 0.5$, then we need to rearrange our equation by **modifying only the left side**:\n",
    "\n",
    "$$L(p=0.5 | n=10, X=8) = \\frac{10!}{8!(10-8)!}0.5^8 0.5^{10-8} = \\frac{10!}{8!2!}0.5^8 0.5^{2} = \\frac{9\\times 10}{2}\\frac{1}{2^{10}} = 9 \\times 5 \\times \\frac{1}{2^{10}} \\approx 0.044$$\n",
    "\n",
    "The left side of the equation reads \"*the likelihood of $p$ (the probability to get a head), given $n$, the number of flips we make, and $X$, the number of heads*\".\n",
    "\n",
    "**Note** that we can modify the values of $p$ in this equation, but the observed data ($n=10$ and $X=8$) remains fixed:\n",
    "\n",
    "$$L(p=0.3 | n=10, X=8) = \\frac{10!}{8!(10-8)!}0.3^8 0.7^{10-8} = \\frac{10!}{8!2!}0.3^8 0.7^{2} = \\frac{9\\times 10}{2}\\times 0.00007\\times 0.49 = 9 \\times 5 \\times 0.00007\\times 0.49 \\approx 0.00145$$\n",
    "\n",
    "$$L(p=0.8 | n=10, X=8) = \\frac{10!}{8!(10-8)!}0.8^8 0.2^{10-8} = \\frac{10!}{8!2!}0.8^8 0.2^{2} \\approx 0.302$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d15760",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrate_likelihood_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c7244",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>Reminder: Joint Probability of Independent R.V.</h4>\n",
    "\n",
    "> How likely is all the data together?</br>\n",
    "\n",
    "We need to use **joint probability**.\n",
    "\n",
    "Let $X_1, X_2, ..., X_n$ be i.i.d.\n",
    "\n",
    "Recall that two events $A$ and $B$ are *independent* if:\n",
    "\n",
    "$$P(A\\cap B) = P(A)\\times P(B)$$\n",
    "\n",
    "For our data, independence means:\n",
    "\n",
    "- Observing $X_1$‚Äã doesn't affect $X_2$\n",
    "- Each data point is drawn separately from the same distribution\n",
    "- Knowing one observation tells us nothing about another\n",
    "\n",
    "For *discrete variables*, the joint probability of independent r.v. is expressed as a product of individual probabilities:\n",
    "$$P(X_1‚Äã=x_1‚Äã, X_2‚Äã=x_2‚Äã, ‚Ä¶, X_n‚Äã=x_n‚Äã‚à£\\theta) = \\prod_{i=1}^n ‚ÄãP(X_i‚Äã=x_i‚Äã‚à£\\theta)$$\n",
    "\n",
    "For *continuous variables*, the joint probability of independent r.v. is expressed as a product of probability density functions (PDFs):\n",
    "$$f(x_1, x_2, \\ldots, x_n | \\theta) = \\prod_{i=1}^{n} f(x_i | \\theta)$$\n",
    "\n",
    "*Example*: Suppose you flip a fair coin 3 times and get: H, H, T with $P(H)=0.5$ for each flip. Flips are independent (one doesn't affect another).\n",
    "\n",
    "Joint probability:\n",
    "$P(H,¬†H,¬†T) = P(H)\\times P(H)\\times P(T) = 0.5\\times 0.5 \\times 0.5 = 0.125$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf5efd",
   "metadata": {},
   "source": [
    "**What if Data Aren't Independent?**\n",
    "If observations are not independent (e.g., time series, spatial data, grouped data):\n",
    "\n",
    "- We cannot write likelihood as a simple product\n",
    "- Need more complex joint distributions\n",
    "- This is why independence assumption is so important!\n",
    "\n",
    "*Example*: Time series\n",
    "\n",
    "- Today's stock price depends on yesterday's\n",
    "- Need to model $P(X_t | X_{t-1}, X_{t-2}, \\ldots)$\n",
    "- Likelihood is more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a9271",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Maximum Likelihood Estimation</h4>\n",
    "\n",
    "Setup:\n",
    "\n",
    "- Data: $X_1, X_2, ..., X_n \\sim f(x | \\theta)$ (i.i.d. from distribution with parameter $\\theta$)\n",
    "- Goal: Estimate $\\theta$\n",
    "\n",
    "**Likelihood Function**, \"*probability of observing the data, as a function of $\\theta$*\": \n",
    "$$L(\\theta | X) = \\prod_{i=1}^{n} f(X_i | \\theta)$$\n",
    "\n",
    "**Log-Likelihood:**\n",
    "$$\\ell(\\theta | X) = \\log L(\\theta | X) = \\sum_{i=1}^{n} \\log f(X_i | \\theta)$$\n",
    "(Taking log: easier to work with, doesn't change location of maximum)\n",
    "\n",
    "**Maximum Likelihood Estimator:**\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L(\\theta | X) = \\arg\\max_{\\theta} \\ell(\\theta | X)$$\n",
    "\n",
    "How to find it:\n",
    "\n",
    "1. Write down likelihood (or log-likelihood)\n",
    "2. Take derivative with respect to $\\theta$\n",
    "3. Set equal to zero and solve\n",
    "4. Verify it's a maximum (second derivative test)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862c271",
   "metadata": {},
   "source": [
    "### MLE for Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d550fe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Worked Example: MLE for Bernoulli Distribution</h4>\n",
    "\n",
    "**Scenario: Email Spam Rate Estimation**\n",
    "\n",
    "You're building a spam classifier. From your training data, you observe:\n",
    "\n",
    "- $n = 1000$ emails\n",
    "- $k = 230$ are spam\n",
    "\n",
    "**Question**: Estimate $p$ = probability that a random email is spam using MLE\n",
    "\n",
    "Model: Each email is spam with probability $p$ (Bernoulli distribution)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed data\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "k = 230\n",
    "observed_data = np.array([1]*k + [0]*(n-k))  # 1=spam, 0=ham\n",
    "np.random.shuffle(observed_data)\n",
    "\n",
    "print(f\"Data (10 first observations): {observed_data[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af030a",
   "metadata": {},
   "source": [
    "1. **Step 1: Write the Likelihood Function**\n",
    "\n",
    "Each email $X_i \\sim Bernoulli(p)$. Therefore, $\\begin{array}{ll} P(X_i = 1) = p   & \\text{(spam)} \\\\ P(X_i = 0) = 1-p & \\text{(not spam)}\\end{array}$\n",
    "\n",
    "$$L(p|X) = \\prod_i^n P(X_i| p) = \\prod_i^n p^{X_i} \\times (1-p)^{(1-X_i)} = [k = \\text{\\# of successes}, n = \\text{\\# trials}] = p^k\\times (1-p)^{(n-k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our data\n",
    "print(\"L(p) = ‚àè·µ¢ P(X·µ¢ | p)\")\n",
    "print(\"     = ‚àè·µ¢ p^X·µ¢ (1-p)^(1-X·µ¢)\")\n",
    "print(f\"     = p^{k} (1-p)^{n-k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b5ef7",
   "metadata": {},
   "source": [
    "2. **Step 2: Write the Log-Likelihood Function**\n",
    "\n",
    "$$l(p|X) = \\log L(p|X) = \\log (p^k\\times (1-p)^{n-k}) = \\log (p^k) + \\log (1-p)^{n-k} = k \\log (p) + (n-k) \\log(1-p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a128ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our data\n",
    "print(\"‚Ñì(p) = log L(p)\")\n",
    "print(f\"     = {k} log(p) + {n-k} log(1-p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb8b1e",
   "metadata": {},
   "source": [
    "3. **Step 3: Take the Derivative**\n",
    "\n",
    "$$\\frac{dl}{dp} = \\frac{d}{dp}(k\\log(p) + (n-k)\\log(1-p)) = k\\times\\frac{1}{p} - (n-k)\\frac{1}{1-p} = \\frac{k}{p} - \\frac{n-k}{1-p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdae545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our data\n",
    "print(f\"d‚Ñì/dp = {k}/p - {n-k}/(1-p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc42eb",
   "metadata": {},
   "source": [
    "4. **Step 4: Set Derivative to 0 and Solve**\n",
    "\n",
    "$$\\frac{dl}{dp} = 0$$\n",
    "\n",
    "$$\\frac{k}{p} - \\frac{n-k}{1-p} = 0$$\n",
    "\n",
    "$$\\frac{k}{p} = \\frac{n-k}{1-p}$$\n",
    "\n",
    "$$k\\times (1-p) = (n-k)p$$\n",
    "\n",
    "$$k - kp = np - kp$$\n",
    "\n",
    "$$k = np + kp - kp$$\n",
    "\n",
    "$$k = np$$\n",
    "\n",
    "$$p = \\frac{k}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21389832",
   "metadata": {},
   "source": [
    "**Step 5: Verify It's a Maximum**\n",
    "\n",
    "Check second derivative:\n",
    "\n",
    "$$\\frac{d^2\\mathcal{l}}{dp^2} = \\frac{d}{dp}\\bigg(\\frac{k}{p} - \\frac{n-k}{1-p}\\bigg) = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}$$\n",
    "\n",
    "At $\\hat{p} = \\frac{k}{n}$, $$\\frac{d^2\\mathcal{l}}{dp^2}\\bigg|_{(p = k/n)} = - \\frac{k}{(k/n)^2} - \\frac{n-k}{(1-k/n)^2} =$$\n",
    "$$= - \\frac{kn^2}{k^2} - \\frac{(n-k)n^2}{(n-k)^2} = - \\frac{n^2}{k} - \\frac{n^2}{(n-k)} = - n^2\\bigg(\\frac{1}{k} + \\frac{1}{n-k}\\bigg) < 0$$\n",
    "\n",
    "Second derivative is NEGATIVE ‚Üí this is a MAXIMUM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mle = k / n\n",
    "# Calculate each term\n",
    "term1 = -k / (p_mle**2)\n",
    "term2 = -(n-k) / ((1-p_mle)**2)\n",
    "second_deriv = term1 + term2\n",
    "print(f\"d¬≤‚Ñì/dp¬≤|_(p={p_mle}) = {term1:.4f} + {term2:.4f} = {second_deriv:.4f}\")\n",
    "alt_form = -n**2 * (1/k + 1/(n-k))\n",
    "print(f\"\")\n",
    "print(f\"d¬≤‚Ñì/dp¬≤|_(p={k}/{n}) = -{n}¬≤ √ó [1/{k} + 1/{(n-k)}]\")\n",
    "print(f\"  = -{n**2} √ó [{1/k:.4f} + {1/(n-k):.4f}]\")\n",
    "print(f\"  = {alt_form:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08eb53",
   "metadata": {},
   "source": [
    "6. **Step 6: Compute the MLE**\n",
    "\n",
    "Intuitive result: MLE = observed proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5497ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mle = k / n\n",
    "print(f\"MLE = {p_mle:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tails = n - k\n",
    "n_heads = k\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "p_range = np.linspace(0.001, 0.999, 500)\n",
    "\n",
    "# Plot 1: Likelihood\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "likelihood = p_range**n_heads * (1-p_range)**n_tails\n",
    "ax1.plot(p_range, likelihood, linewidth=3, color='steelblue')\n",
    "ax1.axvline(p_mle, color='red', linewidth=2, linestyle='--',\n",
    "           label=f'MLE = {p_mle:.2f}')\n",
    "ax1.scatter(p_mle, p_mle**n_heads * (1-p_mle)**n_tails, s=300,\n",
    "           color='red', marker='*', edgecolors='darkred', linewidths=2, zorder=5)\n",
    "ax1.set_xlabel('p', fontsize=12)\n",
    "ax1.set_ylabel('L(p)', fontsize=12)\n",
    "ax1.set_title('Likelihood Function\\n(concave down ‚Üí maximum)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Log-likelihood\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "log_likelihood = n_heads * np.log(p_range) + n_tails * np.log(1-p_range)\n",
    "ax2.plot(p_range, log_likelihood, linewidth=3, color='green')\n",
    "ax2.axvline(p_mle, color='red', linewidth=2, linestyle='--',\n",
    "           label=f'MLE = {p_mle:.2f}')\n",
    "ax2.scatter(p_mle, n_heads * np.log(p_mle) + n_tails * np.log(1-p_mle),\n",
    "           s=300, color='red', marker='*', edgecolors='darkred', linewidths=2, zorder=5)\n",
    "ax2.set_xlabel('p', fontsize=12)\n",
    "ax2.set_ylabel('‚Ñì(p)', fontsize=12)\n",
    "ax2.set_title('Log-Likelihood\\n(easier to work with)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: First derivative (score)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "first_deriv = n_heads / p_range - n_tails / (1 - p_range)\n",
    "ax3.plot(p_range, first_deriv, linewidth=3, color='purple')\n",
    "ax3.axhline(0, color='black', linewidth=1, linestyle='-', alpha=0.5)\n",
    "ax3.axvline(p_mle, color='red', linewidth=2, linestyle='--',\n",
    "           label=f'Zero at p={p_mle:.2f}')\n",
    "ax3.scatter(p_mle, 0, s=300, color='red', marker='o',\n",
    "           edgecolors='darkred', linewidths=2, zorder=5)\n",
    "ax3.set_xlabel('p', fontsize=12)\n",
    "ax3.set_ylabel(\"d‚Ñì/dp (score)\", fontsize=12)\n",
    "ax3.set_title('First Derivative\\n(zero at maximum)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(-50, 50)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7aebe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application: Logistic Regression Loss Function</h4>\n",
    "\n",
    "**Connection:** The MLE for Bernoulli is exactly what logistic regression does.\n",
    "\n",
    "Logistic regression loss (binary cross-entropy):\n",
    "$$\\mathcal{L} = -\\sum_{i=1}^{n} [y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i)]$$\n",
    "\n",
    "This is the **negative log-likelihood** for Bernoulli data.\n",
    "\n",
    "**Training a logistic regression = Finding MLE of the parameters**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79d714",
   "metadata": {},
   "source": [
    "### MLE for Exponential Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f412da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-example\">\n",
    "<h4>Worked Example: MLE for Exponential Distribution</h4>\n",
    "\n",
    "**Scenario: Server Response Times**\n",
    "\n",
    "You're analyzing server response times (in seconds). Theory suggests response times follow an Exponential distribution.\n",
    "\n",
    "Recall, $Exponential(\\lambda)$ has:\n",
    "- PDF: $f(x|\\lambda) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$\n",
    "- Mean: $1/\\lambda$\n",
    "- Interpretation: $\\lambda$ = rate parameter (events per unit time)\n",
    "\n",
    "Observed data: response times for 20 requests\n",
    "\n",
    "**Question:** find $\\hat{\\lambda}$ using MLE.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "true_lambda = 0.5  # True rate (unknown in practice)\n",
    "n = 20\n",
    "data = np.random.exponential(1/true_lambda, n)\n",
    "\n",
    "print(f\"Data (first 10 out of {n} observations): {data[:10]}\")\n",
    "# Summary statistics\n",
    "print(f\"Sample mean: {np.mean(data):.3f} seconds\")\n",
    "print(f\"Sample min:  {np.min(data):.3f} seconds\")\n",
    "print(f\"Sample max:  {np.max(data):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ded9f4",
   "metadata": {},
   "source": [
    "1. **Step 1: Write the Likelihood Function**\n",
    "\n",
    "For i.i.d. data $X_1, X_2, ..., X_n \\sim Exponential(\\lambda)$:\n",
    "\n",
    "$$L(\\lambda | X) = \\prod_{i=1}^n f(X_i | \\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda X_i} = \\lambda^n \\prod_{i=1}^n e^{-\\lambda X_i} = [\\text{by prop. of exp. func., } e^a\\times e^b = e^{a+b}] = \\lambda^n e^{-\\sum_{i=1}^n \\lambda X_i} = \\lambda^n e^{- \\lambda \\sum_{i=1}^n X_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a647ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood for our data\n",
    "sum_X_i = np.sum(data)\n",
    "print(f\"‚àëX·µ¢ = {sum_X_i:.3f}\")\n",
    "print(f\"L(Œª) = Œª^{n} √ó exp(-Œª √ó {np.sum(data):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42201a28",
   "metadata": {},
   "source": [
    "2. **Step 2: Take the Log-Likelihood**\n",
    "\n",
    "$$\\mathcal{l}(\\lambda|X) = \\log L(\\lambda|X) = \\log (\\lambda^n e^{- \\lambda \\sum_{i=1}^n X_i}) = [\\text{by propr. of log}] = \\log(\\lambda^n) + \\log(e^{- \\lambda \\sum_{i=1}^n X_i}) =$$\n",
    "\n",
    "$$= n\\log(\\lambda) + (- \\lambda \\sum_{i=1}^n X_i) = n\\log(\\lambda) - \\lambda \\sum_{i=1}^n X_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our data\n",
    "print(f\"‚Ñì(Œª) = {n} log(Œª) - Œª √ó {np.sum(data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f8bb1",
   "metadata": {},
   "source": [
    "3. **Step 3: Take the Derivative**\n",
    "\n",
    "$$\\frac{d\\mathcal{l}}{d\\lambda} = \\frac{d}{d\\lambda}\\bigg(n\\log(\\lambda) - \\lambda \\sum_{i=1}^n X_i\\bigg) = n\\frac{1}{\\lambda} - \\sum_{i=1}^n X_i = \\frac{n}{\\lambda} - \\sum_{i=1}^n X_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our data\n",
    "print(f\"d‚Ñì/dŒª = {n}/Œª - {np.sum(data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf792bb",
   "metadata": {},
   "source": [
    "4. **Step 4: Set Derivative to 0 and Solve**\n",
    "\n",
    "$$\\frac{d\\mathcal{l}}{d\\lambda} = 0$$\n",
    "\n",
    "$$\\frac{n}{\\lambda} - \\sum_{i=1}^n X_i = 0$$\n",
    "\n",
    "$$\\frac{n}{\\lambda} = \\sum_{i=1}^n X_i$$\n",
    "\n",
    "$$\\lambda = \\frac{n}{\\sum_{i=1}^n X_i}$$\n",
    "\n",
    "Note that $\\bar{X} = 1/n\\sum_{i=1}^n X_i$, so:\n",
    "\n",
    "$$\\hat{\\lambda}_{MLE} = \\frac{1}{\\bar{X}}$$\n",
    "\n",
    "So, MLE for Exponential rate = 1 / sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a10de",
   "metadata": {},
   "source": [
    "5. **Step 5: Verify It's a Maximum**\n",
    "\n",
    "Check second derivative:\n",
    "\n",
    "$$\\frac{d^2\\mathcal{l}}{d\\lambda^2} = \\frac{d}{d\\lambda}\\bigg(\\frac{n}{\\lambda} - \\sum_{i=1}^n X_i\\bigg) = -\\frac{n}{\\lambda^2}$$\n",
    "\n",
    "Since $n > 0$ and $\\lambda^2 > 0$: $\\frac{d^2\\mathcal{l}}{d\\lambda^2} = -\\frac{n}{\\lambda^2} < 0$\n",
    "\n",
    "Second derivative is NEGATIVE ‚Üí this is a MAXIMUM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d3fec",
   "metadata": {},
   "source": [
    "6. **Step 6: Compute the MLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MLE for our data\n",
    "sample_mean = np.mean(data)\n",
    "mle_lambda = 1 / sample_mean\n",
    "print(f\"Sample mean: XÃÑ = {sample_mean:.4f}\")\n",
    "print(f\"ŒªÃÇ_MLE = 1 / XÃÑ = 1 / {sample_mean:.4f} = {mle_lambda:.4f}\")\n",
    "\n",
    "print()\n",
    "print(f\"True Œª (unknown in practice): {true_lambda:.4f}\")\n",
    "print(f\"Error: {abs(mle_lambda - true_lambda):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6181029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(16, 5))\n",
    "    \n",
    "# Plot 1: Data histogram with fitted distribution\n",
    "ax = axes[0]\n",
    "    \n",
    "ax.hist(data, bins=15, density=True, alpha=0.7, color='steelblue',\n",
    "           edgecolor='black', label='Observed data')\n",
    "    \n",
    "x_range = np.linspace(0, np.max(data)*1.2, 200)\n",
    "    \n",
    "# True distribution\n",
    "ax.plot(x_range, true_lambda * np.exp(-true_lambda * x_range),\n",
    "           'g-', linewidth=3, label=f'True: Exp(Œª={true_lambda})')\n",
    "    \n",
    "# MLE distribution\n",
    "ax.plot(x_range, mle_lambda * np.exp(-mle_lambda * x_range),\n",
    "           'r--', linewidth=3, label=f'MLE: Exp(ŒªÃÇ={mle_lambda:.3f})')\n",
    "    \n",
    "ax.set_xlabel('Response Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Data vs Fitted Exponential Distribution', \n",
    "                fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "    \n",
    "# Plot 2: Log-likelihood function\n",
    "ax = axes[1]\n",
    "    \n",
    "lambda_range = np.linspace(0.1, 1.5, 200)\n",
    "log_likelihoods = []\n",
    "    \n",
    "for lam in lambda_range:\n",
    "    ll = n * np.log(lam) - lam * np.sum(data)\n",
    "    log_likelihoods.append(ll)\n",
    "    \n",
    "log_likelihoods = np.array(log_likelihoods)\n",
    "    \n",
    "ax.plot(lambda_range, log_likelihoods, linewidth=3, color='steelblue')\n",
    "ax.axvline(mle_lambda, color='red', linewidth=3, linestyle='--',\n",
    "              label=f'MLE: ŒªÃÇ={mle_lambda:.3f}')\n",
    "ax.scatter(mle_lambda, n * np.log(mle_lambda) - mle_lambda * np.sum(data),\n",
    "              s=400, color='red', marker='*', edgecolors='darkred',\n",
    "              linewidths=2, zorder=5)\n",
    "    \n",
    "ax.axvline(true_lambda, color='gold', linewidth=2, linestyle=':',\n",
    "              label=f'True: Œª={true_lambda}', alpha=0.7)\n",
    "    \n",
    "ax.set_xlabel('Œª (rate parameter)', fontsize=12)\n",
    "ax.set_ylabel('Log-Likelihood ‚Ñì(Œª)', fontsize=12)\n",
    "ax.set_title('Log-Likelihood Function', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "    \n",
    "# Plot 3: Derivative (score function)\n",
    "ax = axes[2]\n",
    "    \n",
    "derivatives = n / lambda_range - np.sum(data)\n",
    "    \n",
    "ax.plot(lambda_range, derivatives, linewidth=3, color='green',\n",
    "           label='Score function: d‚Ñì/dŒª')\n",
    "ax.axhline(0, color='black', linewidth=1, linestyle='-', alpha=0.5)\n",
    "ax.axvline(mle_lambda, color='red', linewidth=3, linestyle='--',\n",
    "              label=f'MLE (d‚Ñì/dŒª=0): ŒªÃÇ={mle_lambda:.3f}')\n",
    "ax.scatter(mle_lambda, 0, s=400, color='red', marker='o',\n",
    "              edgecolors='darkred', linewidths=2, zorder=5)\n",
    "    \n",
    "ax.set_xlabel('Œª (rate parameter)', fontsize=12)\n",
    "ax.set_ylabel('d‚Ñì/dŒª', fontsize=12)\n",
    "ax.set_title('First Derivative (Score Function)\\n= 0 at MLE', \n",
    "                fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-20, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54492418",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4> üí° Key Insights: MLE for Exponential Distribution</h4>\n",
    "    \n",
    "1. For Exponential: MLE = 1 / sample mean \n",
    "    \n",
    "2. $\\lambda$ is RATE (events per time) and $1/\\lambda$ is MEAN TIME between events\n",
    "    \n",
    "3. If $\\bar{X} = 2$ seconds ‚Üí $\\hat{\\lambda} = 0.5$ per second (one event every 2 seconds on average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d242a7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4> ü§ñ ML APPLICATION: Time-to-Event Modeling </h4>\n",
    "    \n",
    "Exponential distribution is used in ML for:\n",
    "1. Waiting times:\n",
    "- Time between user clicks\n",
    "- Server request intervals\n",
    "- Time to next purchase\n",
    "\n",
    "2. Survival Analysis:\n",
    "- Customer churn modeling\n",
    "- Time to equipment failure\n",
    "- Session duration\n",
    "\n",
    "3. Deep Learning:\n",
    "- Dropout regularization (exponential draws)\n",
    "- Exponential learning rate decay\n",
    "\n",
    "*Example*: If $\\hat{\\lambda} = 0.5$ requests/second:\n",
    "- Average wait: 1/0.5 = 2 seconds\n",
    "- $P(wait > 5 sec) = exp(-0.5 √ó 5) \\approx 8.2\\%$\n",
    "- Useful for capacity planning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b8754",
   "metadata": {},
   "source": [
    "### MLE for Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4952c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-exercise\">\n",
    "<h4>Worked Example: MLE for Normal Distribution</h4>\n",
    "\n",
    "Find the MLE for both parameters of Normal distribution $N(\\mu, \\sigma^2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "n = 100\n",
    "data = np.random.normal(true_mu, true_sigma, n)\n",
    "\n",
    "print(f\"Data (10 first observations): {data[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e46ac",
   "metadata": {},
   "source": [
    "1. **Step 1: Write the Likelihood Function**\n",
    "\n",
    "$$L(\\mu, \\sigma^2|X) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp(-(X_i-\\mu)^2/(2\\sigma^2)) = (\\frac{1}{\\sqrt{2\\pi}})^n\\prod_{i=1}^n \\frac{1}{\\sqrt{\\sigma^2}} exp(-(X_i-\\mu)^2/(2\\sigma^2)) =$$\n",
    "$$= (2\\pi)^{-n/2}\\prod_{i=1}^n \\frac{1}{\\sqrt{\\sigma^2}} exp(-(X_i-\\mu)^2/(2\\sigma^2)) = (2\\pi)^{-n/2} (\\frac{1}{\\sqrt{\\sigma^2}})^n\\prod_{i=1}^n  exp(-(X_i-\\mu)^2/(2\\sigma^2)) ==$$\n",
    "$$=(2\\pi)^{-n/2} (\\sigma^2)^{-n/2}\\prod_{i=1}^n  exp(-(X_i-\\mu)^2/(2\\sigma^2))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0cde6",
   "metadata": {},
   "source": [
    "2. **Step 2: Write the Log-Likelihood**\n",
    "\n",
    "$$\\ell(\\mu, \\sigma^2|X) = \\log L(\\mu, \\sigma^2|X) = \\log \\bigg((2\\pi)^{-n/2} (\\sigma^2)^{-n/2}\\prod_{i=1}^n  exp(-(X_i-\\mu)^2/(2\\sigma^2))\\bigg) =$$\n",
    "$$= \\log((2\\pi)^{-n/2}) + \\log((\\sigma^2)^{-n/2}) + \\log\\bigg(\\prod_{i=1}^n  exp(-(X_i-\\mu)^2/(2\\sigma^2))\\bigg) = -n/2 \\log(2\\pi) - n/2 \\log(\\sigma^2) + \\log(exp(-\\sum_{i=1}^n((X_i-\\mu)^2/(2\\sigma^2)))) =$$\n",
    "$$= -n/2 \\log(2\\pi) - n/2 \\log(\\sigma^2) + \\log(exp(-1/(2\\sigma^2)\\sum_{i=1}^n(X_i-\\mu)^2)) =$$\n",
    "$$= -n/2 \\log(2\\pi) - n/2 \\log(\\sigma^2) - 1/(2\\sigma^2) \\sum_{i=1}^n(X_i-\\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec24bf0",
   "metadata": {},
   "source": [
    "3. **Step 3: Take Derivatives**\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\mu} = \\frac{\\partial }{\\partial\\mu} \\bigg(-n/2 \\log(2\\pi) - n/2 \\log(\\sigma^2) - 1/(2\\sigma^2) \\sum_{i=1}^n(X_i-\\mu)^2\\bigg) = \\frac{\\partial }{\\partial\\mu} \\bigg(- 1/(2\\sigma^2) \\sum_{i=1}^n(X_i-\\mu)^2\\bigg) =$$\n",
    "$$=- 1/(2\\sigma^2)\\times(-1)\\times 2\\times \\sum_{i=1}^n(X_i-\\mu) = 1/\\sigma^2 \\sum_{i=1}^n(X_i-\\mu)$$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\sigma^2} = \\frac{\\partial }{\\partial\\sigma^2} \\bigg(-n/2 \\log(2\\pi) - n/2 \\log(\\sigma^2) - 1/(2\\sigma^2) \\sum_{i=1}^n(X_i-\\mu)^2\\bigg) = -n/2\\times 1/\\sigma^2 + 1/(2\\sigma^4) \\sum_{i=1}^n(X_i-\\mu)^2 = -n/(2\\sigma^2) + 1/(2\\sigma^4) \\sum_{i=1}^n(X_i-\\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3105b1",
   "metadata": {},
   "source": [
    "4. **Step 4: Set Derivatives to 0 and Solve**\n",
    "\n",
    "1. for $\\mu$\n",
    "$$\\frac{\\partial l}{\\partial\\mu} =  1/\\sigma^2 \\sum_{i=1}^n(X_i-\\mu) = 0$$\n",
    "$$\\sum_{i=1}^n(X_i-\\mu) = 0$$\n",
    "$$\\sum_{i=1}^nX_i - n\\mu = 0$$\n",
    "$$\\mu = 1/n\\sum_{i=1}^nX_i$$\n",
    "\n",
    "Note that we get *sample mean*.\n",
    "\n",
    "2. for $\\sigma^2$\n",
    "$$\\frac{\\partial l}{\\partial\\sigma^2} =  -n/(2\\sigma^2) + 1/(2\\sigma^4) \\sum_{i=1}^n(X_i-\\mu)^2 = 0$$\n",
    "$$-n/(2\\sigma^2) + 1/(2\\sigma^4) \\sum_{i=1}^n(X_i-\\mu)^2 = 0$$\n",
    "$$-n + 1/(\\sigma^2)\\sum_{i=1}^n(X_i-\\mu)^2 = 0$$\n",
    "$$1/(\\sigma^2)\\sum_{i=1}^n(X_i-\\mu)^2 = n$$\n",
    "$$\\sum_{i=1}^n(X_i-\\mu)^2 = n\\sigma^2$$\n",
    "$$1/n\\sum_{i=1}^n(X_i-\\mu)^2 = \\sigma^2$$\n",
    "\n",
    "Note that we get *sample variance* (biased)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea6702",
   "metadata": {},
   "source": [
    "5. **Step 5: Verify These Are Maximum**\n",
    "\n",
    "Since we have two parameters, we need to compute the Hessian matrix:\n",
    "$$H = \\begin{pmatrix}\n",
    "\\frac{\\partial^2 \\ell}{\\partial \\mu^2} & \\frac{\\partial^2 \\ell}{\\partial \\mu \\partial \\sigma^2} \\\n",
    "\\frac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\mu} & \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "For a maximum, we need the Hessian to be negative definite at the MLE.\n",
    "\n",
    "Computing Each Second Derivative\n",
    "\n",
    "1. Second derivative with respect to $\\mu$\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial \\mu^2} = \\frac{\\partial}{\\partial \\mu}\\left[\\frac{n}{\\sigma^2}(\\bar{X} - \\mu)\\right]$$\n",
    "$$= \\frac{n}{\\sigma^2} \\times (-1) = -\\frac{n}{\\sigma^2}‚Äã$$\n",
    "\n",
    "*Note*: This doesn't depend on $\\mu$.  It's constant.\n",
    "\n",
    "\n",
    "2. Second derivative with respect to $\\sigma^2$\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{\\partial}{\\partial \\sigma^2}\\left[-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i=1}^{n}(X_i - \\mu)^2\\right]$$\n",
    "\n",
    "For the first term:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\sigma^2}\\left[-\\frac{n}{2\\sigma^2}\\right] = -\\frac{n}{2} \\times \\frac{\\partial}{\\partial \\sigma^2}[(\\sigma^2)^{-1}] = -\\frac{n}{2} \\times (-1)(\\sigma^2)^{-2} = \\frac{n}{2\\sigma^4}$$\n",
    "\n",
    "For the second term:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\sigma^2}\\left[\\frac{1}{2\\sigma^4}\\sum_{i=1}^{n}(X_i - \\mu)^2\\right] = \\frac{1}{2}\\sum_{i=1}^{n}(X_i - \\mu)^2 \\times (-2)(\\sigma^2)^{-3} = -\\frac{\\sum_{i=1}^{n}(X_i - \\mu)^2}{\\sigma^6}$$\n",
    "\n",
    "Combining:\n",
    "\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{n}{2\\sigma^4} - \\frac{\\sum_{i=1}^{n}(X_i - \\mu)^2}{\\sigma^6}$$\n",
    "\n",
    "3. Cross partial derivative\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial \\mu \\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2}\\left[\\frac{n}{\\sigma^2}(\\bar{X} - \\mu)\\right]$$\n",
    "$$= n(\\bar{X} - \\mu) \\times \\frac{\\partial}{\\partial \\sigma^2}[(\\sigma^2)^{-1}]=$$\n",
    "$$= n(\\bar{X} - \\mu) \\times (-1)(\\sigma^2)^{-2} = -\\frac{n(\\bar{X} - \\mu)}{\\sigma^4}‚Äã$$\n",
    "\n",
    "Evaluating at the MLE\n",
    "\n",
    "At the MLE: $\\hat{\\mu} = \\bar{X}$ and $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$\n",
    "\n",
    "1. At $\\hat{\\mu}, \\hat{\\sigma}^2$:\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial \\mu^2}\\bigg|_{\\hat{\\mu}, \\hat{\\sigma}^2} = -\\frac{n}{\\hat{\\sigma}^2} < 0 \\quad ‚úì$$\n",
    "\n",
    "*Interpretation*: Always negative ‚Üí concave down in $\\mu$ direction\n",
    "\n",
    "2. At $\\hat{\\mu}, \\hat{\\sigma}^2$:\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2}\\bigg|_{\\hat{\\mu}, \\hat{\\sigma}^2} = \\frac{n}{2\\hat{\\sigma}^4} - \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}{\\hat{\\sigma}^6}$$\n",
    "\n",
    "Substitute $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$:\n",
    "\n",
    "$$= \\frac{n}{2\\hat{\\sigma}^4} - \\frac{n\\hat{\\sigma}^2}{\\hat{\\sigma}^6} = \\frac{n}{2\\hat{\\sigma}^4} - \\frac{n}{\\hat{\\sigma}^4} = -\\frac{n}{2\\hat{\\sigma}^4} < 0 \\quad ‚úì$$\n",
    "\n",
    "*Interpretation*: Negative at the MLE ‚Üí concave down in $\\sigma^2$ direction\n",
    "\n",
    "3. Cross partial at $\\hat{\\mu}, \\hat{\\sigma}^2$:\n",
    "$$\\frac{\\partial^2 \\ell}{\\partial \\mu \\partial \\sigma^2}\\bigg|_{\\hat{\\mu}, \\hat{\\sigma}^2} = -\\frac{n(\\bar{X} - \\hat{\\mu})}{\\hat{\\sigma}^4}$$\n",
    "\n",
    "Since $\\hat{\\mu} = \\bar{X}$:\n",
    "\n",
    "$$= -\\frac{n \\times 0}{\\hat{\\sigma}^4} = 0$$\n",
    "\n",
    "*Interpretation*: No interaction between $\\mu$ and $\\sigma^2¬≤$ at the MLE (parameters are orthogonal)\n",
    "\n",
    "The Hessian Matrix at MLE\n",
    "$$H\\bigg|_{\\hat{\\mu}, \\hat{\\sigma}^2} = \\begin{pmatrix}\n",
    "-\\frac{n}{\\hat{\\sigma}^2} & 0 \\\n",
    "0 & -\\frac{n}{2\\hat{\\sigma}^4}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "This is a diagonal matrix with both diagonal entries negative!\n",
    "\n",
    "*Verifying Negative Definiteness*\n",
    "\n",
    "For a matrix to be negative definite, we need:\n",
    "\n",
    "- All eigenvalues negative, OR\n",
    "- Leading principal minors alternate in sign (starting negative)\n",
    "\n",
    "*Method 1: Eigenvalues*: \n",
    "\n",
    "Since $H$ is diagonal, eigenvalues are just the diagonal entries:\n",
    "- $\\lambda_1 = -\\frac{n}{\\hat{\\sigma}^2} < 0 ‚úì$\n",
    "- $\\lambda_2 = -\\frac{n}{2\\hat{\\sigma}^4} < 0 ‚úì$\n",
    "\n",
    "Both negative ‚Üí negative definite ‚úì\n",
    "\n",
    "*Method 2: Principal Minors* \n",
    "\n",
    "First leading principal minor:\n",
    "\n",
    "$$M_1 = -\\frac{n}{\\hat{\\sigma}^2} < 0 \\quad ‚úì$$\n",
    "\n",
    "Second leading principal minor (determinant):\n",
    "\n",
    "$$M_2 = \\det(H) = \\left(-\\frac{n}{\\hat{\\sigma}^2}\\right) \\times \\left(-\\frac{n}{2\\hat{\\sigma}^4}\\right) - 0^2 = \\frac{n^2}{2\\hat{\\sigma}^6} > 0 \\quad ‚úì$$\n",
    "\n",
    "Signs alternate: (‚àí, +) ‚Üí negative definite ‚úì\n",
    "\n",
    "Conclusion\n",
    "1. Both diagonal entries of Hessian are negative\n",
    "2. Off-diagonal entries are zero (parameters uncorrelated at MLE)\n",
    "3. Hessian is negative definite\n",
    "4. Therefore: $(\\hat{\\mu}, \\hat{\\sigma}^2) = (\\bar{X}, \\frac{1}{n}\\sum(X_i - \\bar{X})^2)$ is a MAXIMUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822047f",
   "metadata": {},
   "source": [
    "6. **Step 6: Compute MLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5128a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical MLEs\n",
    "mle_mu = np.mean(data)\n",
    "mle_sigma = np.std(data, ddof=0)  # ddof=0 for MLE (biased estimator)\n",
    "\n",
    "print(f\"ŒºÃÇ_MLE = {mle_mu:.4f}  (true: {true_mu})\")\n",
    "print(f\"œÉÃÇ_MLE = {mle_sigma:.4f}  (true: {true_sigma})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: 3D likelihood surface\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Create grid for parameters\n",
    "mu_range = np.linspace(true_mu - 2, true_mu + 2, 50)\n",
    "sigma_range = np.linspace(0.5, 4, 50)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "\n",
    "# Compute log-likelihood for each combination\n",
    "log_likelihood = np.zeros_like(MU)\n",
    "for i in range(len(mu_range)):\n",
    "    for j in range(len(sigma_range)):\n",
    "        mu = MU[j, i]\n",
    "        sigma = SIGMA[j, i]\n",
    "        ll = -n/2 * np.log(2*np.pi*sigma**2) - np.sum((data - mu)**2) / (2*sigma**2)\n",
    "        log_likelihood[j, i] = ll\n",
    "\n",
    "# Plot 1: 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(MU, SIGMA, log_likelihood, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([mle_mu], [mle_sigma], [np.max(log_likelihood)], \n",
    "           color='red', s=300, marker='*', edgecolors='darkred', linewidths=2)\n",
    "ax1.set_xlabel('Œº', fontsize=11)\n",
    "ax1.set_ylabel('œÉ', fontsize=11)\n",
    "ax1.set_zlabel('Log-Likelihood', fontsize=11)\n",
    "ax1.set_title('3D Log-Likelihood Surface', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Plot 2: Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(MU, SIGMA, log_likelihood, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.scatter(mle_mu, mle_sigma, s=400, color='red', marker='*', \n",
    "           edgecolors='darkred', linewidths=2, label='MLE', zorder=5)\n",
    "ax2.scatter(true_mu, true_sigma, s=300, color='gold', marker='o', \n",
    "           edgecolors='darkgoldenrod', linewidths=2, label='True', zorder=5)\n",
    "ax2.set_xlabel('Œº', fontsize=11)\n",
    "ax2.set_ylabel('œÉ', fontsize=11)\n",
    "ax2.set_title('Contour Plot', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Fitted distribution\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.hist(data, bins=30, density=True, alpha=0.7, color='lightblue', \n",
    "        edgecolor='black', label='Data')\n",
    "\n",
    "# True distribution\n",
    "x_range = np.linspace(data.min(), data.max(), 200)\n",
    "ax3.plot(x_range, stats.norm.pdf(x_range, true_mu, true_sigma), \n",
    "        'g-', linewidth=3, label=f'True: N({true_mu}, {true_sigma}¬≤)')\n",
    "\n",
    "# MLE distribution\n",
    "ax3.plot(x_range, stats.norm.pdf(x_range, mle_mu, mle_sigma), \n",
    "        'r--', linewidth=3, label=f'MLE: N({mle_mu:.2f}, {mle_sigma:.2f}¬≤)')\n",
    "\n",
    "ax3.set_xlabel('x', fontsize=11)\n",
    "ax3.set_ylabel('Density', fontsize=11)\n",
    "ax3.set_title('Fitted Distribution', fontsize=13, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd53380",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Key Insight: MLE vs. Unbiased Estimator</h4>\n",
    "\n",
    "**Important note:** For normal distribution variance:\n",
    "- MLE: $\\sigma^2_MLE = (1/n) \\sum_{i=1}^n(X_i - \\hat{\\mu})^2$  ‚Üí **Biased** (underestimates on average)\n",
    "- Unbiased: $s^2 = 1/(n-1) \\sum_{i=1}^n(X_i - \\hat{\\mu})^2$  ‚Üí **Unbiased**\n",
    "\n",
    "**Why the difference?**\n",
    "- MLE uses $\\hat{\\mu$ (estimated mean), not true $\\mu$\n",
    "- This introduces dependency, causing bias\n",
    "- Factor $(n-1)$ corrects for this (Bessel's correction)\n",
    "\n",
    "**In practice:** For large $n$, the difference is negligible!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df6007",
   "metadata": {},
   "source": [
    "### MLE Properties and Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b3f03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Properties of MLE</h4>\n",
    "\n",
    "**Asymptotic Properties** (as n ‚Üí ‚àû):\n",
    "\n",
    "1. **Consistency:** Œ∏ÃÇ_MLE ‚Üí Œ∏ (converges to true value)\n",
    "\n",
    "2. **Asymptotic Normality:** \n",
    "   $$\\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta) \\xrightarrow{d} N(0, I(\\theta)^{-1})$$\n",
    "   where $I(\\theta)$ is the Fisher Information\n",
    "\n",
    "3. **Efficiency:** Among all consistent estimators, MLE has minimum asymptotic variance\n",
    "\n",
    "4. **Invariance:** If $\\hat{\\theta}_{MLE}$ is MLE for $\\theta$, then $g(\\hat{\\theta}_{MLE})$ is MLE for $g(\\theta)$\n",
    "\n",
    "**Why MLE is popular:**\n",
    "- Strong theoretical properties\n",
    "- Often has closed-form solution\n",
    "- Intuitive interpretation\n",
    "- Works well in practice\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fb5b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Application: MLE via Optimization</h4>\n",
    "\n",
    "When no closed-form solution exists, we use numerical optimization:\n",
    "\n",
    "**Algorithm:** Gradient Ascent on Log-Likelihood\n",
    "\n",
    "```\n",
    "1. Initialize Œ∏‚ÇÄ\n",
    "2. Repeat:\n",
    "   Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú + Œ± ‚àá‚Ñì(Œ∏‚Çú)\n",
    "   where ‚àá‚Ñì(Œ∏) = gradient of log-likelihood\n",
    "3. Until convergence\n",
    "```\n",
    "\n",
    "This is exactly how we train ML models\n",
    "\n",
    "- Neural networks: gradient descent on negative log-likelihood\n",
    "- Logistic regression: same thing\n",
    "- Many other models: MLE via optimization\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd746b",
   "metadata": {},
   "source": [
    "## MoM vs MLE: Comparison\n",
    "\n",
    "| Aspect | Method of Moments | Maximum Likelihood |\n",
    "|--------|------------------|-------------------|\n",
    "| **Idea** | Match moments | Maximize probability of data |\n",
    "| **Complexity** | Usually simpler | Can be complex |\n",
    "| **Efficiency** | Less efficient | Most efficient (asymptotically) |\n",
    "| **Existence** | Always exists | May not have closed form |\n",
    "| **Optimality** | Not optimal | Optimal (under regularity) |\n",
    "| **Bias** | Often biased | Asymptotically unbiased |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5eaa5",
   "metadata": {},
   "source": [
    "## Maximum A Posteriori (MAP) Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ae817",
   "metadata": {},
   "source": [
    "<h4>The Limitation of MLE</h4>\n",
    "\n",
    "MLE says: \"Which $\\theta$ makes the data most likely?\" But what if we have prior knowledge about $\\theta$?\n",
    "\n",
    "*Example:*\n",
    "\n",
    "- You're estimating spam rate from 10 emails. You observe 9 spam, 1 ham.\n",
    "- MLE: $\\hat{p} = 0.9$ (90% spam rate)\n",
    "- But: You know from experience that typical spam rate is ~20-30%\n",
    "\n",
    "**Question**: Shouldn't we incorporate this knowledge?\n",
    "\n",
    "**Answer**: Yes. Use MAP estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6a9d6",
   "metadata": {},
   "source": [
    "*Background*: Based on millions of emails, we know spam rate ‚âà 25%\n",
    "\n",
    "Let's consider two scenarios: \n",
    "\n",
    "1. *Scenario 1: Small Sample*\n",
    "\n",
    "- Observed: 9 spam in 10 emails\n",
    "- MLE: 90% spam rate\n",
    "- Problem: Seems too high. Small sample might be misleading.\n",
    "- Better idea: Combine data with prior knowledge...\n",
    "\n",
    "2. *Scenario 2: Large Sample*\n",
    "\n",
    "- Observed: 250 spam in 1,000 emails\n",
    "- MLE: 25% spam rate\n",
    "- Prior: ~25% expected\n",
    "- Assessment: Data is strong evidence, prior less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "demonstrate_prior_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca0fc3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h4>Definition: Maximum A Posteriori (MAP) Estimation</h4>\n",
    "\n",
    "Bayes' Theorem:\n",
    "$$P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)} \\propto P(X | \\theta) P(\\theta)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P(\\theta|X)$ = posterior: probability of $\\theta$ given data\n",
    "- $P(X|\\theta)$ = likelihood: probability of data given $\\theta$\n",
    "- $P(\\theta)$ = prior: our belief about $\\theta$ before seeing data\n",
    "- $P(X)$ = evidence: normalizing constant (doesn't depend on $\\theta$)\n",
    "\n",
    "**MAP Estimator:**\n",
    "\n",
    "$$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} P(\\theta | X) = \\arg\\max_{\\theta} [P(X | \\theta) P(\\theta)]$$\n",
    "\n",
    "Or equivalently (taking logs):\n",
    "\n",
    "$$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} [\\log P(X | \\theta) + \\log P(\\theta)] = \\arg\\max_{\\theta} [\\ell(\\theta) + \\log P(\\theta)]$$\n",
    "\n",
    "*Interpretation:*\n",
    "\n",
    "- MLE: Maximize likelihood only\n",
    "- MAP: Maximize likelihood + prior\n",
    "- MAP incorporates prior knowledge\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE vs MAP\n",
    "compare_mle_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f48e2",
   "metadata": {},
   "source": [
    "**Key Observation:** \n",
    "As $n \\rightarrow \\infty$, $MAP \\rightarrow MLE$ (Data dominates prior with large samples)\n",
    "\n",
    "|MLE|MAP|\n",
    "|---|---|\n",
    "| ‚úì Uses only data </br>‚úó Ignores prior knowledge | ‚úì Incorporates prior knowledge |\n",
    "| ‚úì No assumptions beyond model | ‚úì Regularizes estimates | \n",
    "| ‚úó Can overfit with small samples | ‚úì Better with small samples| \n",
    "|  | ‚úó Requires choosing prior |\n",
    "|  | ‚úó Can be biased if prior is wrong \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f9fda",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>üí° Key Insight: When to Use MAP vs MLE?</h4>\n",
    "\n",
    "Use MLE when:\n",
    "\n",
    "- You have lots of data\n",
    "- No strong prior knowledge\n",
    "- Want purely data-driven estimates\n",
    "- Interpretability is critical\n",
    "\n",
    "Use MAP when:\n",
    "\n",
    "- Limited data (prior helps regularize)\n",
    "- Strong prior knowledge exists\n",
    "- Want to incorporate domain expertise\n",
    "- Overfitting is a concern\n",
    "\n",
    "In practice: Many ML methods are actually MAP\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259310a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Connection: MAP = Regularization</h4>\n",
    "\n",
    "**Regularization in ML is just MAP estimation with specific priors**\n",
    "\n",
    "1. Ridge Regression (L2 regularization):\n",
    "$$\\min_w ||y - Xw||^2 + \\lambda||w||^2$$\n",
    "\n",
    "This is equivalent to MAP with Gaussian prior on weights:\n",
    "$$\\sim N(0, \\sigma^2 I)$$\n",
    "where $\\lambda = 1/(2\\sigma^2)$\n",
    "\n",
    "2. Lasso Regression (L1 regularization):\n",
    "$$\\min_w ||y - Xw||^2 + \\lambda||w||_1$$\n",
    "This is equivalent to MAP with Laplace prior on weights:\n",
    "\n",
    "$$w \\sim \\text{Laplace}(0, b)$$\n",
    "\n",
    "What this means:\n",
    "\n",
    "- Regularization = imposing prior belief that weights should be small\n",
    "- $\\lambda$ = strength of prior belief\n",
    "- Different regularizations = different prior distributions\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538bbe72",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h4>‚ö†Ô∏è Common Mistake: Forgetting to Normalize</h4>\n",
    "\n",
    "**Problem:** Prior strength depends on data scale!\n",
    "\n",
    "**Example:**\n",
    "- Features in meters: Œª=1 might be good\n",
    "- Same features in millimeters: Œª=1 is now way too weak!\n",
    "\n",
    "**Solution:** Always normalize/standardize features before regularization:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "This ensures regularization strength is interpretable and consistent\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4875511",
   "metadata": {},
   "source": [
    "## Return to Opening Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea3153",
   "metadata": {},
   "source": [
    "Recall: We had 50 experiments with $\\hat{\\sigma} = 0.15$.\n",
    "\n",
    "1. Is $\\sigma = 0.15$ the 'true' optimal value?\n",
    "   - No. It's our MLE, but there's uncertainty.\n",
    "\n",
    "2. If we ran 500 experiments, would we get the same estimate?\n",
    "   - Probably not exactly, but it would be close\n",
    "   - With more data, our estimate becomes more reliable\n",
    "\n",
    "3. How do we quantify how 'wrong' our estimate might be?\n",
    "   - Use confidence intervals! (Next class)\n",
    "   - Or: compute standard error of estimator\n",
    "\n",
    "4. Your colleague claims $\\sigma = 0.12$ is better. Who's right?\n",
    "   - Use hypothesis testing (will be seen soon)\n",
    "   - Or: compare likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f3b8c",
   "metadata": {},
   "source": [
    "## Common Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e932568",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h4>‚ö†Ô∏è Common Pitfalls</h4>\n",
    "\n",
    "- Confusing estimate with true parameter\n",
    "- Ignoring bias-variance tradeoff\n",
    "- Forgetting to normalize before regularization\n",
    "- Choosing wrong prior in MAP\n",
    "- Not checking if solution is actually a maximum\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b37985",
   "metadata": {},
   "source": [
    "## ML Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635af19",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary\">\n",
    "<h4>ü§ñ ML Applications</h4>\n",
    "\n",
    "- Model Training: All supervised learning is parameter estimation\n",
    "- Regularization: L2/L1 penalties = Gaussian/Laplace priors\n",
    "- Loss Functions: Cross-entropy = negative log-likelihood\n",
    "- Optimization: Gradient descent = finding MLE/MAP\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c06cc0",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157ad83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-summary\">\n",
    "<h4>üéì Key Takeaways</h4>\n",
    "\n",
    "1. Point Estimation:\n",
    "\n",
    "- Estimator = function that produces estimate from data\n",
    "- Key properties: Bias, Variance, MSE\n",
    "- MSE = Bias¬≤ + Variance (fundamental tradeoff)\n",
    "\n",
    "2. Maximum Likelihood Estimation:\n",
    "\n",
    "- Principle: Choose Œ∏ that makes data most likely\n",
    "- Method: Maximize L(Œ∏|X) or ‚Ñì(Œ∏|X)\n",
    "- Properties: Consistent, efficient, asymptotically normal\n",
    "- Computation: Closed-form or gradient ascent\n",
    "\n",
    "3. Maximum A Posteriori:\n",
    "\n",
    "- Principle: Maximize posterior = likelihood √ó prior\n",
    "- Incorporates prior knowledge\n",
    "- Connection: MAP with Gaussian prior = Ridge regression\n",
    "- Becomes MLE as n ‚Üí ‚àû\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198518b",
   "metadata": {},
   "source": [
    "## Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0cf40",
   "metadata": {},
   "source": [
    "1. [Maximum Likelihood, Clearly Explained!!! by StatQuest](https://www.youtube.com/watch?v=XepXtl9YKwc)\n",
    "2. [In Statistics, Probability is not Likelihood by StatQuest](https://www.youtube.com/watch?v=pYxNSUDSFH4)\n",
    "3. [Maximum Likelihood For the Normal Distribution, step-by-step!!! by StatQuest](https://www.youtube.com/watch?v=Dn6b9fCIUpM)\n",
    "4. [Maximum Likelihood for the Exponential Distribution, Clearly Explained!! by StatQuest](https://www.youtube.com/watch?v=p3T-_LMrvBc)\n",
    "5. [Maximum Likelihood for the Binomial Distribution, Clearly Explained!!! by StatQuest!!!](https://www.youtube.com/watch?v=4KKV9yZCoM4)\n",
    "2. [What are degrees of freedom? by James Gilbert](https://www.youtube.com/watch?v=rATNoxKg1yA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probability-statistics-ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
